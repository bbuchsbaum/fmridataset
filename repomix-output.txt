This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: R/**/*.R, R/**/*.r, *.Rmd, *.rmd, DESCRIPTION, tests/**/*.R, tests/**/*.r
- Files matching patterns in .gitignore are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
R/
  all_generic.R
  backends.R
  config.R
  conversions.R
  data_access.R
  data_chunks.R
  dataset_constructors.R
  errors.R
  fmri_dataset_legacy.R
  fmri_dataset.R
  h5_backend.R
  matrix_backend.R
  nifti_backend.R
  print_methods.R
  sampling_frame_adapters.R
  storage_backend.R
tests/
  testthat/
    test_backend_chunking.R
    test_backward_compatibility.R
    test_chunk_utils.R
    test_config.R
    test_conversions.R
    test_data_chunks.R
    test_dataset.R
    test_error_constructors.R
    test_error_handling.R
    test_h5_backend.R
    test_integration.R
    test_internal_chunks.R
    test_latent_backend.R
    test_matrix_backend.R
    test_nifti_backend.R
    test_refactored_modules.R
    test_sampling_frame.R
    test_storage_backend.R
  testthat.R
DESCRIPTION
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="R/sampling_frame_adapters.R">
#' Adapter Methods for fmrihrf sampling_frame
#'
#' These methods provide compatibility between the local sampling_frame
#' implementation and the fmrihrf sampling_frame implementation.
#'
#' @name sampling_frame_adapters
#' @keywords internal
#' @importFrom fmrihrf sampling_frame
#' @importFrom fmrihrf blockids
#' @importFrom fmrihrf blocklens
#' @importFrom fmrihrf samples
#' @importFrom fmrihrf global_onsets
NULL

#' Test if Object is a Sampling Frame
#'
#' This function tests whether an object is of class 'sampling_frame'.
#'
#' @param x An object to test
#' @return TRUE if x is a sampling_frame object, FALSE otherwise
#' @export
is.sampling_frame <- function(x) {
  inherits(x, "sampling_frame")
}

#' @export
get_run_lengths.sampling_frame <- function(x, ...) {
  x$blocklens
}

#' @export
get_total_duration.sampling_frame <- function(x, ...) {
  sum(x$blocklens * x$TR)
}

#' @export
get_run_duration.sampling_frame <- function(x, ...) {
  x$blocklens * x$TR
}

#' @export
n_runs.sampling_frame <- function(x, ...) {
  length(x$blocklens)
}

#' @export
n_timepoints.sampling_frame <- function(x, ...) {
  sum(x$blocklens)
}

#' @export
get_TR.sampling_frame <- function(x, ...) {
  # Always return the first TR value for compatibility
  # (fmrihrf supports per-block TR but our code expects single TR)
  x$TR[1]
}

# Explicit method definitions that delegate to fmrihrf
#' @export
blocklens.sampling_frame <- function(x, ...) {
  x$blocklens
}

#' @export
blockids.sampling_frame <- function(x, ...) {
  rep(seq_along(x$blocklens), times = x$blocklens)
}

#' @export
samples.sampling_frame <- function(x, ...) {
  # Implement samples method directly since fmrihrf method is not exported
  1:sum(x$blocklens)
}
</file>

<file path="tests/testthat/test_error_constructors.R">
library(fmridataset)

test_that("error constructors create structured errors", {
  err <- fmridataset:::fmridataset_error_backend_io("oops", file = "f.h5", operation = "read")
  expect_s3_class(err, "fmridataset_error_backend_io")
  expect_match(err$message, "oops")
  expect_equal(err$file, "f.h5")
  expect_equal(err$operation, "read")

  err2 <- fmridataset:::fmridataset_error_config("bad", parameter = "x", value = 1)
  expect_s3_class(err2, "fmridataset_error_config")
  expect_equal(err2$parameter, "x")
  expect_equal(err2$value, 1)
})


test_that("stop_fmridataset throws the constructed error", {
  expect_error(
    fmridataset:::stop_fmridataset(fmridataset:::fmridataset_error_config, "bad", parameter = "y"),
    class = "fmridataset_error_config"
  )
})
</file>

<file path="tests/testthat/test_internal_chunks.R">
library(fmridataset)

# Tests for exec_strategy and collect_chunks

test_that("exec_strategy and collect_chunks work", {
  set.seed(1)
  Y <- matrix(rnorm(50 * 10), 50, 10)
  dset <- matrix_dataset(Y, TR = 1, run_length = 50)

  strat <- fmridataset:::exec_strategy("chunkwise", nchunks = 3)
  iter <- strat(dset)
  chunks <- fmridataset:::collect_chunks(iter)

  expect_equal(length(chunks), 3)
  expect_true(all(sapply(chunks, inherits, "data_chunk")))

  voxel_inds <- sort(unlist(lapply(chunks, function(ch) ch$voxel_ind)))
  expect_equal(voxel_inds, 1:10)
})
</file>

<file path="tests/testthat/test_sampling_frame.R">
library(fmridataset)

# Tests for sampling_frame utilities

test_that("sampling_frame utilities work", {
  sf <- fmrihrf::sampling_frame(blocklens = c(10, 20, 30), TR = 2)

  expect_true(is.sampling_frame(sf))
  expect_equal(get_TR(sf), 2)
  expect_equal(get_run_lengths(sf), c(10, 20, 30))
  expect_equal(n_runs(sf), 3)
  expect_equal(n_timepoints(sf), 60)
  expect_equal(blocklens(sf), c(10, 20, 30))
  expect_equal(blockids(sf), c(rep(1, 10), rep(2, 20), rep(3, 30)))
  expect_equal(samples(sf), 1:60)
  # Note: fmrihrf global_onsets has different signature - needs onsets and blockids
  # expect_equal(global_onsets(sf), c(1, 11, 31))
  expect_equal(get_total_duration(sf), 120)
  expect_equal(get_run_duration(sf), c(20, 40, 60))
  expect_output(print(sf), "Sampling Frame")
})
</file>

<file path="R/all_generic.R">
# ========================================================================
# Generic Function Declarations for fmridataset Refactored Modules
# ========================================================================
#
# This file declares S3 generic functions for the refactored fMRI dataset
# functionality. These generics support the modular file structure and
# enable method dispatch across different dataset types.
#
# Note: This complements the existing aaa_generics.R which handles
# BIDS and other package-wide generics.
# ========================================================================

#' Generic Functions for fMRI Dataset Operations
#'
#' This file contains all generic function declarations for the refactored
#' fmridataset package. These establish the interface contracts that are
#' implemented by dataset-specific methods in other files.
#'
#' @name generics
NULL

#' Get Data from fMRI Dataset Objects
#'
#' Generic function to extract data from various fMRI dataset types.
#' Returns the underlying data in its native format (NeuroVec, matrix, etc.).
#'
#' @param x An fMRI dataset object
#' @param ... Additional arguments passed to methods
#' @return Dataset-specific data object
#' @export
get_data <- function(x, ...) {
  UseMethod("get_data")
}

#' Get Data Matrix from fMRI Dataset Objects
#'
#' Generic function to extract data as a matrix from various fMRI dataset types.
#' Always returns a matrix with timepoints as rows and voxels as columns.
#'
#' @param x An fMRI dataset object
#' @param ... Additional arguments passed to methods
#' @return A matrix with timepoints as rows and voxels as columns
#' @export
get_data_matrix <- function(x, ...) {
  UseMethod("get_data_matrix")
}

#' Get Mask from fMRI Dataset Objects
#'
#' Generic function to extract masks from various fMRI dataset types.
#' Returns the mask in its appropriate format for the dataset type.
#'
#' @param x An fMRI dataset object
#' @param ... Additional arguments passed to methods
#' @return Mask object (NeuroVol, vector, etc.)
#' @export
get_mask <- function(x, ...) {
  UseMethod("get_mask")
}

#' Get Block Lengths from Objects
#'
#' Generic function to extract block/run lengths from various objects.
#' Extends the sampling_frame generic to work with dataset objects.
#'
#' @param x An object with block structure
#' @param ... Additional arguments passed to methods
#' @return Integer vector of block/run lengths
#' @export
blocklens <- function(x, ...) {
  UseMethod("blocklens")
}

#' Create Data Chunks for Processing
#'
#' Generic function to create data chunks for parallel processing from
#' various fMRI dataset types. Supports different chunking strategies.
#'
#' @param x An fMRI dataset object
#' @param nchunks Number of chunks to create (default: 1)
#' @param runwise If TRUE, create run-wise chunks (default: FALSE)
#' @param ... Additional arguments passed to methods
#' @return A chunk iterator object
#' @export
data_chunks <- function(x, nchunks = 1, runwise = FALSE, ...) {
  UseMethod("data_chunks")
}

#' Convert to Matrix Dataset
#'
#' Generic function to convert various fMRI dataset types to matrix_dataset objects.
#' Provides a unified interface for getting matrix-based representations.
#'
#' @param x An fMRI dataset object
#' @param ... Additional arguments passed to methods
#' @return A matrix_dataset object
#' @export
as.matrix_dataset <- function(x, ...) {
  UseMethod("as.matrix_dataset")
}

# Sampling frame generics
#' Get TR from sampling frame
#' @param x Sampling frame object
#' @param ... Additional arguments
#' @export
get_TR <- function(x, ...) {
  UseMethod("get_TR")
}

#' Get run lengths from sampling frame
#' @param x Sampling frame object
#' @param ... Additional arguments
#' @export
get_run_lengths <- function(x, ...) {
  UseMethod("get_run_lengths")
}

#' Get number of runs from sampling frame
#' @param x Sampling frame object
#' @param ... Additional arguments
#' @export
n_runs <- function(x, ...) {
  UseMethod("n_runs")
}

#' Get number of timepoints from sampling frame
#' @param x Sampling frame object
#' @param ... Additional arguments
#' @export
n_timepoints <- function(x, ...) {
  UseMethod("n_timepoints")
}

#' Get block IDs from sampling frame
#' @param x Sampling frame object
#' @param ... Additional arguments
#' @export
blockids <- function(x, ...) {
  UseMethod("blockids")
}

#' Get samples from sampling frame
#' @param x Sampling frame object
#' @param ... Additional arguments
#' @export
samples <- function(x, ...) {
  UseMethod("samples")
}

#' Get global onsets from sampling frame
#' @param x Sampling frame object
#' @param ... Additional arguments
#' @export
global_onsets <- function(x, ...) {
  UseMethod("global_onsets")
}

#' Get total duration from sampling frame
#' @param x Sampling frame object
#' @param ... Additional arguments
#' @export
get_total_duration <- function(x, ...) {
  UseMethod("get_total_duration")
}

#' Get run duration from sampling frame
#' @param x Sampling frame object
#' @param ... Additional arguments
#' @export
get_run_duration <- function(x, ...) {
  UseMethod("get_run_duration")
}

# ========================================================================
# Documentation
# ========================================================================
#
# These generics enable the modular file structure by providing clean
# interfaces between different components:
#
# - data_access.R implements get_data*, get_mask*, blocklens* methods
# - data_chunks.R implements data_chunks* methods
# - conversions.R implements as.matrix_dataset* methods
# - dataset_constructors.R provides the objects these generics operate on
# - print_methods.R provides specialized display methods
#
# All original fmrireg/fmridataset functionality is preserved while
# improving code organization and maintainability.
# ========================================================================
</file>

<file path="R/backends.R">
#' Create a Latent Backend
#'
#' @description
#' Creates a storage backend for accessing latent space representations of fMRI data
#' using LatentNeuroVec objects from the fmristore package. This backend provides
#' efficient access to data stored in a compressed latent space format.
#'
#' @param source A character vector of file paths to LatentNeuroVec HDF5 files (.lv.h5),
#'   or a list of LatentNeuroVec objects from the fmristore package.
#' @param mask_source Optional mask source. If NULL, the mask will be extracted from
#'   the first LatentNeuroVec object.
#' @param preload Logical indicating whether to preload all data into memory.
#'   Default is FALSE (lazy loading).
#'
#' @return A `latent_backend` object that implements the storage backend interface.
#'
#' @details
#' The latent backend supports LatentNeuroVec objects which store fMRI data in a
#' compressed latent space representation using basis functions and spatial loadings.
#' This format is particularly efficient for data that can be well-represented by
#' a lower-dimensional basis (e.g., from PCA, ICA, or dictionary learning).
#'
#' **LatentNeuroVec Structure:**
#' - `basis`: Temporal components (n_timepoints × k_components)
#' - `loadings`: Spatial components (n_voxels × k_components)
#' - `offset`: Optional per-voxel offset terms
#' - Data is reconstructed as: `data = basis %*% t(loadings) + offset`
#'
#' **IMPORTANT: Data Access Behavior**
#' Unlike other backends that return voxel-wise data, the latent_backend returns
#' **latent scores** (the basis/temporal components) rather than reconstructed voxel data.
#' This is because:
#' - Analyses are typically performed in the latent space for efficiency
#' - The latent scores capture the temporal dynamics in the compressed representation
#' - Reconstructing to ambient voxel space defeats the purpose of the compression
#' - `backend_get_data()` returns a matrix of size (time × components), not (time × voxels)
#' - `backend_get_mask()` returns a logical vector indicating active components, not spatial voxels
#'
#' **Supported Input Types:**
#' - File paths to `.lv.h5` files (LatentNeuroVec HDF5 format)
#' - Pre-loaded LatentNeuroVec objects
#' - Mixed lists of files and objects
#'
#' @examples
#' \dontrun{
#' # From LatentNeuroVec HDF5 files
#' backend <- latent_backend(
#'   source = c("run1.lv.h5", "run2.lv.h5", "run3.lv.h5")
#' )
#'
#' # From pre-loaded LatentNeuroVec objects
#' lvec1 <- fmristore::read_vec("run1.lv.h5")
#' lvec2 <- fmristore::read_vec("run2.lv.h5")
#' backend <- latent_backend(source = list(lvec1, lvec2))
#'
#' # Mixed sources
#' backend <- latent_backend(
#'   source = list(lvec1, "run2.lv.h5", "run3.lv.h5")
#' )
#' }
#'
#' @seealso
#' \code{\link{h5_backend}}, \code{\link{nifti_backend}}, \code{\link{matrix_backend}}
#'
#' @export
latent_backend <- function(source, mask_source = NULL, preload = FALSE) {
  assert_that(is.logical(preload))

  # Validate and process source
  if (is.character(source)) {
    # All file paths
    assert_that(all(file.exists(source)),
      msg = "All source files must exist"
    )
    assert_that(all(grepl("\\.(lv\\.h5|h5)$", source, ignore.case = TRUE)),
      msg = "All source files must be HDF5 files (.h5 or .lv.h5)"
    )
  } else if (is.list(source)) {
    # List of objects and/or file paths
    for (i in seq_along(source)) {
      item <- source[[i]]
      if (is.character(item)) {
        assert_that(length(item) == 1 && file.exists(item),
          msg = paste("Source item", i, "must be an existing file path")
        )
        assert_that(grepl("\\.(lv\\.h5|h5)$", item, ignore.case = TRUE),
          msg = paste("Source file", i, "must be an HDF5 file")
        )
      } else {
        assert_that(inherits(item, "LatentNeuroVec"),
          msg = paste("Source item", i, "must be a LatentNeuroVec object or file path")
        )
      }
    }
  } else {
    stop("source must be a character vector of file paths or a list of LatentNeuroVec objects/file paths")
  }

  # Create the backend object
  backend <- structure(
    list(
      source = source,
      mask_source = mask_source,
      preload = preload,
      data = if (preload) NULL else NULL, # Will be populated by backend_open
      is_open = FALSE
    ),
    class = c("latent_backend", "storage_backend")
  )

  backend
}

# Latent Backend Methods Implementation ====

#' @export
backend_open.latent_backend <- function(backend) {
  if (backend$is_open) {
    return(backend)
  }

  # Check if fmristore package is available
  if (!requireNamespace("fmristore", quietly = TRUE)) {
    stop("The fmristore package is required for latent_backend but is not installed")
  }

  # Load all LatentNeuroVec objects
  source_data <- list()

  if (is.character(backend$source)) {
    # All file paths
    for (i in seq_along(backend$source)) {
      path <- backend$source[i]
      source_data[[i]] <- fmristore::read_vec(path)
    }
  } else if (is.list(backend$source)) {
    # Mixed list
    for (i in seq_along(backend$source)) {
      item <- backend$source[[i]]
      if (is.character(item)) {
        source_data[[i]] <- fmristore::read_vec(item)
      } else {
        source_data[[i]] <- item # Already a LatentNeuroVec
      }
    }
  }

  # Validate all objects are LatentNeuroVec
  for (i in seq_along(source_data)) {
    if (!inherits(source_data[[i]], "LatentNeuroVec")) {
      stop(paste("Item", i, "is not a LatentNeuroVec object"))
    }
  }

  # Check consistency across objects
  if (length(source_data) > 1) {
    first_obj <- source_data[[1]]
    first_space_dims <- dim(neuroim2::space(first_obj))[1:3]
    first_mask <- as.array(neuroim2::mask(first_obj))

    for (i in 2:length(source_data)) {
      obj <- source_data[[i]]
      space_dims <- dim(neuroim2::space(obj))[1:3]
      mask_array <- as.array(neuroim2::mask(obj))

      if (!identical(first_space_dims, space_dims)) {
        stop(paste("LatentNeuroVec", i, "has inconsistent spatial dimensions"))
      }

      if (!identical(first_mask, mask_array)) {
        stop(paste("LatentNeuroVec", i, "has inconsistent mask"))
      }
    }
  }

  # Store the loaded data
  backend$data <- source_data
  backend$is_open <- TRUE
  backend
}

#' @export
backend_close.latent_backend <- function(backend) {
  if (!backend$is_open) {
    return(backend)
  }

  # Close any HDF5 file handles if they exist
  if (!is.null(backend$data)) {
    for (obj in backend$data) {
      if (inherits(obj, "LatentNeuroVec") && !is.null(obj@map)) {
        # LatentNeuroVec objects may have HDF5 handles, but they're usually managed automatically
        # No explicit close needed for LatentNeuroVec in fmristore
      }
    }
  }

  backend$data <- NULL
  backend$is_open <- FALSE
  backend
}

#' @export
backend_get_dims.latent_backend <- function(backend) {
  if (!backend$is_open) {
    stop("Backend must be opened before getting dimensions")
  }

  if (length(backend$data) == 0) {
    stop("No data available in backend")
  }

  # Get dimensions from first object
  first_obj <- backend$data[[1]]
  space_dims <- dim(neuroim2::space(first_obj))

  # Calculate total time across all objects
  total_time <- sum(sapply(backend$data, function(obj) dim(neuroim2::space(obj))[4]))

  # For latent backends, the "space" dimensions refer to the original spatial dimensions
  # but the data dimensions are time x components
  n_components <- ncol(first_obj@basis)

  list(
    space = space_dims[1:3], # Original spatial dimensions (for reference)
    time = total_time, # Total time points across all runs
    n_runs = length(backend$data), # Number of runs
    n_components = n_components, # Number of latent components (actual data columns)
    data_dims = c(total_time, n_components) # Actual data matrix dimensions
  )
}

#' @export
backend_get_mask.latent_backend <- function(backend) {
  if (!backend$is_open) {
    stop("Backend must be opened before getting mask")
  }

  if (length(backend$data) == 0) {
    stop("No data available in backend")
  }

  # For latent backends, the "mask" represents which components are active
  # Since all components are typically used, we return a mask of all TRUE
  # This is different from spatial masks used in other backends
  first_obj <- backend$data[[1]]
  n_components <- ncol(first_obj@basis)

  # Return logical vector indicating all components are active
  rep(TRUE, n_components)
}

#' @export
backend_get_data.latent_backend <- function(backend, rows = NULL, cols = NULL) {
  if (!backend$is_open) {
    stop("Backend must be opened before getting data")
  }

  if (length(backend$data) == 0) {
    stop("No data available in backend")
  }

  # For latent backends, data consists of latent scores (basis functions)
  # NOT reconstructed voxel data. This is the key difference from other backends.

  # Get dimensions
  dims <- backend_get_dims(backend)
  first_obj <- backend$data[[1]]
  n_components <- ncol(first_obj@basis)

  # Default to all rows/cols if not specified
  if (is.null(rows)) {
    rows <- 1:dims$time
  }
  if (is.null(cols)) {
    cols <- 1:n_components
  }

  # Validate column indices (components, not voxels)
  if (any(cols < 1) || any(cols > n_components)) {
    stop(paste("Column indices must be between 1 and", n_components, "(number of components)"))
  }

  # Determine which runs contain the requested rows
  time_offsets <- c(0, cumsum(sapply(backend$data, function(obj) dim(neuroim2::space(obj))[4])))

  # Initialize result matrix (time x components)
  result <- matrix(0, nrow = length(rows), ncol = length(cols))

  for (row_idx in seq_along(rows)) {
    global_row <- rows[row_idx]

    # Find which run this row belongs to
    run_idx <- which(global_row > time_offsets & global_row <= time_offsets[-1])[1]

    if (is.na(run_idx)) {
      next # Skip invalid rows
    }

    # Calculate local row index within the run
    local_row <- global_row - time_offsets[run_idx]

    # Get the LatentNeuroVec object for this run
    obj <- backend$data[[run_idx]]

    # Extract latent scores (basis matrix) for this timepoint
    # The basis matrix is (time x components)
    latent_scores <- as.matrix(obj@basis[local_row, cols, drop = FALSE])

    # Store in result
    result[row_idx, ] <- latent_scores
  }

  result
}

#' @export
backend_get_metadata.latent_backend <- function(backend) {
  if (!backend$is_open) {
    stop("Backend must be opened before getting metadata")
  }

  if (length(backend$data) == 0) {
    stop("No data available in backend")
  }

  # Collect metadata from all LatentNeuroVec objects
  metadata <- list()

  for (i in seq_along(backend$data)) {
    obj <- backend$data[[i]]

    # Extract basic metadata
    space_obj <- neuroim2::space(obj)
    obj_dims <- dim(space_obj)

    run_meta <- list(
      run = i,
      n_timepoints = obj_dims[4],
      spatial_dims = obj_dims[1:3],
      spacing = neuroim2::spacing(space_obj),
      origin = neuroim2::origin(space_obj),
      n_components = ncol(obj@basis),
      label = if (length(obj@label) > 0) obj@label else paste("run", i),
      has_offset = length(obj@offset) > 0,
      basis_class = class(obj@basis)[1],
      loadings_class = class(obj@loadings)[1],
      loadings_sparsity = if (inherits(obj@loadings, "Matrix")) {
        Matrix::nnzero(obj@loadings) / length(obj@loadings)
      } else {
        1.0 # Dense matrix
      }
    )

    metadata[[i]] <- run_meta
  }

  names(metadata) <- paste0("run_", seq_along(metadata))
  metadata
}
</file>

<file path="R/config.R">
#' @keywords internal
#' @noRd
default_config <- function() {
  env <- new.env()
  env$cmd_flags <- ""
  env$jobs <- 1
  env
}


#' read a basic fMRI configuration file
#'
#' @param file_name name of configuration file
#' @param base_path the file path to be prepended to relative file names
#' @importFrom assertthat assert_that
#' @importFrom tibble as_tibble
#' @importFrom utils read.table
#' @export
#' @return a \code{fmri_config} instance
read_fmri_config <- function(file_name, base_path = NULL) {
  # print(file_name)
  env <- default_config()

  source(file_name, env)

  env$base_path <- if (is.null(env$base_path) && is.null(base_path)) {
    "."
  } else if (!is.null(base_path) && is.null(env$base_path)) {
    base_path
  }

  if (is.null(env$output_dir)) {
    env$output_dir <- "stat_out"
  }


  assert_that(!is.null(env$scans))
  assert_that(!is.null(env$TR))
  assert_that(!is.null(env$mask))
  assert_that(!is.null(env$run_length))
  assert_that(!is.null(env$event_model))
  assert_that(!is.null(env$event_table))
  assert_that(!is.null(env$block_column))
  assert_that(!is.null(env$baseline_model))

  if (!is.null(env$censor_file)) {
    env$censor_file <- NULL
  }

  if (!is.null(env$contrasts)) {
    env$contrasts <- NULL
  }

  if (!is.null(env$nuisance)) {
    env$nuisance <- NULL
  }

  dname <- file.path(env$base_path, env$event_table)

  assert_that(file.exists(dname))
  env$design <- suppressMessages(tibble::as_tibble(read.table(dname, header = TRUE), .name_repair = "check_unique"))

  out <- as.list(env)
  class(out) <- c("fmri_config", "list")
  out
}
</file>

<file path="R/errors.R">
#' Custom Error Classes for fmridataset
#'
#' @description
#' A hierarchy of custom S3 error classes for the fmridataset package.
#' These provide structured error handling for storage backend operations.
#'
#' @name fmridataset-errors
#' @keywords internal
NULL

#' Create a Custom fmridataset Error
#'
#' @param message Character string describing the error
#' @param class Character vector of error classes
#' @param ... Additional data to include in the error condition
#' @return A condition object
#' @keywords internal
fmridataset_error <- function(message, class = character(), ...) {
  structure(
    list(message = message, ...),
    class = c(class, "fmridataset_error", "error", "condition")
  )
}

#' Backend I/O Error
#'
#' @description
#' Raised when a storage backend encounters read/write failures.
#'
#' @param message Character string describing the I/O error
#' @param file Path to the file that caused the error (optional)
#' @param operation The operation that failed (e.g., "read", "write")
#' @param ... Additional context
#' @return A backend I/O error condition
#' @keywords internal
fmridataset_error_backend_io <- function(message, file = NULL, operation = NULL, ...) {
  fmridataset_error(
    message = message,
    class = "fmridataset_error_backend_io",
    file = file,
    operation = operation,
    ...
  )
}

#' Configuration Error
#'
#' @description
#' Raised when invalid configuration is provided to a backend or dataset.
#'
#' @param message Character string describing the configuration error
#' @param parameter The parameter that was invalid
#' @param value The invalid value provided
#' @param ... Additional context
#' @return A configuration error condition
#' @keywords internal
fmridataset_error_config <- function(message, parameter = NULL, value = NULL, ...) {
  fmridataset_error(
    message = message,
    class = "fmridataset_error_config",
    parameter = parameter,
    value = value,
    ...
  )
}

#' Stop with a Custom Error
#'
#' @param error_fn Error constructor function
#' @param ... Arguments passed to the error constructor
#' @keywords internal
stop_fmridataset <- function(error_fn, ...) {
  err <- error_fn(...)
  stop(err)
}
</file>

<file path="R/fmri_dataset_legacy.R">
#' Legacy fMRI Dataset Implementation
#'
#' @description
#' This file contains the original fmri_file_dataset implementation,
#' preserved for backwards compatibility and testing during the transition
#' to the new backend architecture.
#'
#' @name fmri_dataset_legacy
#' @keywords internal
NULL

#' @export
#' @keywords internal
fmri_dataset_legacy <- function(scans, mask, TR,
                                run_length,
                                event_table = data.frame(),
                                base_path = ".",
                                censor = NULL,
                                preload = FALSE,
                                mode = c("normal", "bigvec", "mmap", "filebacked")) {
  assert_that(is.character(mask), msg = "'mask' should be the file name of the binary mask file")
  mode <- match.arg(mode)

  if (is.null(censor)) {
    censor <- rep(0, sum(run_length))
  }

  frame <- fmrihrf::sampling_frame(blocklens = run_length, TR = TR)

  maskfile <- paste0(base_path, "/", mask)
  scans <- paste0(base_path, "/", scans)

  maskvol <- if (preload) {
    assert_that(file.exists(maskfile))
    message(paste("preloading masks", maskfile))
    neuroim2::read_vol(maskfile)
  }

  vec <- if (preload) {
    message(paste("preloading scans", paste(scans, collapse = " ")))
    neuroim2::read_vec(scans, mode = mode, mask = maskvol)
  }


  ret <- list(
    scans = scans,
    vec = vec,
    mask_file = maskfile,
    mask = maskvol,
    nruns = length(run_length),
    event_table = suppressMessages(tibble::as_tibble(event_table, .name_repair = "check_unique")),
    base_path = base_path,
    sampling_frame = frame,
    censor = censor,
    mode = mode,
    preload = preload
  )

  class(ret) <- c("fmri_file_dataset", "volumetric_dataset", "fmri_dataset", "list")
  ret
}
</file>

<file path="R/fmri_dataset.R">
# ========================================================================
# fMRI Dataset Package - Main Entry Point
# ========================================================================
#
# This file serves as the main entry point for the fmridataset package.
# The original fmri_dataset.R file has been refactored into multiple
# modular files for better maintainability:
#
# Code Organization:
# ------------------
#
# 📁 config.R             - Configuration and file reading functions
#    • default_config()
#    • read_fmri_config()
#
# 📁 dataset_constructors.R - Dataset creation functions
#    • matrix_dataset()
#    • fmri_mem_dataset()
#    • latent_dataset()
#    • fmri_dataset()
#
# 📁 data_access.R         - Data access and mask methods
#    • get_data.* methods
#    • get_data_matrix.* methods
#    • get_mask.* methods
#    • get_data_from_file()
#    • blocklens.* methods
#
# 📁 data_chunks.R         - Data chunking and iteration
#    • data_chunk()
#    • chunk_iter()
#    • data_chunks.* methods
#    • exec_strategy()
#    • collect_chunks()
#    • arbitrary_chunks()
#    • slicewise_chunks()
#    • one_chunk()
#
# 📁 print_methods.R       - Print and display methods
#    • print.fmri_dataset()
#    • print.latent_dataset()
#    • print.chunkiter()
#    • print.data_chunk()
#
# 📁 conversions.R         - Type conversion methods
#    • as.matrix_dataset()
#    • as.matrix_dataset.* methods
#
# ========================================================================

# Essential imports and operators that are used across multiple files
`%dopar%` <- foreach::`%dopar%`
`%do%` <- foreach::`%do%`

# ========================================================================
# Package-level documentation and imports
# ========================================================================
#
# This refactoring improves:
# 1. Code organization and readability
# 2. Maintainability - easier to find and modify specific functionality
# 3. Testing - can test individual modules in isolation
# 4. Development - multiple developers can work on different aspects
# 5. Documentation - clearer separation of concerns
#
# All original functionality is preserved - only the organization changed.
# ========================================================================
</file>

<file path="R/matrix_backend.R">
#' Matrix Storage Backend
#'
#' @description
#' A storage backend implementation for in-memory matrix data.
#' This backend wraps existing matrix data in the storage backend interface.
#'
#' @name matrix-backend
#' @keywords internal
NULL

#' Create a Matrix Backend
#'
#' @param data_matrix A matrix in timepoints × voxels orientation
#' @param mask Logical vector indicating which voxels are valid
#' @param spatial_dims Numeric vector of length 3 specifying spatial dimensions
#' @param metadata Optional list of metadata
#' @return A matrix_backend S3 object
#' @export
#' @keywords internal
matrix_backend <- function(data_matrix, mask = NULL, spatial_dims = NULL, metadata = NULL) {
  # Validate inputs
  if (!is.matrix(data_matrix)) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "data_matrix must be a matrix",
      parameter = "data_matrix",
      value = class(data_matrix)
    )
  }

  n_timepoints <- nrow(data_matrix)
  n_voxels <- ncol(data_matrix)

  # Default mask: all voxels are valid
  if (is.null(mask)) {
    mask <- rep(TRUE, n_voxels)
  }

  # Validate mask
  if (!is.logical(mask)) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "mask must be a logical vector",
      parameter = "mask",
      value = class(mask)
    )
  }

  if (length(mask) != n_voxels) {
    stop_fmridataset(
      fmridataset_error_config,
      message = sprintf(
        "mask length (%d) must equal number of columns (%d)",
        length(mask), n_voxels
      ),
      parameter = "mask"
    )
  }

  # Default spatial dimensions: try to factorize n_voxels
  if (is.null(spatial_dims)) {
    # Simple approach: create a "flat" 3D volume
    spatial_dims <- c(n_voxels, 1, 1)
  }

  # Validate spatial dimensions
  if (length(spatial_dims) != 3 || !is.numeric(spatial_dims)) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "spatial_dims must be a numeric vector of length 3",
      parameter = "spatial_dims",
      value = spatial_dims
    )
  }

  if (prod(spatial_dims) != n_voxels) {
    stop_fmridataset(
      fmridataset_error_config,
      message = sprintf(
        "Product of spatial_dims (%d) must equal number of voxels (%d)",
        prod(spatial_dims), n_voxels
      ),
      parameter = "spatial_dims"
    )
  }

  backend <- list(
    data_matrix = data_matrix,
    mask = mask,
    spatial_dims = spatial_dims,
    metadata = metadata %||% list()
  )

  class(backend) <- c("matrix_backend", "storage_backend")
  backend
}

#' @export
backend_open.matrix_backend <- function(backend) {
  # Matrix backend is stateless - no resources to acquire
  backend
}

#' @export
backend_close.matrix_backend <- function(backend) {
  # Matrix backend is stateless - no resources to release
  invisible(NULL)
}

#' @export
backend_get_dims.matrix_backend <- function(backend) {
  list(
    spatial = backend$spatial_dims,
    time = nrow(backend$data_matrix)
  )
}

#' @export
backend_get_mask.matrix_backend <- function(backend) {
  backend$mask
}

#' @export
backend_get_data.matrix_backend <- function(backend, rows = NULL, cols = NULL) {
  data <- backend$data_matrix

  # Apply subsetting if requested
  if (!is.null(rows)) {
    data <- data[rows, , drop = FALSE]
  }

  if (!is.null(cols)) {
    data <- data[, cols, drop = FALSE]
  }

  data
}

#' @export
backend_get_metadata.matrix_backend <- function(backend) {
  backend$metadata
}

# Helper function
`%||%` <- function(x, y) if (is.null(x)) y else x
</file>

<file path="R/nifti_backend.R">
#' NIfTI Storage Backend
#'
#' @description
#' A storage backend implementation for NIfTI format neuroimaging data.
#' Supports both file-based and in-memory NIfTI data.
#'
#' @details
#' The NiftiBackend can work with:
#' - File paths to NIfTI images
#' - Pre-loaded niftiImage objects (from RNifti package)
#' - neuroim2 NeuroVec objects
#'
#' @name nifti-backend
#' @keywords internal
NULL

#' Create a NIfTI Backend
#'
#' @param source Character vector of file paths or list of in-memory niftiImage/NeuroVec objects
#' @param mask_source File path to mask or in-memory NeuroVol object
#' @param preload Logical, whether to eagerly load data into memory
#' @param mode Storage mode for file-backed data: 'normal', 'bigvec', 'mmap', or 'filebacked'
#' @return A nifti_backend S3 object
#' @export
#' @keywords internal
nifti_backend <- function(source, mask_source, preload = FALSE,
                          mode = c("normal", "bigvec", "mmap", "filebacked")) {
  mode <- match.arg(mode)

  # Validate inputs
  if (is.character(source)) {
    # File paths provided
    if (!all(file.exists(source))) {
      missing_files <- source[!file.exists(source)]
      stop_fmridataset(
        fmridataset_error_backend_io,
        message = sprintf("Source files not found: %s", paste(missing_files, collapse = ", ")),
        file = missing_files,
        operation = "open"
      )
    }
  } else if (is.list(source)) {
    # In-memory objects provided
    valid_types <- vapply(source, function(x) {
      inherits(x, "NeuroVec") || inherits(x, "niftiImage")
    }, logical(1))

    if (!all(valid_types)) {
      stop_fmridataset(
        fmridataset_error_config,
        message = "All source objects must be NeuroVec or niftiImage objects",
        parameter = "source"
      )
    }
  } else {
    stop_fmridataset(
      fmridataset_error_config,
      message = "source must be character vector (file paths) or list (in-memory objects)",
      parameter = "source",
      value = class(source)
    )
  }

  # Validate mask
  if (is.character(mask_source)) {
    if (!file.exists(mask_source)) {
      stop_fmridataset(
        fmridataset_error_backend_io,
        message = sprintf("Mask file not found: %s", mask_source),
        file = mask_source,
        operation = "open"
      )
    }
  } else if (!inherits(mask_source, "NeuroVol")) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "mask_source must be file path or NeuroVol object",
      parameter = "mask_source",
      value = class(mask_source)
    )
  }

  backend <- list(
    source = source,
    mask_source = mask_source,
    preload = preload,
    mode = mode,
    data = NULL,
    mask = NULL,
    dims = NULL,
    metadata = NULL
  )

  class(backend) <- c("nifti_backend", "storage_backend")
  backend
}

#' @export
backend_open.nifti_backend <- function(backend) {
  if (backend$preload && is.null(backend$data)) {
    # Load mask first
    backend$mask <- if (is.character(backend$mask_source)) {
      tryCatch(
        neuroim2::read_vol(backend$mask_source),
        error = function(e) {
          stop_fmridataset(
            fmridataset_error_backend_io,
            message = sprintf("Failed to read mask: %s", e$message),
            file = backend$mask_source,
            operation = "read"
          )
        }
      )
    } else {
      backend$mask_source
    }

    # Load data
    backend$data <- if (is.character(backend$source)) {
      tryCatch(
        neuroim2::read_vec(backend$source, mask = backend$mask, mode = backend$mode),
        error = function(e) {
          stop_fmridataset(
            fmridataset_error_backend_io,
            message = sprintf("Failed to read data: %s", e$message),
            file = backend$source,
            operation = "read"
          )
        }
      )
    } else {
      # Handle in-memory objects
      if (length(backend$source) > 1) {
        do.call(neuroim2::NeuroVecSeq, backend$source)
      } else {
        backend$source[[1]]
      }
    }

    # Extract dimensions
    if (inherits(backend$data, "NeuroVec")) {
      d <- dim(backend$data)
      backend$dims <- list(
        spatial = d[1:3],
        time = d[4]
      )
    }
  }

  backend
}

#' @export
backend_close.nifti_backend <- function(backend) {
  # For NIfTI backend, we don't need to explicitly close file handles
  # as neuroim2 manages this internally
  invisible(NULL)
}

#' @export
backend_get_dims.nifti_backend <- function(backend) {
  if (!is.null(backend$dims)) {
    return(backend$dims)
  }

  # Get dimensions without loading full data
  if (is.character(backend$source)) {
    # Read header only - neuroim2 doesn't support header_only, so read minimal data
    tryCatch(
      {
        vec <- neuroim2::read_vec(backend$source[1])
        d <- dim(vec)
        # Sum time dimension across all files
        total_time <- if (length(backend$source) > 1) {
          sum(sapply(backend$source, function(f) {
            v <- neuroim2::read_vec(f)
            dim(v)[4]
          }))
        } else {
          d[4]
        }

        list(spatial = d[1:3], time = total_time)
      },
      error = function(e) {
        stop_fmridataset(
          fmridataset_error_backend_io,
          message = sprintf("Failed to read dimensions: %s", e$message),
          file = backend$source[1],
          operation = "read_header"
        )
      }
    )
  } else {
    # In-memory objects
    obj <- if (is.list(backend$source)) backend$source[[1]] else backend$source
    d <- dim(obj)
    total_time <- if (is.list(backend$source) && length(backend$source) > 1) {
      sum(sapply(backend$source, function(x) dim(x)[4]))
    } else {
      d[4]
    }

    list(spatial = d[1:3], time = total_time)
  }
}

#' @export
backend_get_mask.nifti_backend <- function(backend) {
  if (!is.null(backend$mask)) {
    # Already loaded
    mask_vol <- backend$mask
  } else if (is.character(backend$mask_source)) {
    # Load from file
    mask_vol <- tryCatch(
      neuroim2::read_vol(backend$mask_source),
      error = function(e) {
        stop_fmridataset(
          fmridataset_error_backend_io,
          message = sprintf("Failed to read mask: %s", e$message),
          file = backend$mask_source,
          operation = "read"
        )
      }
    )
  } else {
    # In-memory mask
    mask_vol <- backend$mask_source
  }

  # Convert to logical vector
  mask_vec <- as.logical(as.vector(mask_vol))

  # Validate mask
  if (any(is.na(mask_vec))) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "Mask contains NA values",
      parameter = "mask"
    )
  }

  if (sum(mask_vec) == 0) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "Mask contains no TRUE values",
      parameter = "mask"
    )
  }

  mask_vec
}

#' @export
backend_get_data.nifti_backend <- function(backend, rows = NULL, cols = NULL) {
  # Get the full data first
  if (!is.null(backend$data)) {
    vec <- backend$data
  } else {
    # Load data on demand
    mask <- if (is.character(backend$mask_source)) {
      neuroim2::read_vol(backend$mask_source)
    } else {
      backend$mask_source
    }

    vec <- if (is.character(backend$source)) {
      tryCatch(
        neuroim2::read_vec(backend$source, mask = mask, mode = backend$mode),
        error = function(e) {
          stop_fmridataset(
            fmridataset_error_backend_io,
            message = sprintf("Failed to read data: %s", e$message),
            file = backend$source,
            operation = "read"
          )
        }
      )
    } else {
      if (length(backend$source) > 1) {
        do.call(neuroim2::NeuroVecSeq, backend$source)
      } else {
        backend$source[[1]]
      }
    }
  }

  # Extract data matrix in timepoints × voxels format
  mask_vec <- backend_get_mask(backend)
  voxel_indices <- which(mask_vec)

  # Use neuroim2::series to extract time series data
  data_matrix <- neuroim2::series(vec, voxel_indices)

  # Apply subsetting if requested
  if (!is.null(rows)) {
    data_matrix <- data_matrix[rows, , drop = FALSE]
  }

  if (!is.null(cols)) {
    data_matrix <- data_matrix[, cols, drop = FALSE]
  }

  data_matrix
}

#' @export
backend_get_metadata.nifti_backend <- function(backend) {
  if (!is.null(backend$metadata)) {
    return(backend$metadata)
  }

  # Extract metadata from first source
  if (is.character(backend$source)) {
    vec <- tryCatch(
      neuroim2::read_vec(backend$source[1]),
      error = function(e) {
        stop_fmridataset(
          fmridataset_error_backend_io,
          message = sprintf("Failed to read metadata: %s", e$message),
          file = backend$source[1],
          operation = "read_header"
        )
      }
    )
  } else {
    vec <- if (is.list(backend$source)) backend$source[[1]] else backend$source
  }

  # Extract key metadata
  metadata <- list(
    affine = neuroim2::trans(vec),
    voxel_dims = neuroim2::spacing(vec),
    space = neuroim2::space(vec),
    origin = neuroim2::origin(vec)
  )

  # Cache for future use
  backend$metadata <- metadata
  metadata
}
</file>

<file path="R/storage_backend.R">
#' Storage Backend S3 Contract
#'
#' @description
#' Defines the S3 generic functions that all storage backends must implement.
#' This provides a pluggable architecture for different data storage formats.
#'
#' @details
#' A storage backend is responsible for:
#' - Managing stateful resources (file handles, connections)
#' - Providing dimension information
#' - Reading data in canonical timepoints × voxels orientation
#' - Providing mask information
#' - Extracting metadata
#'
#' @name storage-backend
#' @keywords internal
NULL

#' Open a Storage Backend
#'
#' @description
#' Opens a storage backend and acquires any necessary resources (e.g., file handles).
#' Stateless backends can implement this as a no-op.
#'
#' @param backend A storage backend object
#' @return The backend object (possibly modified with state)
#' @export
#' @keywords internal
backend_open <- function(backend) {
  UseMethod("backend_open")
}

#' Close a Storage Backend
#'
#' @description
#' Closes a storage backend and releases any resources.
#' Stateless backends can implement this as a no-op.
#'
#' @param backend A storage backend object
#' @return NULL (invisibly)
#' @export
#' @keywords internal
backend_close <- function(backend) {
  UseMethod("backend_close")
}

#' Get Dimensions from Backend
#'
#' @description
#' Returns the dimensions of the data stored in the backend.
#'
#' @param backend A storage backend object
#' @return A named list with elements:
#'   - spatial: numeric vector of length 3 (x, y, z dimensions)
#'   - time: integer, number of timepoints
#' @export
#' @keywords internal
backend_get_dims <- function(backend) {
  UseMethod("backend_get_dims")
}

#' Get Mask from Backend
#'
#' @description
#' Returns a logical mask indicating which voxels contain valid data.
#'
#' @param backend A storage backend object
#' @return A logical vector satisfying:
#'   - length(mask) == prod(backend_get_dims(backend)$spatial)
#'   - sum(mask) > 0 (no empty masks allowed)
#'   - No NA values allowed
#' @export
#' @keywords internal
backend_get_mask <- function(backend) {
  UseMethod("backend_get_mask")
}

#' Get Data from Backend
#'
#' @description
#' Reads data from the backend in canonical timepoints × voxels orientation.
#'
#' @param backend A storage backend object
#' @param rows Integer vector of row indices (timepoints) to read, or NULL for all
#' @param cols Integer vector of column indices (voxels) to read, or NULL for all
#' @return A matrix in timepoints × voxels orientation
#' @export
#' @keywords internal
backend_get_data <- function(backend, rows = NULL, cols = NULL) {
  UseMethod("backend_get_data")
}

#' Get Metadata from Backend
#'
#' @description
#' Returns metadata associated with the data (e.g., affine matrix, voxel dimensions).
#'
#' @param backend A storage backend object
#' @return A list containing neuroimaging metadata, which may include:
#'   - affine: 4x4 affine transformation matrix
#'   - voxel_dims: numeric vector of voxel dimensions
#'   - intent_code: NIfTI intent code
#'   - Additional format-specific metadata
#' @export
#' @keywords internal
backend_get_metadata <- function(backend) {
  UseMethod("backend_get_metadata")
}

#' Validate Backend Implementation
#'
#' @description
#' Validates that a backend implements the required contract correctly.
#'
#' @param backend A storage backend object
#' @return TRUE if valid, otherwise throws an error
#' @keywords internal
validate_backend <- function(backend) {
  backend <- backend_open(backend)
  on.exit(backend_close(backend))

  dims <- backend_get_dims(backend)
  if (!is.list(dims) || !all(c("spatial", "time") %in% names(dims))) {
    stop_fmridataset(
      fmridataset_error_config,
      "backend_get_dims must return a list with 'spatial' and 'time' elements"
    )
  }

  if (length(dims$spatial) != 3 || !is.numeric(dims$spatial)) {
    stop_fmridataset(
      fmridataset_error_config,
      "spatial dimensions must be a numeric vector of length 3"
    )
  }

  if (!is.numeric(dims$time) || length(dims$time) != 1 || dims$time < 1) {
    stop_fmridataset(
      fmridataset_error_config,
      "time dimension must be a positive integer"
    )
  }

  mask <- backend_get_mask(backend)
  expected_length <- prod(dims$spatial)

  if (!is.logical(mask)) {
    stop_fmridataset(
      fmridataset_error_config,
      "backend_get_mask must return a logical vector"
    )
  }

  if (length(mask) != expected_length) {
    stop_fmridataset(
      fmridataset_error_config,
      sprintf(
        "mask length (%d) must equal prod(spatial dims) (%d)",
        length(mask), expected_length
      )
    )
  }

  if (sum(mask) == 0) {
    stop_fmridataset(
      fmridataset_error_config,
      "mask must contain at least one TRUE value"
    )
  }

  if (any(is.na(mask))) {
    stop_fmridataset(
      fmridataset_error_config,
      "mask cannot contain NA values"
    )
  }

  TRUE
}
</file>

<file path="tests/testthat/test_backend_chunking.R">
test_that("backend chunking doesn't load full dataset into memory", {
  skip("Memory benchmarking can be unreliable in test environments")
  skip_if_not_installed("bench")
  skip_if_not_installed("neuroim2")

  # Create a moderately large test dataset
  n_timepoints <- 300
  n_voxels <- 10000
  spatial_dims <- c(100, 100, 1)

  # Create test data
  test_data <- matrix(rnorm(n_timepoints * n_voxels),
    nrow = n_timepoints,
    ncol = n_voxels
  )

  # Create mask
  mask <- rep(TRUE, n_voxels)

  # Create backend
  backend <- matrix_backend(
    data_matrix = test_data,
    mask = mask,
    spatial_dims = spatial_dims
  )

  # Create dataset
  dset <- fmri_dataset(
    scans = backend,
    TR = 2,
    run_length = c(150, 150)
  )

  # Benchmark chunked iteration
  bench_result <- bench::mark(
    chunked = {
      chunks <- data_chunks(dset, nchunks = 10)
      total <- 0
      for (i in 1:10) {
        chunk <- chunks$nextElem()
        total <- total + sum(chunk$data)
      }
      total
    },
    iterations = 1,
    check = FALSE
  )

  # Memory allocated should be much less than full dataset size
  full_size <- object.size(test_data)
  allocated_mem <- bench_result$mem_alloc[[1]]

  # The allocated memory should be closer to chunk size than full dataset
  # Allowing for some overhead, but should be less than 50% of full size
  expect_true(
    allocated_mem < 0.5 * full_size,
    info = sprintf(
      "Allocated memory (%s) should be much less than full dataset (%s)",
      format(allocated_mem, units = "auto"),
      format(full_size, units = "auto")
    )
  )
})

test_that("backend chunking produces correct results with matrix backend", {
  skip_if_not_installed("neuroim2")

  # Create small test dataset
  test_data <- matrix(1:100, nrow = 10, ncol = 10)
  mask <- rep(TRUE, 10)

  backend <- matrix_backend(
    data_matrix = test_data,
    mask = mask,
    spatial_dims = c(10, 1, 1)
  )

  dset <- fmri_dataset(
    scans = backend,
    TR = 2,
    run_length = 10
  )

  # Test single chunk
  chunks <- data_chunks(dset, nchunks = 1)
  chunk <- chunks$nextElem()

  expect_equal(chunk$data, test_data)
  expect_equal(chunk$voxel_ind, 1:10)
  expect_equal(chunk$row_ind, 1:10)

  # Test multiple chunks
  chunks <- data_chunks(dset, nchunks = 2)
  chunk1 <- chunks$nextElem()
  chunk2 <- chunks$nextElem()

  # Verify chunks partition the data correctly
  expect_equal(ncol(chunk1$data) + ncol(chunk2$data), ncol(test_data))

  # Test runwise chunks
  dset2 <- fmri_dataset(
    scans = backend,
    TR = 2,
    run_length = c(5, 5)
  )

  chunks <- data_chunks(dset2, runwise = TRUE)
  chunk1 <- chunks$nextElem()
  chunk2 <- chunks$nextElem()

  expect_equal(nrow(chunk1$data), 5)
  expect_equal(nrow(chunk2$data), 5)
  expect_equal(chunk1$row_ind, 1:5)
  expect_equal(chunk2$row_ind, 6:10)
})

test_that("backend chunking works with file-based datasets", {
  # Use matrix backend to simulate file-based behavior
  # Create test data
  n_time <- 100
  n_voxels <- 1000
  test_data <- matrix(rnorm(n_time * n_voxels), nrow = n_time, ncol = n_voxels)

  backend <- matrix_backend(
    data_matrix = test_data,
    spatial_dims = c(10, 10, 10)
  )

  dset <- fmri_dataset(
    scans = backend,
    TR = 2,
    run_length = c(50, 50)
  )

  # Test different chunking strategies
  # 1. Single chunk
  chunks <- data_chunks(dset, nchunks = 1)
  single_chunk <- chunks$nextElem()
  # Single chunk should have all timepoints
  expect_equal(nrow(single_chunk$data), 100)
  expect_equal(ncol(single_chunk$data), 1000)

  # 2. Multiple chunks
  chunks <- data_chunks(dset, nchunks = 5)
  all_chunks <- list()
  for (i in 1:5) {
    all_chunks[[i]] <- chunks$nextElem()
  }

  # Verify chunks partition the voxels
  total_voxels <- sum(sapply(all_chunks, function(x) ncol(x$data)))
  expect_equal(total_voxels, 1000)

  # 3. Runwise chunks
  chunks <- data_chunks(dset, runwise = TRUE)
  run1 <- chunks$nextElem()
  run2 <- chunks$nextElem()

  expect_equal(nrow(run1$data), 50)
  expect_equal(nrow(run2$data), 50)
  expect_equal(ncol(run1$data), 1000)
  expect_equal(ncol(run2$data), 1000)
})

test_that("chunking correctly handles subsetting", {
  # Create data with known pattern
  n_time <- 30
  n_voxels <- 20
  test_data <- matrix(0, nrow = n_time, ncol = n_voxels)

  # Fill with pattern: column i contains value i
  for (i in 1:n_voxels) {
    test_data[, i] <- i
  }

  backend <- matrix_backend(test_data)
  dset <- fmri_dataset(backend, TR = 2, run_length = n_time)

  # Test that chunks contain correct voxel subsets
  chunks <- data_chunks(dset, nchunks = 4)

  chunk1 <- chunks$nextElem()
  # First chunk should have first set of voxels
  expect_true(all(chunk1$data[1, ] %in% 1:5))

  chunk2 <- chunks$nextElem()
  # Second chunk should have next set of voxels
  expect_true(all(chunk2$data[1, ] %in% 6:10))
})

test_that("chunk iterator stops correctly", {
  test_data <- matrix(1:100, nrow = 10, ncol = 10)
  backend <- matrix_backend(test_data)
  dset <- fmri_dataset(backend, TR = 2, run_length = 10)

  chunks <- data_chunks(dset, nchunks = 3)

  # Get all chunks
  for (i in 1:3) {
    chunk <- chunks$nextElem()
    expect_s3_class(chunk, "data_chunk")
  }

  # Next call should error
  expect_error(chunks$nextElem(), "StopIteration")
})

test_that("data_chunks preserves chunk metadata", {
  test_data <- matrix(rnorm(300), nrow = 30, ncol = 10)
  backend <- matrix_backend(test_data)
  dset <- fmri_dataset(backend, TR = 2, run_length = c(15, 15))

  # Test runwise chunks
  chunks <- data_chunks(dset, runwise = TRUE)

  chunk1 <- chunks$nextElem()
  expect_equal(chunk1$chunk_num, 1)
  expect_true(is.numeric(chunk1$row_ind))
  expect_true(length(chunk1$row_ind) > 0)
  expect_equal(ncol(chunk1$data), 10)

  chunk2 <- chunks$nextElem()
  expect_equal(chunk2$chunk_num, 2)
  expect_true(is.numeric(chunk2$row_ind))
  expect_true(length(chunk2$row_ind) > 0)
})
</file>

<file path="tests/testthat/test_backward_compatibility.R">
test_that("legacy file-based interface still works", {
  skip_if_not_installed("neuroim2")

  # Create temporary test files
  temp_dir <- tempdir()

  # Create mock scan data
  scan_data <- array(rnorm(10 * 10 * 10 * 20), c(10, 10, 10, 20))
  mask_data <- array(sample(0:1, 10 * 10 * 10, replace = TRUE), c(10, 10, 10))

  # Mock file paths
  scan_file <- file.path(temp_dir, "test_scan.nii")
  mask_file <- file.path(temp_dir, "test_mask.nii")

  # Test that legacy interface creates a backend internally
  with_mocked_bindings(
    file.exists = function(x) TRUE,
    .package = "base",
    code = {
      with_mocked_bindings(
        read_vol = function(x) {
          if (grepl("mask", x)) mask_data else NULL
        },
        read_vec = function(x, ...) {
          structure(scan_data, class = c("NeuroVec", "array"))
        },
        .package = "neuroim2",
        {
          # Create dataset using legacy interface
          dset <- fmri_dataset(
            scans = "test_scan.nii",
            mask = "test_mask.nii",
            TR = 2,
            run_length = 20,
            base_path = temp_dir
          )

          expect_s3_class(dset, "fmri_dataset")
          expect_s3_class(dset$backend, "nifti_backend")

          # Verify the backend was created with correct paths
          expect_equal(dset$backend$source, scan_file)
          expect_equal(dset$backend$mask_source, mask_file)
        }
      )
    }
  )
})

test_that("matrix_dataset continues to work without backend", {
  # Create test matrix
  test_data <- matrix(rnorm(100), nrow = 10, ncol = 10)

  # Create matrix dataset using original interface
  dset <- matrix_dataset(
    datamat = test_data,
    TR = 2,
    run_length = 10
  )

  expect_s3_class(dset, "matrix_dataset")
  expect_s3_class(dset, "fmri_dataset")

  # Verify it has the original structure (no backend)
  expect_null(dset$backend)
  expect_equal(dset$datamat, test_data)
  expect_equal(dset$TR, 2)
  expect_equal(length(dset$mask), 10)

  # Test that data access methods still work
  retrieved_data <- get_data_matrix(dset)
  expect_equal(retrieved_data, test_data)

  mask <- get_mask(dset)
  expect_equal(mask, rep(1, 10))
})

test_that("fmri_mem_dataset continues to work", {
  skip_if_not_installed("neuroim2")

  # Create mock NeuroVec and mask
  dims <- c(5, 5, 5, 10)
  mock_vec <- structure(
    array(rnorm(prod(dims)), dims),
    class = c("NeuroVec", "array"),
    dim = dims
  )

  mock_mask <- structure(
    array(1, dims[1:3]),
    class = c("NeuroVol", "array"),
    dim = dims[1:3]
  )

  # Create fmri_mem_dataset
  dset <- fmri_mem_dataset(
    scans = list(mock_vec),
    mask = mock_mask,
    TR = 2
  )

  expect_s3_class(dset, "fmri_mem_dataset")
  expect_s3_class(dset, "fmri_dataset")

  # Verify it doesn't have a backend
  expect_null(dset$backend)

  # Test data access
  with_mocked_bindings(
    NeuroVecSeq = function(...) mock_vec,
    series = function(vec, indices) {
      matrix(rnorm(length(indices) * dims[4]),
        nrow = dims[4],
        ncol = length(indices)
      )
    },
    .package = "neuroim2",
    {
      data <- get_data(dset)
      expect_s3_class(data, "NeuroVec")

      data_matrix <- get_data_matrix(dset)
      expect_true(is.matrix(data_matrix))
    }
  )
})

test_that("latent_dataset continues to work", {
  skip_if(
    !requireNamespace("fmristore", quietly = TRUE),
    "fmristore not available"
  )

  # Create mock latent data
  basis <- matrix(rnorm(100), nrow = 10, ncol = 10)
  loadings <- matrix(rnorm(1000), nrow = 100, ncol = 10)

  # Create mock LatentNeuroVec
  mock_lvec <- structure(
    list(
      basis = basis,
      loadings = loadings,
      mask = rep(TRUE, 100)
    ),
    class = "LatentNeuroVec"
  )

  # Skip this test as it requires fmristore and complex mocking
  skip("Latent dataset requires fmristore package")
})

test_that("all dataset types work with data_chunks", {
  # Test matrix_dataset
  mat_data <- matrix(1:100, nrow = 10, ncol = 10)
  mat_dset <- matrix_dataset(mat_data, TR = 2, run_length = 10)

  chunks <- data_chunks(mat_dset, nchunks = 2)
  chunk1 <- chunks$nextElem()
  expect_s3_class(chunk1, "data_chunk")
  expect_true(is.matrix(chunk1$data))

  # Test with backend
  backend <- matrix_backend(mat_data)
  backend_dset <- fmri_dataset(backend, TR = 2, run_length = 10)

  chunks2 <- data_chunks(backend_dset, nchunks = 2)
  chunk2 <- chunks2$nextElem()
  expect_s3_class(chunk2, "data_chunk")
  expect_true(is.matrix(chunk2$data))
})

test_that("conversion functions work with both old and new datasets", {
  # Create test data
  test_matrix <- matrix(rnorm(200), nrow = 20, ncol = 10)

  # Old style matrix dataset
  old_dset <- matrix_dataset(test_matrix, TR = 2, run_length = 20)

  # New style with backend
  backend <- matrix_backend(test_matrix)
  new_dset <- fmri_dataset(backend, TR = 2, run_length = 20)

  # Test as.matrix_dataset on both
  old_converted <- as.matrix_dataset(old_dset)
  expect_s3_class(old_converted, "matrix_dataset")
  expect_equal(old_converted$datamat, test_matrix)

  # For new style, we need to implement conversion
  # This would need to be added to conversions.R
  # For now, just verify the old style works
})

test_that("print methods work for all dataset types", {
  # Matrix dataset
  mat_dset <- matrix_dataset(
    matrix(1:100, 10, 10),
    TR = 2,
    run_length = 10
  )
  expect_output(print(mat_dset), "fMRI Dataset")

  # Backend-based dataset
  backend <- matrix_backend(matrix(1:100, 10, 10))
  backend_dset <- fmri_dataset(backend, TR = 2, run_length = 10)
  expect_output(print(backend_dset), "fMRI Dataset")
})

test_that("sampling frame works with all dataset types", {
  # Test with various dataset types
  datasets <- list(
    matrix_dataset(matrix(1:100, 10, 10), TR = 2, run_length = 10),
    matrix_dataset(matrix(1:200, 20, 10), TR = 1.5, run_length = c(10, 10))
  )

  for (dset in datasets) {
    frame <- dset$sampling_frame
    expect_s3_class(frame, "sampling_frame")
    expect_equal(get_TR(frame), dset$TR)
    expect_equal(sum(get_run_lengths(frame)), nrow(dset$datamat))
  }
})
</file>

<file path="tests/testthat/test_config.R">
library(fmridataset)

test_that("default_config returns expected defaults", {
  cfg <- fmridataset:::default_config()
  expect_equal(cfg$cmd_flags, "")
  expect_equal(cfg$jobs, 1)
})


test_that("read_fmri_config parses configuration files", {
  temp_dir <- tempdir()
  event_file <- file.path(temp_dir, "events.tsv")
  write.table(data.frame(onset = c(1, 2, 3), duration = c(0.5, 0.5, 0.5)),
    event_file,
    row.names = FALSE, sep = "\t"
  )

  cfg_file <- file.path(temp_dir, "config.R")
  cat(
    "scans <- c('scan1.nii', 'scan2.nii')\n",
    "TR <- 2\n",
    "mask <- 'mask.nii'\n",
    "run_length <- c(2,2)\n",
    "event_model <- 'model'\n",
    "event_table <- 'events.tsv'\n",
    "block_column <- 'run'\n",
    "baseline_model <- 'hrf'\n",
    file = cfg_file
  )

  cfg <- read_fmri_config(cfg_file, base_path = temp_dir)
  expect_s3_class(cfg, "fmri_config")
  expect_equal(cfg$TR, 2)
  expect_equal(cfg$run_length, c(2, 2))
  expect_equal(nrow(cfg$design), 3)
  expect_equal(cfg$base_path, temp_dir)
})
</file>

<file path="tests/testthat/test_conversions.R">
test_that("as.matrix_dataset works with all dataset types", {
  # Test with matrix_dataset (should return itself)
  mat_data <- matrix(rnorm(200), nrow = 20, ncol = 10)
  mat_dset <- matrix_dataset(mat_data, TR = 2, run_length = 20)

  converted <- as.matrix_dataset(mat_dset)
  expect_identical(converted, mat_dset)
  expect_equal(converted$datamat, mat_data)

  # Test with fmri_mem_dataset
  skip_if_not_installed("neuroim2")

  dims <- c(5, 5, 5, 20)
  mock_vec <- structure(
    array(rnorm(prod(dims)), dims),
    class = c("NeuroVec", "array"),
    dim = dims
  )

  mock_mask <- structure(
    array(TRUE, dims[1:3]),
    class = c("NeuroVol", "array"),
    dim = dims[1:3]
  )

  mem_dset <- fmri_mem_dataset(
    scans = list(mock_vec),
    mask = mock_mask,
    TR = 2
  )

  # Mock the series function for testing
  with_mocked_bindings(
    series = function(vec, indices) {
      matrix(rnorm(length(indices) * dims[4]),
        nrow = dims[4],
        ncol = length(indices)
      )
    },
    .package = "neuroim2",
    {
      converted_mem <- as.matrix_dataset(mem_dset)
      expect_s3_class(converted_mem, "matrix_dataset")
      expect_equal(nrow(converted_mem$datamat), 20)
      expect_equal(ncol(converted_mem$datamat), 125) # 5*5*5
    }
  )

  # Test with backend-based fmri_dataset
  backend <- matrix_backend(mat_data)
  backend_dset <- fmri_dataset(backend, TR = 2, run_length = 20)

  # For now, this should use the fmri_file_dataset method
  # which will call get_data_matrix
  converted_backend <- as.matrix_dataset(backend_dset)
  expect_s3_class(converted_backend, "matrix_dataset")
  expect_equal(converted_backend$datamat, mat_data)
})

test_that("conversion preserves essential properties", {
  # Create dataset with specific properties
  test_data <- matrix(1:300, nrow = 30, ncol = 10)
  event_table <- data.frame(
    onset = c(5, 15, 25),
    duration = rep(2, 3),
    condition = c("A", "B", "A")
  )

  original <- matrix_dataset(
    datamat = test_data,
    TR = 1.5,
    run_length = c(15, 15),
    event_table = event_table
  )

  # Convert to itself
  converted <- as.matrix_dataset(original)

  # Check all properties preserved
  expect_equal(converted$TR, original$TR)
  expect_equal(converted$nruns, original$nruns)
  expect_equal(converted$event_table, original$event_table)
  expect_equal(converted$sampling_frame$blocklens, original$sampling_frame$blocklens)
  expect_equal(length(converted$mask), ncol(test_data))
})
</file>

<file path="tests/testthat/test_data_chunks.R">
# Test data chunking functionality

library(fmridataset)

test_that("matrix_dataset chunking works correctly", {
  # Create test data
  n_time <- 100
  n_vox <- 10
  n_runs <- 2

  Y <- matrix(rnorm(n_time * n_vox), n_time, n_vox)
  run_length <- rep(n_time / n_runs, n_runs)

  dset <- matrix_dataset(Y, TR = 1, run_length = run_length)

  # Test runwise chunking
  chunks <- data_chunks(dset, runwise = TRUE)
  expect_s3_class(chunks, "chunkiter")

  # Should have 2 chunks (one per run)
  expect_equal(chunks$nchunks, 2)

  # Collect all chunks
  chunk_list <- list()
  for (i in 1:chunks$nchunks) {
    chunk_list[[i]] <- chunks$nextElem()
  }

  expect_equal(length(chunk_list), n_runs)

  # Check first chunk structure
  chunk1 <- chunk_list[[1]]
  expect_s3_class(chunk1, "data_chunk")
  expect_true(all(c("data", "voxel_ind", "row_ind", "chunk_num") %in% names(chunk1)))

  # Check dimensions
  expect_equal(nrow(chunk1$data), n_time / n_runs)
  expect_equal(ncol(chunk1$data), n_vox)
  expect_equal(chunk1$chunk_num, 1)
  expect_equal(chunk1$row_ind, 1:(n_time / n_runs))
})

test_that("matrix_dataset single chunk works", {
  n_time <- 50
  n_vox <- 5

  Y <- matrix(rnorm(n_time * n_vox), n_time, n_vox)
  dset <- matrix_dataset(Y, TR = 1, run_length = n_time)

  chunks <- data_chunks(dset, nchunks = 1)
  chunk <- chunks$nextElem()

  expect_s3_class(chunk, "data_chunk")
  expect_equal(dim(chunk$data), dim(Y))
  expect_equal(chunk$chunk_num, 1)
  expect_equal(chunk$voxel_ind, 1:n_vox)
})

test_that("matrix_dataset voxel chunking works", {
  n_time <- 50
  n_vox <- 20

  Y <- matrix(rnorm(n_time * n_vox), n_time, n_vox)
  dset <- matrix_dataset(Y, TR = 1, run_length = n_time)

  # Split into 4 chunks
  chunks <- data_chunks(dset, nchunks = 4)
  expect_equal(chunks$nchunks, 4)

  chunk_list <- list()
  for (i in 1:chunks$nchunks) {
    chunk_list[[i]] <- chunks$nextElem()
  }

  expect_equal(length(chunk_list), 4)

  # Check that all voxels are covered
  all_vox_ind <- unlist(lapply(chunk_list, function(ch) ch$voxel_ind))
  expect_equal(sort(all_vox_ind), 1:n_vox)

  # Check chunk dimensions
  for (i in 1:4) {
    expect_equal(nrow(chunk_list[[i]]$data), n_time)
    expect_true(ncol(chunk_list[[i]]$data) > 0)
    expect_equal(chunk_list[[i]]$chunk_num, i)
  }
})

test_that("data_chunk object has correct structure", {
  mat <- matrix(1:12, 3, 4)
  # Use the public interface instead of internal function
  # Create a simple dataset and extract a chunk to test the structure
  dset <- matrix_dataset(mat, TR = 1, run_length = 3)
  chunks <- data_chunks(dset, nchunks = 1)
  chunk <- chunks$nextElem()

  expect_s3_class(chunk, "data_chunk")
  expect_identical(chunk$data, mat)
  expect_equal(chunk$voxel_ind, 1:4)
  expect_equal(chunk$row_ind, 1:3)
  expect_equal(chunk$chunk_num, 1)
})
</file>

<file path="tests/testthat/test_error_handling.R">
test_that("error handling for invalid inputs", {
  # Test invalid TR
  expect_error(
    matrix_dataset(matrix(1:100, 10, 10), TR = -1, run_length = 10),
    "TR"
  )

  # Test run_length mismatch
  expect_error(
    matrix_dataset(matrix(1:100, 10, 10), TR = 2, run_length = 20),
    "sum\\(run_length\\) not equal to nrow\\(datamat\\)"
  )

  # Test invalid backend source - fix order of validation
  expect_error(
    nifti_backend(source = 123, mask_source = "mask.nii"),
    "source must be character vector"
  )

  # Test invalid spatial dimensions in matrix_backend
  expect_error(
    matrix_backend(matrix(1:100, 10, 10), spatial_dims = c(5, 5)),
    "spatial_dims must be a numeric vector of length 3"
  )

  # Test spatial dims product mismatch
  expect_error(
    matrix_backend(matrix(1:100, 10, 10), spatial_dims = c(2, 2, 2)),
    "Product of spatial_dims .* must equal number of voxels"
  )
})

test_that("backend error propagation", {
  # Create a matrix backend that works and then test error propagation through dataset validation
  test_matrix <- matrix(1:100, 10, 10)
  backend <- matrix_backend(test_matrix)

  # Mock a failing get_dims function to test error propagation
  with_mocked_bindings(
    backend_get_dims = function(x) {
      stop("Simulated backend failure")
    },
    .package = "fmridataset",
    {
      expect_error(
        fmri_dataset(backend, TR = 2, run_length = 10),
        "Simulated backend failure"
      )
    }
  )
})

test_that("edge cases in chunking", {
  # Test with single voxel
  single_voxel <- matrix_backend(matrix(1:10, nrow = 10, ncol = 1))
  dset <- fmri_dataset(single_voxel, TR = 2, run_length = 10)

  chunks <- data_chunks(dset, nchunks = 1)
  chunk <- chunks$nextElem()
  expect_equal(ncol(chunk$data), 1)
  expect_equal(nrow(chunk$data), 10)

  # Test with more chunks than voxels
  small_data <- matrix_backend(matrix(1:30, nrow = 10, ncol = 3))
  dset2 <- fmri_dataset(small_data, TR = 2, run_length = 10)

  # Should handle gracefully - nchunks will be capped to number of voxels
  suppressWarnings({
    chunks2 <- data_chunks(dset2, nchunks = 10)
  })

  # Get all chunks (should be 3 since we have 3 voxels)
  all_chunks <- list()
  for (i in 1:3) {
    all_chunks[[i]] <- chunks2$nextElem()
  }

  # Verify we got all voxels
  total_voxels <- sum(sapply(all_chunks, function(x) ncol(x$data)))
  expect_equal(total_voxels, 3)
})

test_that("validate_backend catches all error conditions", {
  # Test missing methods - create a backend with a class that has no methods
  incomplete_backend <- structure(
    list(),
    class = c("nonexistent_backend_class", "storage_backend")
  )

  expect_error(
    validate_backend(incomplete_backend),
    class = "error"
  )

  # Test backend returning wrong dimension format
  test_matrix <- matrix(1:100, 10, 10)
  backend <- matrix_backend(test_matrix)

  # Mock backend_get_dims to return wrong format
  with_mocked_bindings(
    backend_get_dims = function(x) {
      # Wrong format - should be list with spatial and time
      c(10, 10, 10, 100)
    },
    .package = "fmridataset",
    {
      expect_error(
        validate_backend(backend),
        "must return a list with 'spatial' and 'time' elements"
      )
    }
  )
})

test_that("mask validation in backends", {
  # Test non-logical mask
  expect_error(
    {
      backend <- matrix_backend(
        matrix(1:100, 10, 10),
        mask = 1:10 # Should be logical
      )
    },
    "mask must be a logical vector"
  )

  # Test NA in mask by mocking the mask function
  test_matrix <- matrix(1:100, 10, 10)
  backend <- matrix_backend(test_matrix)

  # Mock mask with NA values
  bad_mask <- rep(TRUE, 10)
  bad_mask[5] <- NA

  with_mocked_bindings(
    backend_get_mask = function(x) bad_mask,
    .package = "fmridataset",
    {
      expect_error(
        validate_backend(backend),
        "missing value where TRUE/FALSE needed"
      )
    }
  )
})
</file>

<file path="tests/testthat/test_integration.R">
test_that("complete workflow with matrix backend", {
  # 1. Create data
  n_timepoints <- 100
  n_voxels <- 50
  n_runs <- 2

  # Generate synthetic fMRI data
  set.seed(123)
  time_series <- matrix(0, nrow = n_timepoints, ncol = n_voxels)

  # Add signal to some voxels
  signal_voxels <- 1:10
  for (v in signal_voxels) {
    time_series[, v] <- sin(seq(0, 4 * pi, length.out = n_timepoints)) +
      rnorm(n_timepoints, sd = 0.5)
  }

  # Add noise to other voxels
  noise_voxels <- 11:n_voxels
  for (v in noise_voxels) {
    time_series[, v] <- rnorm(n_timepoints)
  }

  # 2. Create backend
  backend <- matrix_backend(
    data_matrix = time_series,
    spatial_dims = c(10, 5, 1),
    metadata = list(
      study = "test_study",
      subject = "sub01"
    )
  )

  # 3. Create dataset
  dataset <- fmri_dataset(
    scans = backend,
    TR = 2,
    run_length = c(50, 50),
    event_table = data.frame(
      onset = c(10, 30, 60, 80),
      duration = c(5, 5, 5, 5),
      condition = c("A", "B", "A", "B"),
      run = c(1, 1, 2, 2)
    )
  )

  # 4. Test basic accessors
  expect_equal(get_TR(dataset$sampling_frame), 2)
  expect_equal(n_runs(dataset$sampling_frame), 2)
  expect_equal(n_timepoints(dataset$sampling_frame), n_timepoints)

  # 5. Test data retrieval
  full_data <- get_data_matrix(dataset)
  expect_equal(dim(full_data), c(n_timepoints, n_voxels))
  expect_equal(full_data, time_series)

  # 6. Test mask
  mask <- get_mask(dataset)
  expect_true(is.array(mask))
  expect_equal(dim(mask), c(10, 5, 1))

  # 7. Test chunking
  chunks <- data_chunks(dataset, nchunks = 5)
  chunk_list <- list()
  for (i in 1:5) {
    chunk_list[[i]] <- chunks$nextElem()
  }

  # Verify chunks cover all voxels
  all_voxel_inds <- unlist(lapply(chunk_list, function(x) x$voxel_ind))
  expect_equal(sort(unique(all_voxel_inds)), 1:n_voxels)

  # 8. Test runwise processing
  run_chunks <- data_chunks(dataset, runwise = TRUE)
  run1 <- run_chunks$nextElem()
  run2 <- run_chunks$nextElem()

  expect_equal(nrow(run1$data), 50)
  expect_equal(nrow(run2$data), 50)

  # 9. Test metadata preservation
  metadata <- backend_get_metadata(dataset$backend)
  expect_equal(metadata$study, "test_study")
  expect_equal(metadata$subject, "sub01")
})

test_that("complete workflow with file-based backend", {
  skip_if_not_installed("neuroim2")

  # Mock file system
  temp_dir <- tempdir()
  scan_files <- file.path(temp_dir, c("run1.nii", "run2.nii"))
  mask_file <- file.path(temp_dir, "mask.nii")

  # Create event data
  events <- data.frame(
    onset = c(5, 15, 25, 35),
    duration = rep(2, 4),
    trial_type = c("left", "right", "left", "right"),
    run = c(1, 1, 2, 2)
  )

  with_mocked_bindings(
    file.exists = function(x) TRUE,
    .package = "base",
    code = {
      with_mocked_bindings(
        read_vol = function(x) {
          # Return mock mask
          structure(
            array(c(rep(1, 500), rep(0, 500)), c(10, 10, 10)),
            class = c("NeuroVol", "array"),
            dim = c(10, 10, 10)
          )
        },
        read_vec = function(files, ...) {
          # Return mock 4D data
          n_files <- if (is.character(files)) length(files) else 1
          total_time <- n_files * 50
          structure(
            array(rnorm(10 * 10 * 10 * total_time), c(10, 10, 10, total_time)),
            class = c("NeuroVec", "array"),
            dim = c(10, 10, 10, total_time)
          )
        },
        trans = function(x) diag(4),
        spacing = function(x) c(2, 2, 2),
        space = function(x) "MNI",
        origin = function(x) c(0, 0, 0),
        series = function(vec, indices) {
          # Return time series for selected voxels
          n_time <- dim(vec)[4]
          matrix(rnorm(n_time * length(indices)),
            nrow = n_time,
            ncol = length(indices)
          )
        },
        .package = "neuroim2",
        {
          # Create dataset using file paths
          dataset <- fmri_dataset(
            scans = scan_files,
            mask = "mask.nii",
            TR = 2.5,
            run_length = c(50, 50),
            event_table = events,
            base_path = temp_dir,
            preload = FALSE
          )

          # Verify dataset structure
          expect_s3_class(dataset, "fmri_dataset")
          expect_s3_class(dataset$backend, "nifti_backend")

          # Test data access
          dims <- backend_get_dims(dataset$backend)
          expect_equal(dims$spatial, c(10, 10, 10))
          expect_equal(dims$time, 100)

          # Test metadata
          metadata <- backend_get_metadata(dataset$backend)
          expect_true("affine" %in% names(metadata))
          expect_equal(metadata$voxel_dims, c(2, 2, 2))

          # Test chunked processing with foreach
          if (requireNamespace("foreach", quietly = TRUE)) {
            chunks <- data_chunks(dataset, nchunks = 4)

            # Process chunks to compute mean activation
            results <- foreach::foreach(chunk = chunks, .combine = c) %do% {
              mean(chunk$data)
            }

            expect_length(results, 4)
            expect_true(all(is.numeric(results)))
          }
        }
      )
    }
  )
})

test_that("error handling in integrated workflow", {
  # Test various error conditions

  # 1. Invalid run length
  backend <- matrix_backend(matrix(1:100, 10, 10))
  expect_error(
    fmri_dataset(backend, TR = 2, run_length = 20),
    "Sum of run_length .* must equal total time points"
  )

  # 2. Backend validation failure
  backend <- matrix_backend(matrix(1:100, 10, 10))

  # Mock a failing mask (all FALSE)
  with_mocked_bindings(
    backend_get_mask = function(x) rep(FALSE, 10),
    .package = "fmridataset",
    {
      expect_error(
        fmri_dataset(backend, TR = 2, run_length = 10),
        "mask must contain at least one TRUE value"
      )
    }
  )
})

test_that("print and summary methods work in integrated workflow", {
  # Create a small dataset
  backend <- matrix_backend(
    matrix(rnorm(200), 20, 10),
    metadata = list(description = "Test dataset")
  )

  dataset <- fmri_dataset(
    backend,
    TR = 1.5,
    run_length = c(10, 10),
    event_table = data.frame(
      onset = c(5, 15),
      condition = c("A", "B")
    )
  )

  # Test print output
  output <- capture.output(print(dataset))
  expect_true(any(grepl("fMRI Dataset", output)))

  # Test sampling frame print
  frame_output <- capture.output(print(dataset$sampling_frame))
  expect_true(any(grepl("Sampling Frame", frame_output)))
  expect_true(any(grepl("TR: 1.5", frame_output)))
})

test_that("conversion between dataset types", {
  # Start with matrix dataset
  mat_data <- matrix(rnorm(300), 30, 10)
  mat_dataset <- matrix_dataset(mat_data, TR = 2, run_length = 30)

  # Convert to itself (should return identical)
  converted <- as.matrix_dataset(mat_dataset)
  expect_identical(converted$datamat, mat_dataset$datamat)

  # Create backend-based dataset
  backend <- matrix_backend(mat_data)
  backend_dataset <- fmri_dataset(backend, TR = 2, run_length = 30)

  # Both should have same data access
  expect_equal(
    get_data_matrix(mat_dataset),
    get_data_matrix(backend_dataset)
  )
})
</file>

<file path="tests/testthat/test_matrix_backend.R">
test_that("matrix_backend validates inputs correctly", {
  # Test non-matrix input
  expect_error(
    matrix_backend(data_matrix = "not a matrix"),
    class = "fmridataset_error_config"
  )

  # Test invalid mask type
  test_matrix <- matrix(1:100, nrow = 10, ncol = 10)
  expect_error(
    matrix_backend(data_matrix = test_matrix, mask = "not logical"),
    class = "fmridataset_error_config"
  )

  # Test mask length mismatch
  expect_error(
    matrix_backend(data_matrix = test_matrix, mask = c(TRUE, FALSE)),
    "mask length .* must equal number of columns"
  )

  # Test invalid spatial dimensions
  expect_error(
    matrix_backend(data_matrix = test_matrix, spatial_dims = c(5, 5)),
    "spatial_dims must be a numeric vector of length 3"
  )

  # Test spatial dims product mismatch
  expect_error(
    matrix_backend(data_matrix = test_matrix, spatial_dims = c(2, 2, 2)),
    "Product of spatial_dims .* must equal number of voxels"
  )
})

test_that("matrix_backend creates valid backend with defaults", {
  test_matrix <- matrix(rnorm(100), nrow = 10, ncol = 10)

  backend <- matrix_backend(data_matrix = test_matrix)

  expect_s3_class(backend, "matrix_backend")
  expect_s3_class(backend, "storage_backend")

  # Check defaults
  expect_equal(length(backend$mask), 10)
  expect_true(all(backend$mask))
  expect_equal(backend$spatial_dims, c(10, 1, 1))
})

test_that("matrix_backend methods work correctly", {
  # Create test data
  n_time <- 20
  n_voxels <- 100
  test_matrix <- matrix(seq_len(n_time * n_voxels),
    nrow = n_time,
    ncol = n_voxels
  )

  # Create mask with some FALSE values
  mask <- rep(TRUE, n_voxels)
  mask[1:10] <- FALSE

  backend <- matrix_backend(
    data_matrix = test_matrix,
    mask = mask,
    spatial_dims = c(10, 10, 1),
    metadata = list(source = "test")
  )

  # Test open/close (should be no-ops)
  opened <- backend_open(backend)
  expect_identical(opened, backend)
  expect_silent(backend_close(backend))

  # Test dimensions
  dims <- backend_get_dims(backend)
  expect_equal(dims$spatial, c(10, 10, 1))
  expect_equal(dims$time, n_time)

  # Test mask
  retrieved_mask <- backend_get_mask(backend)
  expect_identical(retrieved_mask, mask)

  # Test full data retrieval
  data <- backend_get_data(backend)
  expect_identical(data, test_matrix)

  # Test metadata
  metadata <- backend_get_metadata(backend)
  expect_equal(metadata$source, "test")
})

test_that("matrix_backend data subsetting works", {
  # Create test data with known pattern
  test_matrix <- matrix(1:200, nrow = 20, ncol = 10)

  backend <- matrix_backend(data_matrix = test_matrix)

  # Test row subsetting
  rows_subset <- backend_get_data(backend, rows = 1:5)
  expect_equal(dim(rows_subset), c(5, 10))
  expect_equal(rows_subset[1, 1], 1)
  expect_equal(rows_subset[5, 1], 5)

  # Test column subsetting
  cols_subset <- backend_get_data(backend, cols = c(1, 3, 5))
  expect_equal(dim(cols_subset), c(20, 3))
  expect_equal(cols_subset[1, 1], 1)
  expect_equal(cols_subset[1, 2], 41) # Column 3
  expect_equal(cols_subset[1, 3], 81) # Column 5

  # Test both row and column subsetting
  both_subset <- backend_get_data(backend, rows = 1:5, cols = 1:3)
  expect_equal(dim(both_subset), c(5, 3))
  expect_equal(both_subset[1, 1], 1)
  expect_equal(both_subset[5, 3], 45)

  # Test single row/column (should not drop dimensions)
  single_row <- backend_get_data(backend, rows = 1)
  expect_equal(dim(single_row), c(1, 10))

  single_col <- backend_get_data(backend, cols = 1)
  expect_equal(dim(single_col), c(20, 1))
})

test_that("matrix_backend validates with validate_backend", {
  test_matrix <- matrix(rnorm(500), nrow = 50, ncol = 10)

  backend <- matrix_backend(
    data_matrix = test_matrix,
    spatial_dims = c(5, 2, 1)
  )

  # Should pass validation
  expect_true(validate_backend(backend))

  # Test with all FALSE mask (should fail validation)
  backend_fail <- matrix_backend(
    data_matrix = test_matrix,
    mask = rep(FALSE, 10),
    spatial_dims = c(5, 2, 1)
  )

  expect_error(
    validate_backend(backend_fail),
    "mask must contain at least one TRUE value"
  )
})

test_that("matrix_backend works with fmri_dataset", {
  # Create test data
  test_data <- matrix(rnorm(300), nrow = 30, ncol = 10)

  backend <- matrix_backend(
    data_matrix = test_data,
    spatial_dims = c(10, 1, 1)
  )

  # Create dataset using backend
  dataset <- fmri_dataset(
    scans = backend,
    TR = 2,
    run_length = 30
  )

  expect_s3_class(dataset, "fmri_dataset")
  expect_s3_class(dataset$backend, "matrix_backend")

  # Test that data access works
  data_retrieved <- get_data_matrix(dataset)
  expect_equal(dim(data_retrieved), dim(test_data))
  expect_equal(data_retrieved, test_data)
})

test_that("matrix_backend preserves data integrity", {
  # Create data with specific patterns to verify integrity
  n_time <- 15
  n_voxels <- 20

  # Create data where each column has a distinct pattern
  test_data <- matrix(0, nrow = n_time, ncol = n_voxels)
  for (i in 1:n_voxels) {
    test_data[, i] <- sin(seq(0, 2 * pi, length.out = n_time) + i)
  }

  backend <- matrix_backend(
    data_matrix = test_data,
    spatial_dims = c(4, 5, 1)
  )

  # Retrieve and verify data
  retrieved <- backend_get_data(backend)
  expect_equal(retrieved, test_data, tolerance = .Machine$double.eps^0.5)

  # Verify specific patterns are preserved
  expect_equal(retrieved[, 1], test_data[, 1])
  expect_equal(retrieved[, n_voxels], test_data[, n_voxels])
})
</file>

<file path="tests/testthat/test_nifti_backend.R">
test_that("nifti_backend validates inputs correctly", {
  # Test invalid source type
  expect_error(
    nifti_backend(source = 123, mask_source = "mask.nii"),
    class = "fmridataset_error_config"
  )

  # Test non-existent files
  expect_error(
    nifti_backend(source = "nonexistent.nii", mask_source = "mask.nii"),
    class = "fmridataset_error_backend_io"
  )

  expect_error(
    nifti_backend(source = "test.nii", mask_source = "nonexistent_mask.nii"),
    class = "fmridataset_error_backend_io"
  )

  # Test invalid mask type
  expect_error(
    nifti_backend(source = list(), mask_source = 123),
    class = "fmridataset_error_config"
  )
})

test_that("nifti_backend works with mock NeuroVec objects", {
  skip_if_not_installed("neuroim2")

  # Create mock data
  dims <- c(10, 10, 10, 20)

  # Create a mock NeuroVec
  data_array <- array(rnorm(prod(dims)), dims)
  mock_vec <- structure(
    data_array,
    class = c("DenseNeuroVec", "NeuroVec", "array"),
    space = structure(
      list(dim = dims[1:3], origin = c(0, 0, 0), spacing = c(2, 2, 2)),
      class = "NeuroSpace"
    )
  )

  # Create mock mask
  mock_mask <- structure(
    array(c(rep(1, 500), rep(0, 500)), c(10, 10, 10)),
    class = c("LogicalNeuroVol", "NeuroVol", "array"),
    dim = c(10, 10, 10)
  )

  # Create backend with in-memory objects
  backend <- nifti_backend(
    source = list(mock_vec),
    mask_source = mock_mask,
    preload = TRUE
  )

  expect_s3_class(backend, "nifti_backend")
  expect_s3_class(backend, "storage_backend")

  # Test dimensions
  dims_result <- backend_get_dims(backend)
  expect_equal(dims_result$spatial, c(10, 10, 10))
  expect_equal(dims_result$time, 20)

  # Test mask
  mask_result <- backend_get_mask(backend)
  expect_type(mask_result, "logical")
  expect_length(mask_result, 1000)
  expect_equal(sum(mask_result), 500)
})

test_that("nifti_backend handles multiple source files", {
  # Create mock file list
  mock_files <- c("scan1.nii", "scan2.nii", "scan3.nii")

  # Mock the file.exists function for this test
  with_mocked_bindings(
    file.exists = function(x) TRUE,
    .package = "base",
    {
      backend <- nifti_backend(
        source = mock_files,
        mask_source = "mask.nii",
        preload = FALSE
      )

      expect_equal(backend$source, mock_files)
      expect_equal(backend$mask_source, "mask.nii")
      expect_false(backend$preload)
    }
  )
})

test_that("nifti_backend data subsetting works", {
  skip_if_not_installed("neuroim2")

  # Create small test data
  n_time <- 10
  n_voxels <- 100
  test_matrix <- matrix(1:(n_time * n_voxels),
    nrow = n_time,
    ncol = n_voxels
  )

  # Create mock backend with known data
  mock_backend <- structure(
    list(
      data = structure(
        list(.Data = test_matrix),
        class = "MockNeuroVec"
      ),
      mask = rep(TRUE, n_voxels),
      dims = list(spatial = c(10, 10, 1), time = n_time)
    ),
    class = c("nifti_backend", "storage_backend")
  )

  # Override the backend_get_data method for testing
  backend_get_data.nifti_backend <- function(backend, rows = NULL, cols = NULL) {
    data <- backend$data$.Data
    if (!is.null(rows)) {
      data <- data[rows, , drop = FALSE]
    }
    if (!is.null(cols)) {
      data <- data[, cols, drop = FALSE]
    }
    data
  }

  # Test full data retrieval
  full_data <- backend_get_data(mock_backend)
  expect_equal(dim(full_data), c(n_time, n_voxels))

  # Test row subsetting
  subset_rows <- backend_get_data(mock_backend, rows = 1:5)
  expect_equal(dim(subset_rows), c(5, n_voxels))
  expect_equal(subset_rows[1, 1], test_matrix[1, 1])

  # Test column subsetting
  subset_cols <- backend_get_data(mock_backend, cols = 1:10)
  expect_equal(dim(subset_cols), c(n_time, 10))
  expect_equal(subset_cols[1, 1], test_matrix[1, 1])

  # Test both row and column subsetting
  subset_both <- backend_get_data(mock_backend, rows = 1:5, cols = 1:10)
  expect_equal(dim(subset_both), c(5, 10))
})

test_that("nifti_backend metadata extraction works", {
  # Create mock backend
  mock_backend <- structure(
    list(
      metadata = list(
        affine = diag(4),
        voxel_dims = c(2, 2, 2),
        space = "MNI",
        origin = c(0, 0, 0)
      )
    ),
    class = c("nifti_backend", "storage_backend")
  )

  # Mock the method
  backend_get_metadata.nifti_backend <- function(backend) {
    backend$metadata
  }

  metadata <- backend_get_metadata(mock_backend)

  expect_type(metadata, "list")
  expect_true("affine" %in% names(metadata))
  expect_equal(dim(metadata$affine), c(4, 4))
  expect_equal(metadata$voxel_dims, c(2, 2, 2))
})

test_that("nifti_backend validates with validate_backend", {
  skip_if_not_installed("neuroim2")

  # Create a simple valid backend
  dims <- c(5, 5, 5, 10)
  test_data <- array(rnorm(prod(dims)), dims)

  mock_vec <- structure(
    test_data,
    class = c("NeuroVec", "array")
  )

  mock_mask <- structure(
    array(TRUE, dims[1:3]),
    class = c("LogicalNeuroVol", "NeuroVol", "array"),
    dim = dims[1:3]
  )

  backend <- nifti_backend(
    source = list(mock_vec),
    mask_source = mock_mask
  )

  # Mock the backend methods for validation
  with_mocked_bindings(
    backend_get_dims = function(backend) {
      list(spatial = c(5, 5, 5), time = 10)
    },
    backend_get_mask = function(backend) {
      rep(TRUE, 125)
    },
    .package = "fmridataset",
    {
      # Should pass validation
      expect_true(validate_backend(backend))
    }
  )
})
</file>

<file path="tests/testthat/test_refactored_modules.R">
# Test refactored modular structure
# This file tests that the refactored components work together correctly

test_that("all generic functions are properly declared", {
  # Test that generic functions exist and work
  expect_true(exists("get_data"))
  expect_true(exists("get_data_matrix"))
  expect_true(exists("get_mask"))
  expect_true(exists("blocklens"))
  expect_true(exists("data_chunks"))
  expect_true(exists("as.matrix_dataset"))

  # Test that they are indeed generic functions
  expect_true(is.function(get_data))
  expect_true(is.function(get_data_matrix))
  expect_true(is.function(get_mask))
  expect_true(is.function(blocklens))
  expect_true(is.function(data_chunks))
  expect_true(is.function(as.matrix_dataset))
})

test_that("dataset constructors work from dataset_constructors.R", {
  # Test matrix_dataset constructor
  Y <- matrix(rnorm(50 * 10), 50, 10)
  dset_matrix <- matrix_dataset(Y, TR = 2, run_length = 50)

  expect_s3_class(dset_matrix, "matrix_dataset")
  expect_s3_class(dset_matrix, "fmri_dataset")
  expect_equal(dset_matrix$TR, 2)
  expect_equal(dset_matrix$nruns, 1)

  # Test fmri_mem_dataset constructor
  arr <- array(rnorm(5 * 5 * 5 * 20), c(5, 5, 5, 20))
  bspace <- neuroim2::NeuroSpace(dim = c(5, 5, 5, 20))
  nvec <- neuroim2::NeuroVec(arr, bspace)
  mask <- neuroim2::LogicalNeuroVol(array(TRUE, c(5, 5, 5)), neuroim2::NeuroSpace(dim = c(5, 5, 5)))

  dset_mem <- fmri_mem_dataset(scans = list(nvec), mask = mask, TR = 1.5)

  expect_s3_class(dset_mem, "fmri_mem_dataset")
  expect_s3_class(dset_mem, "fmri_dataset")
  expect_equal(length(dset_mem$scans), 1)
})

test_that("data access methods work from data_access.R", {
  # Create test datasets
  Y <- matrix(rnorm(30 * 8), 30, 8)
  dset_matrix <- matrix_dataset(Y, TR = 1, run_length = 30)

  # Test get_data generic and method
  data_result <- get_data(dset_matrix)
  expect_identical(data_result, Y)

  # Test get_data_matrix generic and method
  matrix_result <- get_data_matrix(dset_matrix)
  expect_identical(matrix_result, Y)

  # Test get_mask generic and method
  mask_result <- get_mask(dset_matrix)
  expect_equal(length(mask_result), 8)
  expect_true(all(mask_result == 1))

  # Test blocklens generic and method
  blocklens_result <- blocklens(dset_matrix)
  expect_equal(blocklens_result, c(30))
})

test_that("data chunking works from data_chunks.R", {
  # Create test data
  Y <- matrix(rnorm(40 * 12), 40, 12)
  run_lengths <- c(20, 20)
  dset <- matrix_dataset(Y, TR = 1, run_length = run_lengths)

  # Test data_chunks generic and method
  chunks_runwise <- data_chunks(dset, runwise = TRUE)
  expect_s3_class(chunks_runwise, "chunkiter")
  expect_equal(chunks_runwise$nchunks, 2)

  # Test single chunk
  chunks_single <- data_chunks(dset, nchunks = 1)
  expect_s3_class(chunks_single, "chunkiter")
  expect_equal(chunks_single$nchunks, 1)

  # Extract a chunk and test structure
  chunk <- chunks_single$nextElem()
  expect_s3_class(chunk, "data_chunk")
  expect_true(all(c("data", "voxel_ind", "row_ind", "chunk_num") %in% names(chunk)))
})

test_that("type conversions work from conversions.R", {
  # Create a matrix dataset
  Y <- matrix(rnorm(25 * 6), 25, 6)
  dset_matrix <- matrix_dataset(Y, TR = 2, run_length = 25)

  # Test as.matrix_dataset generic and method
  converted <- as.matrix_dataset(dset_matrix)
  expect_s3_class(converted, "matrix_dataset")
  expect_identical(converted, dset_matrix) # Should be the same object
})

test_that("print methods work from print_methods.R", {
  # Create test dataset
  Y <- matrix(rnorm(20 * 5), 20, 5)
  dset <- matrix_dataset(Y, TR = 1.5, run_length = 20)

  # Test that print method exists and runs without error
  expect_output(print(dset), "fMRI Dataset")

  # Test data chunk printing
  chunks <- data_chunks(dset, nchunks = 1)
  chunk <- chunks$nextElem()
  expect_output(print(chunk), "Data Chunk Object")

  # Test chunk iterator printing
  expect_output(print(chunks), "Chunk Iterator")
})

test_that("configuration functions work from config.R", {
  # Test that default_config function exists (internal)
  # Note: We can't easily test read_fmri_config without external files

  # Test foreach operators are available from foreach package
  expect_true(exists("%dopar%", where = asNamespace("foreach")))
  expect_true(exists("%do%", where = asNamespace("foreach")))
})

test_that("cross-module integration works correctly", {
  # Test the full workflow using multiple modules

  # 1. Create dataset (dataset_constructors.R)
  Y <- matrix(rnorm(60 * 15), 60, 15)
  dset <- matrix_dataset(Y, TR = 2.5, run_length = c(30, 30))

  # 2. Access data (data_access.R)
  data_mat <- get_data_matrix(dset)
  mask <- get_mask(dset)

  # 3. Create chunks (data_chunks.R)
  chunks <- data_chunks(dset, nchunks = 3)

  # 4. Process chunks
  chunk_means <- list()
  for (i in 1:chunks$nchunks) {
    chunk <- chunks$nextElem()
    chunk_means[[i]] <- colMeans(chunk$data)
  }

  # 5. Convert types (conversions.R)
  converted_dset <- as.matrix_dataset(dset)

  # Verify the workflow worked
  expect_equal(length(chunk_means), 3)
  expect_true(all(sapply(chunk_means, length) > 0))
  expect_s3_class(converted_dset, "matrix_dataset")
  expect_equal(nrow(data_mat), 60)
  expect_equal(ncol(data_mat), 15)
})

test_that("backwards compatibility is maintained", {
  # Test that the refactored code maintains the same API

  # Old API calls should still work
  Y <- matrix(rnorm(40 * 8), 40, 8)
  dset <- matrix_dataset(Y, TR = 1, run_length = 40)

  # These calls should work exactly as before
  expect_true(!is.null(dset))
  expect_true(!is.null(get_data(dset)))
  expect_true(!is.null(data_chunks(dset)))

  # Class structure should be preserved
  expect_true(inherits(dset, "matrix_dataset"))
  expect_true(inherits(dset, "fmri_dataset"))
  expect_true(inherits(dset, "list"))
})
</file>

<file path="tests/testthat/test_storage_backend.R">
test_that("storage backend contract validation works", {
  # Create a minimal mock backend without any methods defined
  mock_backend <- structure(
    list(),
    class = c("nonexistent_backend_type", "storage_backend")
  )

  # Test that validation fails without required methods
  expect_error(
    validate_backend(mock_backend),
    class = "error" # Will be a generic error about missing methods
  )
})

test_that("backend validation checks dimension requirements", {
  # Use an existing backend (matrix_backend) and mock the get_dims method
  test_matrix <- matrix(1:1000, 100, 10)
  mock_backend <- matrix_backend(test_matrix)

  # Test normal case first
  expect_true(validate_backend(mock_backend))

  # Test invalid spatial dimensions
  with_mocked_bindings(
    backend_get_dims = function(backend) {
      list(spatial = c(10, 10), time = 100) # Wrong length
    },
    .package = "fmridataset",
    {
      expect_error(
        validate_backend(mock_backend),
        "spatial dimensions must be a numeric vector of length 3"
      )
    }
  )

  # Test invalid time dimension
  with_mocked_bindings(
    backend_get_dims = function(backend) {
      list(spatial = c(10, 10, 10), time = -1)
    },
    .package = "fmridataset",
    {
      expect_error(
        validate_backend(mock_backend),
        "time dimension must be a positive integer"
      )
    }
  )
})

test_that("backend validation checks mask requirements", {
  # Use existing backend and mock the mask method
  test_matrix <- matrix(1:1000, 100, 10)
  mock_backend <- matrix_backend(test_matrix)

  # Test all FALSE mask
  with_mocked_bindings(
    backend_get_mask = function(backend) rep(FALSE, 10),
    .package = "fmridataset",
    {
      expect_error(
        validate_backend(mock_backend),
        "mask must contain at least one TRUE value"
      )
    }
  )

  # Test mask with NA values
  with_mocked_bindings(
    backend_get_mask = function(backend) c(rep(TRUE, 9), NA),
    .package = "fmridataset",
    {
      expect_error(
        validate_backend(mock_backend),
        "missing value where TRUE/FALSE needed"
      )
    }
  )

  # Test wrong mask length
  with_mocked_bindings(
    backend_get_mask = function(backend) rep(TRUE, 5), # Should be 10
    backend_get_dims = function(backend) list(spatial = c(2, 5, 1), time = 100),
    .package = "fmridataset",
    {
      expect_error(
        validate_backend(mock_backend),
        "mask length .* must equal prod"
      )
    }
  )
})

test_that("error classes work correctly", {
  # Test fmridataset_error_backend_io
  err <- fmridataset_error_backend_io(
    message = "Failed to read file",
    file = "test.nii",
    operation = "read"
  )

  expect_s3_class(err, "fmridataset_error_backend_io")
  expect_s3_class(err, "fmridataset_error")
  expect_equal(err$file, "test.nii")
  expect_equal(err$operation, "read")

  # Test fmridataset_error_config
  err <- fmridataset_error_config(
    message = "Invalid parameter",
    parameter = "mask",
    value = NULL
  )

  expect_s3_class(err, "fmridataset_error_config")
  expect_equal(err$parameter, "mask")

  # Test stop_fmridataset
  expect_error(
    stop_fmridataset(
      fmridataset_error_config,
      message = "Test error"
    ),
    class = "fmridataset_error_config"
  )
})
</file>

<file path="tests/testthat.R">
library(testthat)
library(fmridataset)

test_check("fmridataset")
</file>

<file path="R/conversions.R">
#' @importFrom neuroim2 series

# ========================================================================
# Type Conversion Methods for fMRI Datasets
# ========================================================================
#
# This file implements methods for the as.matrix_dataset() generic
# declared in all_generic.R. Provides conversion from various dataset
# types to matrix_dataset objects.
# ========================================================================

#' @export
as.matrix_dataset.matrix_dataset <- function(x, ...) {
  x # Already a matrix_dataset
}

#' @export
as.matrix_dataset.fmri_mem_dataset <- function(x, ...) {
  # Get the data matrix
  bvec <- get_data(x)
  mask <- get_mask(x)
  datamat <- series(bvec, which(mask != 0))

  # Create matrix_dataset
  matrix_dataset(
    datamat = datamat,
    TR = x$sampling_frame$TR,
    run_length = x$sampling_frame$blocklens,
    event_table = x$event_table
  )
}

#' @export
as.matrix_dataset.fmri_file_dataset <- function(x, ...) {
  # Get the data matrix - handle both backend and legacy cases
  if (!is.null(x$backend)) {
    # Backend-based dataset - get_data_matrix already returns matrix
    datamat <- get_data_matrix(x)
  } else {
    # Legacy dataset - need to use series
    vec <- get_data(x)
    mask <- get_mask(x)
    datamat <- series(vec, which(mask != 0))
  }

  # Create matrix_dataset
  matrix_dataset(
    datamat = datamat,
    TR = x$sampling_frame$TR,
    run_length = x$sampling_frame$blocklens,
    event_table = x$event_table
  )
}
</file>

<file path="R/data_access.R">
#' @importFrom neuroim2 series
#' @import memoise

#' @export
#' @importFrom neuroim2 NeuroVecSeq
get_data.latent_dataset <- function(x, ...) {
  x$lvec@basis
}

#' @export
#' @importFrom neuroim2 NeuroVecSeq
get_data.fmri_mem_dataset <- function(x, ...) {
  if (length(x$scans) > 1) {
    do.call(neuroim2::NeuroVecSeq, x$scans)
  } else {
    x$scans[[1]]
  }
}

#' @export
#' @importFrom neuroim2 NeuroVecSeq
get_data.matrix_dataset <- function(x, ...) {
  x$datamat
}

#' @export
#' @importFrom neuroim2 NeuroVecSeq FileBackedNeuroVec
get_data.fmri_file_dataset <- function(x, ...) {
  if (!is.null(x$backend)) {
    # New backend path - return raw data matrix
    backend_get_data(x$backend, ...)
  } else if (is.null(x$vec)) {
    # Legacy path
    get_data_from_file(x, ...)
  } else {
    x$vec
  }
}

#' @export
get_data_matrix.matrix_dataset <- function(x, ...) {
  x$datamat
}


#' @export
get_data_matrix.fmri_mem_dataset <- function(x, ...) {
  bvec <- get_data(x)
  mask <- get_mask(x)
  series(bvec, which(mask != 0))
}


#' @export
get_data_matrix.fmri_file_dataset <- function(x, ...) {
  if (!is.null(x$backend)) {
    # New backend path - already returns matrix in correct format
    backend_get_data(x$backend, ...)
  } else {
    # Legacy path
    bvec <- get_data(x)
    mask <- get_mask(x)
    series(bvec, which(mask != 0))
  }
}



#' @import memoise
#' @keywords internal
#' @noRd
get_data_from_file <- memoise::memoise(function(x, ...) {
  m <- get_mask(x)
  neuroim2::read_vec(x$scans, mask = m, mode = x$mode, ...)
})



#' @export
get_mask.fmri_file_dataset <- function(x, ...) {
  if (!is.null(x$backend)) {
    # New backend path - returns logical vector
    mask_vec <- backend_get_mask(x$backend)
    # Need to reshape to 3D volume for compatibility
    dims <- backend_get_dims(x$backend)$spatial
    array(mask_vec, dims)
  } else if (is.null(x$mask)) {
    # Legacy path
    neuroim2::read_vol(x$mask_file)
  } else {
    x$mask
  }
}


#' @export
get_mask.fmri_mem_dataset <- function(x, ...) {
  x$mask
}

#' @export
get_mask.matrix_dataset <- function(x, ...) {
  x$mask
}

#' @export
get_mask.latent_dataset <- function(x, ...) {
  x$lvec@mask
}

#' @export
blocklens.matrix_dataset <- function(x, ...) {
  blocklens(x$sampling_frame)
}
</file>

<file path="R/h5_backend.R">
#' H5 Storage Backend
#'
#' @description
#' A storage backend implementation for H5 format neuroimaging data using fmristore.
#' Each scan is stored as an H5 file that loads to an H5NeuroVec object.
#'
#' @details
#' The H5Backend integrates with the fmristore package to work with:
#' - File paths to H5 neuroimaging files
#' - Pre-loaded H5NeuroVec objects from fmristore
#' - Multiple H5 files representing different scans
#'
#' @name h5-backend
#' @keywords internal
#' @importFrom neuroim2 space trans spacing origin series
NULL

#' Create an H5 Backend
#'
#' @param source Character vector of file paths to H5 files or list of H5NeuroVec objects
#' @param mask_source File path to H5 mask file, H5 file containing mask, or in-memory NeuroVol object
#' @param mask_dataset Character string specifying the dataset path within H5 file for mask (default: "data/elements")
#' @param data_dataset Character string specifying the dataset path within H5 files for data (default: "data")
#' @param preload Logical, whether to eagerly load H5NeuroVec objects into memory
#' @return An h5_backend S3 object
#' @export
#' @keywords internal
h5_backend <- function(source, mask_source,
                       mask_dataset = "data/elements",
                       data_dataset = "data",
                       preload = FALSE) {
  # Check if fmristore is available FIRST
  if (!requireNamespace("fmristore", quietly = TRUE)) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "Package 'fmristore' is required for H5 backend but is not available",
      parameter = "backend_type"
    )
  }

  # Validate inputs
  if (is.character(source)) {
    # File paths provided
    if (!all(file.exists(source))) {
      missing_files <- source[!file.exists(source)]
      stop_fmridataset(
        fmridataset_error_backend_io,
        message = sprintf("H5 source files not found: %s", paste(missing_files, collapse = ", ")),
        file = missing_files,
        operation = "open"
      )
    }
  } else if (is.list(source)) {
    # In-memory H5NeuroVec objects provided
    valid_types <- vapply(source, function(x) {
      inherits(x, "H5NeuroVec")
    }, logical(1))

    if (!all(valid_types)) {
      stop_fmridataset(
        fmridataset_error_config,
        message = "All source objects must be H5NeuroVec objects",
        parameter = "source"
      )
    }
  } else {
    stop_fmridataset(
      fmridataset_error_config,
      message = "source must be character vector (H5 file paths) or list (H5NeuroVec objects)",
      parameter = "source",
      value = class(source)
    )
  }

  # Validate mask source
  if (is.character(mask_source)) {
    if (!file.exists(mask_source)) {
      stop_fmridataset(
        fmridataset_error_backend_io,
        message = sprintf("H5 mask file not found: %s", mask_source),
        file = mask_source,
        operation = "open"
      )
    }
  } else if (!inherits(mask_source, "NeuroVol") && !inherits(mask_source, "H5NeuroVol")) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "mask_source must be file path, NeuroVol, or H5NeuroVol object",
      parameter = "mask_source",
      value = class(mask_source)
    )
  }

  backend <- list(
    source = source,
    mask_source = mask_source,
    mask_dataset = mask_dataset,
    data_dataset = data_dataset,
    preload = preload,
    h5_objects = NULL,
    mask = NULL,
    dims = NULL,
    metadata = NULL
  )

  class(backend) <- c("h5_backend", "storage_backend")
  backend
}

#' @export
backend_open.h5_backend <- function(backend) {
  if (backend$preload && is.null(backend$h5_objects)) {
    # Load H5NeuroVec objects
    backend$h5_objects <- if (is.character(backend$source)) {
      # Load from H5 files
      tryCatch(
        {
          lapply(backend$source, function(file_path) {
            fmristore::H5NeuroVec(file_path, dataset_name = backend$data_dataset)
          })
        },
        error = function(e) {
          stop_fmridataset(
            fmridataset_error_backend_io,
            message = sprintf("Failed to load H5NeuroVec from files: %s", e$message),
            file = backend$source,
            operation = "read"
          )
        }
      )
    } else {
      # Use pre-loaded H5NeuroVec objects
      backend$source
    }

    # Load mask
    backend$mask <- if (is.character(backend$mask_source)) {
      tryCatch(
        {
          # Try to load as H5NeuroVol first, then fall back to regular volume
          if (endsWith(tolower(backend$mask_source), ".h5")) {
            # Load as H5NeuroVol and extract array
            h5_mask <- fmristore::H5NeuroVol(backend$mask_source, dataset_name = backend$mask_dataset)
            mask_array <- as.array(h5_mask)
            close(h5_mask) # Close the H5 handle
            neuroim2::NeuroVol(mask_array, space = space(backend$h5_objects[[1]]))
          } else {
            # Load as regular volume file
            neuroim2::read_vol(backend$mask_source)
          }
        },
        error = function(e) {
          stop_fmridataset(
            fmridataset_error_backend_io,
            message = sprintf("Failed to read H5 mask: %s", e$message),
            file = backend$mask_source,
            operation = "read"
          )
        }
      )
    } else {
      # Use in-memory mask object
      backend$mask_source
    }

    # Extract dimensions from first H5NeuroVec
    if (length(backend$h5_objects) > 0) {
      first_obj <- backend$h5_objects[[1]]
      d <- dim(first_obj)

      # Calculate total time dimension across all H5 files
      total_time <- if (length(backend$h5_objects) > 1) {
        sum(sapply(backend$h5_objects, function(obj) dim(obj)[4]))
      } else {
        d[4]
      }

      backend$dims <- list(
        spatial = d[1:3],
        time = total_time
      )
    }
  }

  backend
}

#' @export
backend_close.h5_backend <- function(backend) {
  # Close any open H5NeuroVec objects
  if (!is.null(backend$h5_objects)) {
    lapply(backend$h5_objects, function(obj) {
      tryCatch(close(obj), error = function(e) invisible(NULL))
    })
  }
  invisible(NULL)
}

#' @export
backend_get_dims.h5_backend <- function(backend) {
  if (!is.null(backend$dims)) {
    return(backend$dims)
  }

  # Get dimensions without loading full data
  if (is.character(backend$source)) {
    # Read from first H5 file to get spatial dimensions
    tryCatch(
      {
        first_h5 <- fmristore::H5NeuroVec(backend$source[1], dataset_name = backend$data_dataset)
        d <- dim(first_h5)
        close(first_h5) # Close immediately after getting dimensions

        # Calculate total time dimension across all files
        total_time <- if (length(backend$source) > 1) {
          sum(sapply(backend$source, function(file_path) {
            h5_obj <- fmristore::H5NeuroVec(file_path, dataset_name = backend$data_dataset)
            time_dim <- dim(h5_obj)[4]
            close(h5_obj)
            time_dim
          }))
        } else {
          d[4]
        }

        list(spatial = d[1:3], time = total_time)
      },
      error = function(e) {
        stop_fmridataset(
          fmridataset_error_backend_io,
          message = sprintf("Failed to read H5 dimensions: %s", e$message),
          file = backend$source[1],
          operation = "read_header"
        )
      }
    )
  } else {
    # In-memory H5NeuroVec objects
    first_obj <- backend$source[[1]]
    d <- dim(first_obj)
    total_time <- if (length(backend$source) > 1) {
      sum(sapply(backend$source, function(obj) dim(obj)[4]))
    } else {
      d[4]
    }

    list(spatial = d[1:3], time = total_time)
  }
}

#' @export
backend_get_mask.h5_backend <- function(backend) {
  if (!is.null(backend$mask)) {
    # Already loaded
    mask_vol <- backend$mask
  } else if (is.character(backend$mask_source)) {
    # Load from file
    mask_vol <- tryCatch(
      {
        if (endsWith(tolower(backend$mask_source), ".h5")) {
          # Load as H5NeuroVol
          h5_mask <- fmristore::H5NeuroVol(backend$mask_source, dataset_name = backend$mask_dataset)
          mask_array <- as.array(h5_mask)
          close(h5_mask) # Close the H5 handle

          # Get space information from first data file if available
          if (is.character(backend$source) && length(backend$source) > 0) {
            first_h5 <- fmristore::H5NeuroVec(backend$source[1], dataset_name = backend$data_dataset)
            space_info <- space(first_h5)
            close(first_h5)
            neuroim2::NeuroVol(mask_array, space = space_info)
          } else {
            # Create with minimal space info
            neuroim2::NeuroVol(mask_array)
          }
        } else {
          # Load as regular volume file
          neuroim2::read_vol(backend$mask_source)
        }
      },
      error = function(e) {
        stop_fmridataset(
          fmridataset_error_backend_io,
          message = sprintf("Failed to read H5 mask: %s", e$message),
          file = backend$mask_source,
          operation = "read"
        )
      }
    )
  } else {
    # In-memory mask
    mask_vol <- backend$mask_source
  }

  # Convert to logical vector
  mask_vec <- as.logical(as.vector(mask_vol))

  # Validate mask
  if (any(is.na(mask_vec))) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "H5 mask contains NA values",
      parameter = "mask"
    )
  }

  if (sum(mask_vec) == 0) {
    stop_fmridataset(
      fmridataset_error_config,
      message = "H5 mask contains no TRUE values",
      parameter = "mask"
    )
  }

  mask_vec
}

#' @export
backend_get_data.h5_backend <- function(backend, rows = NULL, cols = NULL) {
  # Get or load H5NeuroVec objects
  h5_objects <- if (!is.null(backend$h5_objects)) {
    backend$h5_objects
  } else {
    # Load on demand
    if (is.character(backend$source)) {
      tryCatch(
        {
          lapply(backend$source, function(file_path) {
            fmristore::H5NeuroVec(file_path, dataset_name = backend$data_dataset)
          })
        },
        error = function(e) {
          stop_fmridataset(
            fmridataset_error_backend_io,
            message = sprintf("Failed to load H5NeuroVec from files: %s", e$message),
            file = backend$source,
            operation = "read"
          )
        }
      )
    } else {
      backend$source
    }
  }

  # Get mask information
  mask_vec <- backend_get_mask(backend)
  voxel_indices <- which(mask_vec)

  # Extract data matrix in timepoints × voxels format
  if (length(h5_objects) == 1) {
    # Single H5NeuroVec object
    h5_obj <- h5_objects[[1]]
    data_matrix <- neuroim2::series(h5_obj, voxel_indices)
  } else {
    # Multiple H5NeuroVec objects - concatenate along time dimension
    data_matrices <- lapply(h5_objects, function(h5_obj) {
      neuroim2::series(h5_obj, voxel_indices)
    })
    data_matrix <- do.call(rbind, data_matrices)
  }

  # Close H5 objects if we loaded them on demand
  if (is.null(backend$h5_objects)) {
    lapply(h5_objects, function(obj) {
      tryCatch(close(obj), error = function(e) invisible(NULL))
    })
  }

  # Apply subsetting if requested
  if (!is.null(rows)) {
    data_matrix <- data_matrix[rows, , drop = FALSE]
  }

  if (!is.null(cols)) {
    data_matrix <- data_matrix[, cols, drop = FALSE]
  }

  data_matrix
}

#' @export
backend_get_metadata.h5_backend <- function(backend) {
  # Get metadata from first H5NeuroVec object
  h5_obj <- if (!is.null(backend$h5_objects)) {
    backend$h5_objects[[1]]
  } else if (is.character(backend$source)) {
    # Load temporarily to get metadata
    first_h5 <- fmristore::H5NeuroVec(backend$source[1], dataset_name = backend$data_dataset)
    on.exit(close(first_h5))
    first_h5
  } else {
    backend$source[[1]]
  }

  # Extract neuroimaging metadata
  space_obj <- space(h5_obj)

  list(
    format = "h5",
    affine = trans(space_obj),
    voxel_dims = spacing(space_obj),
    origin = origin(space_obj),
    dimensions = dim(space_obj),
    data_files = if (is.character(backend$source)) backend$source else NULL,
    mask_file = if (is.character(backend$mask_source)) backend$mask_source else NULL
  )
}
</file>

<file path="R/print_methods.R">
#' @importFrom utils head tail
#' @export
#' @rdname print
print.fmri_dataset <- function(x, ...) {
  # Header
  cat("\n=== fMRI Dataset ===\n")

  # Basic dimensions
  cat("\n** Dimensions:\n")
  cat("  - Timepoints:", sum(x$sampling_frame$blocklens), "\n")
  cat("  - Runs:", x$nruns, "\n")

  # Data source info
  print_data_source_info(x)

  # Mask info
  mask <- get_mask(x)
  cat("  - Voxels in mask:", sum(mask > 0), "\n")
  cat("  - Mask dimensions:", paste(dim(mask), collapse = " x "), "\n")

  # Sampling frame info
  cat("\n** Temporal Structure:\n")
  cat("  - TR:", x$sampling_frame$TR, "seconds\n")
  cat("  - Run lengths:", paste(x$sampling_frame$blocklens, collapse = ", "), "\n")

  # Event table summary
  cat("\n** Event Table:\n")
  if (nrow(x$event_table) > 0) {
    cat("  - Rows:", nrow(x$event_table), "\n")
    cat("  - Variables:", paste(names(x$event_table), collapse = ", "), "\n")

    # Show first few events if they exist
    if (nrow(x$event_table) > 0) {
      cat("  - First few events:\n")
      print(head(x$event_table, 3))
    }
  } else {
    cat("  - Empty event table\n")
  }

  cat("\n")
}

#' @export
#' @rdname print
print.latent_dataset <- function(x, ...) {
  # Header
  cat("\n=== Latent Dataset ===\n")

  # Basic dimensions
  cat("\n** Dimensions:\n")
  cat("  - Timepoints:", nrow(x$datamat), "\n")
  cat("  - Latent components:", ncol(x$datamat), "\n")
  cat("  - Runs:", x$nruns, "\n")

  # Original space info if available
  if (!is.null(x$original_space)) {
    cat("  - Original space:", paste(x$original_space, collapse = " x "), "\n")
  }

  # Sampling frame info
  cat("\n** Temporal Structure:\n")
  cat("  - TR:", x$sampling_frame$TR, "seconds\n")
  cat("  - Run lengths:", paste(x$sampling_frame$blocklens, collapse = ", "), "\n")

  # Event table summary
  cat("\n** Event Table:\n")
  if (nrow(x$event_table) > 0) {
    cat("  - Rows:", nrow(x$event_table), "\n")
    cat("  - Variables:", paste(names(x$event_table), collapse = ", "), "\n")

    # Show first few events if they exist
    if (nrow(x$event_table) > 0) {
      cat("  - First few events:\n")
      print(head(x$event_table, 3))
    }
  } else {
    cat("  - Empty event table\n")
  }

  # Data summary
  cat("\n** Latent Data Summary:\n")
  data_summary <- summary(as.vector(x$datamat[1:min(1000, length(x$datamat))]))[c(1, 3, 4, 6)]
  cat("  - Values (sample):", paste(names(data_summary), data_summary, sep = ":", collapse = ", "), "\n")

  cat("\n")
}

#' Pretty Print a Chunk Iterator
#'
#' This function prints a summary of a chunk iterator using colored output.
#'
#' @param x A chunkiter object.
#' @param ... Additional arguments (ignored).
#' @export
#' @rdname print
print.chunkiter <- function(x, ...) {
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Please install the crayon package to use this function.")
  }
  cat(crayon::blue("Chunk Iterator:\n"))
  cat(crayon::magenta("  Total number of chunks: "), x$nchunks, "\n")
  invisible(x)
}

#' Pretty Print a Data Chunk Object
#'
#' This function prints a summary of a data chunk using crayon for colored output.
#'
#' @param x A data_chunk object.
#' @param ... Additional arguments (ignored).
#' @export
#' @rdname print
print.data_chunk <- function(x, ...) {
  if (!requireNamespace("crayon", quietly = TRUE)) {
    stop("Please install the crayon package to use this function.")
  }
  cat(crayon::blue("Data Chunk Object\n"))
  cat(crayon::magenta("  Chunk number: "), x$chunk_num, "\n")
  cat(crayon::magenta("  Number of voxels: "), length(x$voxel_ind), "\n")
  cat(crayon::magenta("  Number of rows: "), length(x$row_ind), "\n")
  if (!is.null(dim(x$data))) {
    cat(crayon::magenta("  Data dimensions: "), paste(dim(x$data), collapse = " x "), "\n")
  } else {
    cat(crayon::magenta("  Data: "), paste(head(x$data, 10), collapse = ", "), "\n")
  }
  invisible(x)
}

#' Helper function to print data source information
#' @keywords internal
#' @noRd
print_data_source_info <- function(x) {
  if (inherits(x, "matrix_dataset")) {
    cat("  - Matrix:", nrow(x$datamat), "x", ncol(x$datamat), "(timepoints x voxels)\n")
  } else if (inherits(x, "fmri_mem_dataset")) {
    n_objects <- length(x$scans)
    cat("  - Objects:", n_objects, "pre-loaded NeuroVec object(s)\n")
  } else if (inherits(x, "fmri_file_dataset")) {
    if (!is.null(x$backend)) {
      # New backend-based dataset
      cat("  - Backend:", class(x$backend)[1], "\n")
      dims <- backend_get_dims(x$backend)
      cat(
        "  - Data dimensions:", dims$time, "x", sum(backend_get_mask(x$backend)),
        "(timepoints x voxels)\n"
      )
    } else {
      # Legacy file-based dataset
      n_files <- length(x$scans)
      cat("  - Files:", n_files, "NIfTI file(s)\n")
      if (n_files <= 3) {
        file_names <- basename(x$scans)
        cat("    ", paste(file_names, collapse = ", "), "\n")
      } else {
        file_names <- basename(x$scans)
        cat(
          "    ", paste(head(file_names, 2), collapse = ", "),
          ", ..., ", tail(file_names, 1), "\n"
        )
      }
    }
  }
}

#' @export
#' @rdname print
print.matrix_dataset <- function(x, ...) {
  # Use the generic fmri_dataset print method
  print.fmri_dataset(x, ...)
}
</file>

<file path="tests/testthat/test_chunk_utils.R">
context("chunk utilities")

library(fmridataset)

## tests for arbitrary_chunks and one_chunk

test_that("arbitrary_chunks handles too many chunks", {
  Y <- matrix(1:20, nrow = 10, ncol = 2)
  dset <- matrix_dataset(Y, TR = 1, run_length = 10)
  expect_warning(
    ch <- fmridataset:::arbitrary_chunks(dset, 5),
    "greater than number of voxels"
  )
  expect_length(ch, 2)
  # Extract all elements from the deflist object
  all_indices <- unlist(lapply(seq_len(length(ch)), function(i) ch[[i]]))
  expect_equal(sort(unique(all_indices)), 1:2)
})

test_that("one_chunk returns all voxel indices", {
  Y <- matrix(1:20, nrow = 10, ncol = 2)
  dset <- matrix_dataset(Y, TR = 1, run_length = 10)
  oc <- fmridataset:::one_chunk(dset)
  expect_equal(oc[[1]], 1:2)
})

## tests for exec_strategy warnings and print methods

test_that("exec_strategy handles large requested chunks", {
  Y <- matrix(1:20, nrow = 10, ncol = 2)
  dset <- matrix_dataset(Y, TR = 1, run_length = 10)
  strat <- fmridataset:::exec_strategy("chunkwise", nchunks = 5)
  expect_warning(iter <- strat(dset), "greater than number of voxels")
  expect_equal(iter$nchunks, 2)
})

test_that("print methods for chunk objects work", {
  Y <- matrix(rnorm(20), nrow = 10, ncol = 2)
  dset <- matrix_dataset(Y, TR = 1, run_length = 10)
  iter <- data_chunks(dset, nchunks = 1)
  chunk <- iter$nextElem()
  skip_if_not_installed("crayon")
  expect_output(print(iter), "Chunk Iterator")
  expect_output(print(chunk), "Data Chunk Object")
})
</file>

<file path="tests/testthat/test_dataset.R">
test_that("can construct an fmri_dataset", {
  # Test with mock files that actually exist using tempfiles
  temp_files <- c(
    tempfile(fileext = ".nii"),
    tempfile(fileext = ".nii"),
    tempfile(fileext = ".nii")
  )
  temp_mask <- tempfile(fileext = ".nii")

  # Create mock NIfTI files
  for (f in c(temp_files, temp_mask)) {
    file.create(f)
  }

  # Mock the validation step that checks file existence
  with_mocked_bindings(
    nifti_backend = function(source, mask_source, preload = FALSE, ...) {
      # Create a mock nifti backend that bypasses file validation
      backend <- matrix_backend(matrix(rnorm(1000), 100, 10))
      class(backend) <- c("nifti_backend", "storage_backend")
      backend$source <- source
      backend$mask_source <- mask_source
      backend$preload <- preload
      backend$data <- NULL # Add this to avoid the boolean error
      backend
    },
    # Mock the validation function to skip file reading
    backend_get_dims.nifti_backend = function(backend) {
      list(spatial = c(10, 1, 1), time = 300) # Match the run_length total
    },
    .package = "fmridataset",
    {
      dset <- fmri_dataset(
        scans = temp_files,
        mask = temp_mask,
        run_length = c(100, 100, 100),
        TR = 2
      )
      expect_true(!is.null(dset))
      expect_s3_class(dset, "fmri_dataset")
      expect_s3_class(dset$backend, "nifti_backend")
    }
  )

  # Clean up
  unlink(c(temp_files, temp_mask))
})


## design file not found during testing - commented out until extdata is available
# test_that("can read a config file to create fmri_dataset", {
# fname <- system.file("extdata", "config.R", package = "fmridataset")
# base_path=dirname(fname)

# config <- read_fmri_config(fname, base_path)
# expect_true(!is.null(config))
# })

test_that("can construct an fmri_mem_dataset", {
  # Create synthetic design data since extdata may not be available
  facedes <- data.frame(
    run = rep(1:2, each = 244),
    rep_num = rep(1:244, 2),
    trial_type = sample(c("face", "house"), 488, replace = TRUE)
  )
  facedes$repnum <- factor(facedes$rep_num)

  scans <- lapply(1:length(unique(facedes$run)), function(i) {
    arr <- array(rnorm(10 * 10 * 10 * 244), c(10, 10, 10, 244))
    bspace <- neuroim2::NeuroSpace(dim = c(10, 10, 10, 244))
    neuroim2::NeuroVec(arr, bspace)
  })

  mask <- neuroim2::LogicalNeuroVol(array(rnorm(10 * 10 * 10), c(10, 10, 10)) > 0, neuroim2::NeuroSpace(dim = c(10, 10, 10)))

  dset <- fmri_mem_dataset(
    scans = scans,
    mask = mask,
    TR = 1.5,
    event_table = tibble::as_tibble(facedes)
  )

  expect_true(!is.null(dset))
  expect_s3_class(dset, "fmri_mem_dataset")
  expect_s3_class(dset, "fmri_dataset")
})
</file>

<file path="tests/testthat/test_h5_backend.R">
# Tests for H5 Backend functionality

# Mock methods for H5NeuroVec and H5NeuroVol objects
dim.H5NeuroVec <- function(x) {
  x$space$dims # This should already be a vector
}

dim.H5NeuroVol <- function(x) {
  x$space$dims # This should already be a vector
}

close.H5NeuroVec <- function(con, ...) {
  invisible(NULL)
}

close.H5NeuroVol <- function(con, ...) {
  invisible(NULL)
}

space.H5NeuroVec <- function(x) {
  x$space
}

space.H5NeuroVol <- function(x) {
  x$space
}

as.array.H5NeuroVol <- function(x, ...) {
  x$h5obj[["data/elements"]]
}

as.logical.mock_h5_dataset <- function(x, ...) {
  as.logical(as.vector(x))
}

# Mock neuroim2 series function
series.H5NeuroVec <- function(x, i, ...) {
  data_arr <- x$obj[[x$dataset_name]]
  if (missing(i)) {
    # Return all data as matrix (time x voxels)
    dims <- dim(data_arr)
    matrix(as.vector(data_arr), nrow = dims[4], ncol = prod(dims[1:3]))
  } else {
    # Return data for specific voxel indices
    dims <- dim(data_arr)
    n_time <- dims[4]
    n_voxels <- length(i)

    # Create matrix with time x voxels
    result_matrix <- matrix(0, nrow = n_time, ncol = n_voxels)

    # Fill in data for each voxel index
    for (v in seq_along(i)) {
      voxel_idx <- i[v]
      # Convert linear index to 3D coordinates
      coords <- arrayInd(voxel_idx, dims[1:3])
      # Extract time series for this voxel
      result_matrix[, v] <- data_arr[coords[1], coords[2], coords[3], ]
    }

    result_matrix
  }
}

# Mock methods for NeuroSpace objects
trans.NeuroSpace <- function(x) {
  x$trans
}

spacing.NeuroSpace <- function(x) {
  x$spacing
}

origin.NeuroSpace <- function(x) {
  x$origin
}

dim.NeuroSpace <- function(x) {
  x$dims
}

# Helper function to create mock H5NeuroVec objects
create_mock_h5neurovec <- function(dims = c(10, 10, 5, 50), dataset_name = "data") {
  # Ensure dims is a vector
  dims <- as.numeric(dims)

  # Create a mock H5File object
  mock_h5file <- list(
    `[[` = function(name) {
      if (name == dataset_name) {
        # Return a mock dataset that acts like an array
        structure(
          array(rnorm(prod(dims)), dim = dims),
          class = "mock_h5_dataset"
        )
      } else if (name == "space/dim") {
        structure(dims, class = "mock_h5_attr")
      } else if (name == "space/origin") {
        structure(c(0, 0, 0), class = "mock_h5_attr")
      } else if (name == "space/trans") {
        structure(diag(4), class = "mock_h5_attr")
      }
    },
    exists = function(name) name %in% c(dataset_name, "space/dim", "space/origin", "space/trans"),
    is_valid = TRUE
  )

  # Create mock NeuroSpace - ensure dims is a vector
  mock_space <- structure(
    list(
      dims = dims, # Keep as vector
      origin = c(0, 0, 0),
      trans = diag(4),
      spacing = c(1, 1, 1)
    ),
    class = "NeuroSpace"
  )

  # Create mock H5NeuroVec
  structure(
    list(
      space = mock_space,
      obj = mock_h5file,
      dataset_name = dataset_name
    ),
    class = "H5NeuroVec"
  )
}

# Helper function to create mock H5NeuroVol objects
create_mock_h5neurovol <- function(dims = c(10, 10, 5)) {
  # Ensure dims is a vector
  dims <- as.numeric(dims)

  # Create a mock H5File object
  mock_h5file <- list(
    `[[` = function(name) {
      if (name == "data/elements") {
        structure(
          array(runif(prod(dims)), dim = dims),
          class = "mock_h5_dataset"
        )
      } else if (name == "space/dim") {
        structure(dims, class = "mock_h5_attr")
      } else if (name == "space/origin") {
        structure(c(0, 0, 0), class = "mock_h5_attr")
      } else if (name == "space/trans") {
        structure(diag(4), class = "mock_h5_attr")
      }
    },
    exists = function(name) name %in% c("data/elements", "space/dim", "space/origin", "space/trans"),
    is_valid = TRUE
  )

  # Create mock NeuroSpace - ensure dims is a vector
  mock_space <- structure(
    list(
      dims = dims, # Keep as vector
      origin = c(0, 0, 0),
      trans = diag(4),
      spacing = c(1, 1, 1)
    ),
    class = "NeuroSpace"
  )

  # Create mock H5NeuroVol
  structure(
    list(
      space = mock_space,
      h5obj = mock_h5file
    ),
    class = "H5NeuroVol"
  )
}

test_that("h5_backend constructor validates inputs correctly", {
  skip_if_not_installed("fmristore")

  # Test missing files
  expect_error(
    h5_backend(c("nonexistent1.h5", "nonexistent2.h5"), "mask.h5"),
    "H5 source files not found"
  )

  # Test missing mask file - create a temporary file to pass first validation
  temp_file <- tempfile(fileext = ".h5")
  file.create(temp_file)
  on.exit(unlink(temp_file))

  expect_error(
    h5_backend(temp_file, "nonexistent_mask.h5"),
    "H5 mask file not found"
  )

  # Test invalid source type
  expect_error(
    h5_backend(123, "mask.h5"),
    "source must be character vector.*or list"
  )

  # Test invalid H5NeuroVec objects in list
  expect_error(
    h5_backend(list("not_h5neurovec"), "mask.h5"),
    "All source objects must be H5NeuroVec objects"
  )
})

test_that("h5_backend works with file paths", {
  skip_if_not_installed("fmristore")
  skip_if_not_installed("neuroim2")
  skip_if_not_installed("hdf5r")

  # Create temporary H5 files for testing
  temp_dir <- tempdir()
  h5_file1 <- file.path(temp_dir, "test_scan1.h5")
  h5_file2 <- file.path(temp_dir, "test_scan2.h5")
  mask_file <- file.path(temp_dir, "test_mask.h5")

  # Create test H5 files using fmristore helpers (if available)
  # This is a simplified test - in practice you'd create proper H5 files
  skip("H5 file creation helpers not available for testing")

  # If we had the files, the test would look like:
  # backend <- h5_backend(
  #   source = c(h5_file1, h5_file2),
  #   mask_source = mask_file
  # )
  #
  # expect_s3_class(backend, "h5_backend")
  # expect_s3_class(backend, "storage_backend")
  # expect_equal(backend$source, c(h5_file1, h5_file2))
  # expect_equal(backend$mask_source, mask_file)
})

test_that("h5_backend constructor sets parameters correctly", {
  skip_if_not_installed("fmristore")

  # Create temporary files for validation
  temp_file1 <- tempfile(fileext = ".h5")
  temp_file2 <- tempfile(fileext = ".h5")
  mask_file <- tempfile(fileext = ".h5")
  file.create(c(temp_file1, temp_file2, mask_file))
  on.exit(unlink(c(temp_file1, temp_file2, mask_file)))

  backend <- h5_backend(
    source = c(temp_file1, temp_file2),
    mask_source = mask_file,
    data_dataset = "custom_data",
    mask_dataset = "custom_mask",
    preload = TRUE
  )

  expect_s3_class(backend, "h5_backend")
  expect_s3_class(backend, "storage_backend")
  expect_equal(backend$source, c(temp_file1, temp_file2))
  expect_equal(backend$mask_source, mask_file)
  expect_equal(backend$data_dataset, "custom_data")
  expect_equal(backend$mask_dataset, "custom_mask")
  expect_true(backend$preload)
})

test_that("h5_backend handles custom dataset paths", {
  skip_if_not_installed("fmristore")

  # Create temporary files for validation
  temp_file <- tempfile(fileext = ".h5")
  mask_file <- tempfile(fileext = ".h5")
  file.create(c(temp_file, mask_file))
  on.exit(unlink(c(temp_file, mask_file)))

  backend <- h5_backend(
    source = temp_file,
    mask_source = mask_file,
    data_dataset = "scan_data",
    mask_dataset = "brain_mask"
  )

  expect_equal(backend$data_dataset, "scan_data")
  expect_equal(backend$mask_dataset, "brain_mask")
})

test_that("h5_backend validates fmristore dependency", {
  # Skip this test if fmristore is actually available
  skip_if(requireNamespace("fmristore", quietly = TRUE), "fmristore is available")

  # If fmristore is not installed, h5_backend should error
  expect_error(
    h5_backend("test.h5", "mask.h5"),
    "Package 'fmristore' is required for H5 backend but is not available"
  )
})

test_that("fmri_h5_dataset constructor works", {
  skip_if_not_installed("fmristore")

  # Mock the h5_backend function to avoid file dependencies
  with_mocked_bindings(
    h5_backend = function(...) {
      structure(
        list(
          source = list(...)[["source"]],
          mask_source = list(...)[["mask_source"]],
          preload = FALSE
        ),
        class = c("h5_backend", "storage_backend")
      )
    },
    validate_backend = function(backend) TRUE,
    backend_open = function(backend) backend,
    backend_get_dims = function(backend) list(spatial = c(10, 10, 5), time = 100),
    {
      dataset <- fmri_h5_dataset(
        h5_files = c("scan1.h5", "scan2.h5"),
        mask_source = "mask.h5",
        TR = 2,
        run_length = c(50, 50)
      )

      expect_s3_class(dataset, "fmri_file_dataset")
      expect_s3_class(dataset, "fmri_dataset")
      expect_s3_class(dataset$backend, "h5_backend")
    }
  )
})

test_that("h5_backend handles base_path correctly", {
  skip_if_not_installed("fmristore")

  # Mock the h5_backend function
  h5_backend_calls <- list()
  with_mocked_bindings(
    h5_backend = function(...) {
      h5_backend_calls <<- append(h5_backend_calls, list(list(...)))
      structure(list(), class = c("h5_backend", "storage_backend"))
    },
    validate_backend = function(backend) TRUE,
    backend_open = function(backend) backend,
    backend_get_dims = function(backend) list(spatial = c(10, 10, 5), time = 50),
    {
      dataset <- fmri_h5_dataset(
        h5_files = "scan.h5",
        mask_source = "mask.h5",
        TR = 2,
        run_length = 50,
        base_path = "/path/to/data"
      )

      # Check that base_path was properly prepended
      call_args <- h5_backend_calls[[1]]
      expect_equal(call_args$source, "/path/to/data/scan.h5")
      expect_equal(call_args$mask_source, "/path/to/data/mask.h5")
    }
  )
})

test_that("h5_backend error handling works correctly", {
  skip_if_not_installed("fmristore")

  # Create temporary files to pass file existence check
  temp_file <- tempfile(fileext = ".h5")
  file.create(temp_file)
  on.exit(unlink(temp_file))

  # Test with invalid mask source type
  expect_error(
    h5_backend(temp_file, 123),
    "mask_source must be file path, NeuroVol, or H5NeuroVol object"
  )
})

test_that("h5_backend integration with storage_backend interface", {
  skip_if_not_installed("fmristore")

  # Test that h5_backend properly inherits from storage_backend
  backend <- structure(
    list(
      source = character(0),
      mask_source = character(0),
      preload = FALSE
    ),
    class = c("h5_backend", "storage_backend")
  )

  expect_s3_class(backend, "storage_backend")
  expect_true(inherits(backend, "h5_backend"))

  # Test that all required methods exist
  expect_true(exists("backend_open.h5_backend"))
  expect_true(exists("backend_close.h5_backend"))
  expect_true(exists("backend_get_dims.h5_backend"))
  expect_true(exists("backend_get_mask.h5_backend"))
  expect_true(exists("backend_get_data.h5_backend"))
  expect_true(exists("backend_get_metadata.h5_backend"))
})

test_that("h5_backend structure validation", {
  skip_if_not_installed("fmristore")

  # Create temporary files
  temp_file <- tempfile(fileext = ".h5")
  mask_file <- tempfile(fileext = ".h5")
  file.create(c(temp_file, mask_file))
  on.exit(unlink(c(temp_file, mask_file)))

  backend <- h5_backend(
    source = temp_file,
    mask_source = mask_file
  )

  # Test that backend has all required fields
  expect_true("source" %in% names(backend))
  expect_true("mask_source" %in% names(backend))
  expect_true("preload" %in% names(backend))
  expect_true("data_dataset" %in% names(backend))
  expect_true("mask_dataset" %in% names(backend))
  expect_true("h5_objects" %in% names(backend))
  expect_true("mask" %in% names(backend))
  expect_true("dims" %in% names(backend))

  # Test default values
  expect_equal(backend$data_dataset, "data")
  expect_equal(backend$mask_dataset, "data/elements")
  expect_false(backend$preload)
  expect_null(backend$h5_objects)
  expect_null(backend$mask)
  expect_null(backend$dims)
})

test_that("h5_backend handles multiple source files", {
  skip_if_not_installed("fmristore")

  # Create temporary files
  temp_files <- paste0(tempfile(), c("_1.h5", "_2.h5", "_3.h5"))
  mask_file <- tempfile(fileext = ".h5")
  file.create(c(temp_files, mask_file))
  on.exit(unlink(c(temp_files, mask_file)))

  backend <- h5_backend(
    source = temp_files,
    mask_source = mask_file,
    preload = FALSE
  )

  expect_equal(length(backend$source), 3)
  expect_equal(backend$source, temp_files)
})

test_that("fmri_h5_dataset validates parameters", {
  skip_if_not_installed("fmristore")

  # Mock dependencies to focus on parameter validation
  with_mocked_bindings(
    h5_backend = function(...) structure(list(), class = c("h5_backend", "storage_backend")),
    validate_backend = function(backend) TRUE,
    backend_open = function(backend) backend,
    backend_get_dims = function(backend) list(spatial = c(10, 10, 5), time = 100),
    {
      # Test that function validates TR
      expect_error(
        fmri_h5_dataset(
          h5_files = "scan.h5",
          mask_source = "mask.h5",
          TR = -1, # Invalid TR
          run_length = 100
        ),
        "TR not greater than 0"
      )

      # Test that valid parameters work
      expect_silent({
        result <- fmri_h5_dataset(
          h5_files = c("scan1.h5", "scan2.h5"),
          mask_source = "mask.h5",
          TR = 2,
          run_length = c(50, 50) # Total matches mock time dimension
        )
      })
    }
  )
})

test_that("h5_backend basic functionality works", {
  skip_if_not_installed("fmristore")

  # Create temporary files
  temp_file <- tempfile(fileext = ".h5")
  mask_file <- tempfile(fileext = ".h5")
  file.create(c(temp_file, mask_file))
  on.exit(unlink(c(temp_file, mask_file)))

  # Test basic backend creation and method existence
  backend <- h5_backend(
    source = temp_file,
    mask_source = mask_file
  )

  # Test that we can call basic methods without errors
  expect_silent(backend_close(backend))

  # Test backend configuration
  expect_false(backend$preload)
  expect_equal(backend$source, temp_file)
  expect_equal(backend$mask_source, mask_file)
})
</file>

<file path="R/data_chunks.R">
#' @importFrom assertthat assert_that
#' @importFrom deflist deflist

#' @keywords internal
#' @noRd
data_chunk <- function(mat, voxel_ind, row_ind, chunk_num) {
  ret <- list(
    data = mat,
    voxel_ind = voxel_ind,
    row_ind = row_ind,
    chunk_num = chunk_num
  )

  class(ret) <- c("data_chunk", "list")
  ret
}

#' @keywords internal
#' @noRd
chunk_iter <- function(x, nchunks, get_chunk) {
  chunk_num <- 1

  nextEl <- function() {
    if (chunk_num > nchunks) {
      stop("StopIteration")
    } else {
      ret <- get_chunk(chunk_num)
      chunk_num <<- chunk_num + 1
      ret
    }
  }

  iter <- list(nchunks = nchunks, nextElem = nextEl)
  class(iter) <- c("chunkiter", "abstractiter", "iter")
  iter
}

#' Create Data Chunks for fmri_mem_dataset Objects
#'
#' This function creates data chunks for fmri_mem_dataset objects. It allows for the retrieval of run-wise or sequence-wise data chunks, as well as arbitrary chunks.
#'
#' @param x An object of class 'fmri_mem_dataset'.
#' @param nchunks The number of data chunks to create. Default is 1.
#' @param runwise If TRUE, the data chunks are created run-wise. Default is FALSE.
#' @param ... Additional arguments.
#'
#' @return A list of data chunks, with each chunk containing the data, voxel indices, row indices, and chunk number.
#' @importFrom neuroim2 series
#' @export
#'
#' @examples
#' \dontrun{
#' # Create a simple fmri_mem_dataset for demonstration
#' d <- c(10, 10, 10, 10)
#' nvec <- neuroim2::NeuroVec(array(rnorm(prod(d)), d), space = neuroim2::NeuroSpace(d))
#' mask <- neuroim2::LogicalNeuroVol(array(TRUE, d[1:3]), neuroim2::NeuroSpace(d[1:3]))
#' dset <- fmri_mem_dataset(list(nvec), mask, TR = 2)
#'
#' # Create an iterator with 5 chunks
#' iter <- data_chunks(dset, nchunks = 5)
#' `%do%` <- foreach::`%do%`
#' y <- foreach::foreach(chunk = iter) %do% {
#'   colMeans(chunk$data)
#' }
#' length(y) == 5
#'
#' # Create an iterator with 100 chunks
#' iter <- data_chunks(dset, nchunks = 100)
#' y <- foreach::foreach(chunk = iter) %do% {
#'   colMeans(chunk$data)
#' }
#' length(y) == 100
#'
#' # Create a "runwise" iterator
#' iter <- data_chunks(dset, runwise = TRUE)
#' y <- foreach::foreach(chunk = iter) %do% {
#'   colMeans(chunk$data)
#' }
#' length(y) == 1
#' }
data_chunks.fmri_mem_dataset <- function(x, nchunks = 1, runwise = FALSE, ...) {
  mask <- get_mask(x)
  # print("data chunks")
  # print(nchunks)
  get_run_chunk <- function(chunk_num) {
    bvec <- x$scans[[chunk_num]]
    voxel_ind <- which(mask > 0)
    # print(voxel_ind)
    row_ind <- which(blockids(x$sampling_frame) == chunk_num)
    ret <- data_chunk(neuroim2::series(bvec, voxel_ind),
      voxel_ind = voxel_ind,
      row_ind = row_ind,
      chunk_num = chunk_num
    )
  }

  get_seq_chunk <- function(chunk_num) {
    bvecs <- x$scans
    voxel_ind <- maskSeq[[chunk_num]]
    # print(voxel_ind)

    m <- do.call(rbind, lapply(bvecs, function(bv) neuroim2::series(bv, voxel_ind)))
    ret <- data_chunk(do.call(rbind, lapply(bvecs, function(bv) neuroim2::series(bv, voxel_ind))),
      voxel_ind = voxel_ind,
      row_ind = 1:nrow(m),
      chunk_num = chunk_num
    )
  }

  maskSeq <- NULL
  if (runwise) {
    chunk_iter(x, length(x$scans), get_run_chunk)
  } else if (nchunks == 1) {
    maskSeq <- one_chunk()
    chunk_iter(x, 1, get_seq_chunk)
    # } #else if (nchunks == dim(mask)[3]) {
    # maskSeq <<- slicewise_chunks(x)
    # chunk_iter(x, length(maskSeq), get_seq_chunk)
  } else {
    maskSeq <- arbitrary_chunks(x, nchunks)
    chunk_iter(x, length(maskSeq), get_seq_chunk)
  }
}


#' Create Data Chunks for fmri_file_dataset Objects
#'
#' This function creates data chunks for fmri_file_dataset objects. It allows for the retrieval of run-wise or sequence-wise data chunks, as well as arbitrary chunks.
#'
#' @param x An object of class 'fmri_file_dataset'.
#' @param nchunks The number of data chunks to create. Default is 1.
#' @param runwise If TRUE, the data chunks are created run-wise. Default is FALSE.
#' @param ... Additional arguments.
#'
#' @return A list of data chunks, with each chunk containing the data, voxel indices, row indices, and chunk number.
#' @export
data_chunks.fmri_file_dataset <- function(x, nchunks = 1, runwise = FALSE, ...) {
  maskSeq <- NULL

  if (!is.null(x$backend)) {
    # New backend path - stream data directly
    mask_vec <- backend_get_mask(x$backend)
    voxel_ind <- which(mask_vec)
    n_voxels <- sum(mask_vec)
    dims <- backend_get_dims(x$backend)

    get_run_chunk <- function(chunk_num) {
      # Get row indices for this run
      row_ind <- which(blockids(x$sampling_frame) == chunk_num)
      # Stream only the needed rows from backend
      mat <- backend_get_data(x$backend, rows = row_ind, cols = NULL)
      data_chunk(mat, voxel_ind = voxel_ind, row_ind = row_ind, chunk_num = chunk_num)
    }

    get_seq_chunk <- function(chunk_num) {
      # Get column indices for this chunk
      col_ind <- maskSeq[[chunk_num]]
      # Map voxel indices to valid column indices
      valid_cols <- match(col_ind, voxel_ind)
      valid_cols <- valid_cols[!is.na(valid_cols)]
      # Stream only the needed columns from backend
      mat <- backend_get_data(x$backend, rows = NULL, cols = valid_cols)
      data_chunk(mat, voxel_ind = col_ind, row_ind = 1:dims$time, chunk_num = chunk_num)
    }
  } else {
    # Legacy path
    mask <- get_mask(x)

    get_run_chunk <- function(chunk_num) {
      bvec <- neuroim2::read_vec(file.path(x$scans[chunk_num]), mask = mask)
      ret <- data_chunk(bvec@data,
        voxel_ind = which(x$mask > 0),
        row_ind = which(blockids(x$sampling_frame) == chunk_num),
        chunk_num = chunk_num
      )
    }

    get_seq_chunk <- function(chunk_num) {
      v <- get_data(x)
      vind <- maskSeq[[chunk_num]]
      m <- series(v, vind)
      ret <- data_chunk(m,
        voxel_ind = vind,
        row_ind = 1:nrow(x$event_table),
        chunk_num = chunk_num
      )
    }
  }


  # Then create iterator based on strategy
  if (runwise) {
    if (!is.null(x$backend)) {
      # For backend, use number of runs from sampling frame
      chunk_iter(x, x$nruns, get_run_chunk)
    } else {
      # Legacy path uses number of scan files
      chunk_iter(x, length(x$scans), get_run_chunk)
    }
  } else if (nchunks == 1) {
    maskSeq <- one_chunk(x)
    chunk_iter(x, 1, get_seq_chunk)
  } else {
    maskSeq <- arbitrary_chunks(x, nchunks)
    chunk_iter(x, length(maskSeq), get_seq_chunk)
  }
}


#' Create Data Chunks for matrix_dataset Objects
#'
#' This function creates data chunks for matrix_dataset objects. It allows for the retrieval
#' of run-wise or sequence-wise data chunks, as well as arbitrary chunks.
#'
#' @param x An object of class 'matrix_dataset'
#' @param nchunks The number of chunks to split the data into. Default is 1.
#' @param runwise If TRUE, creates run-wise chunks instead of arbitrary chunks
#' @param ... Additional arguments passed to methods
#' @return A list of data chunks, each containing data, indices and chunk number
#' @export
data_chunks.matrix_dataset <- function(x, nchunks = 1, runwise = FALSE, ...) {
  get_run_chunk <- function(chunk_num) {
    ind <- which(blockids(x$sampling_frame) == chunk_num)
    mat <- x$datamat[ind, , drop = FALSE]
    # browser()
    data_chunk(mat, voxel_ind = 1:ncol(mat), row_ind = ind, chunk_num = chunk_num)
  }

  get_one_chunk <- function(chunk_num) {
    data_chunk(x$datamat, voxel_ind = 1:ncol(x$datamat), row_ind = 1:nrow(x$datamat), chunk_num = chunk_num)
  }

  if (runwise) {
    chunk_iter(x, length(blocklens(x$sampling_frame)), get_run_chunk)
  } else if (nchunks == 1) {
    chunk_iter(x, 1, get_one_chunk)
  } else {
    sidx <- split(1:ncol(x$datamat), sort(rep(1:nchunks, length.out = ncol(x$datamat))))
    get_chunk <- function(chunk_num) {
      data_chunk(x$datamat[, sidx[[chunk_num]], drop = FALSE],
        voxel_ind = sidx[[chunk_num]],
        row_ind = 1:nrow(x$datamat),
        chunk_num = chunk_num
      )
    }
    chunk_iter(x, nchunks, get_chunk)
  }
}

#' Create an Execution Strategy for Data Processing
#'
#' This function creates an execution strategy that can be used to process
#' fMRI datasets in different ways: voxelwise, runwise, or chunkwise.
#'
#' @param strategy Character string specifying the processing strategy.
#'   Options are "voxelwise", "runwise", or "chunkwise".
#' @param nchunks Number of chunks to use for "chunkwise" strategy.
#'   Ignored for other strategies.
#' @return A function that takes a dataset and returns a chunk iterator
#'   configured according to the specified strategy.
#' @export
exec_strategy <- function(strategy = c("voxelwise", "runwise", "chunkwise"), nchunks = NULL) {
  strategy <- match.arg(strategy)

  function(dset) {
    if (strategy == "runwise") {
      data_chunks(dset, runwise = TRUE)
    } else if (strategy == "voxelwise") {
      m <- get_mask(dset)
      data_chunks(dset, nchunks = sum(m), runwise = FALSE)
    } else if (strategy == "chunkwise") {
      m <- get_mask(dset)
      ## message("nchunks is", nchunks)
      assert_that(!is.null(nchunks) && is.numeric(nchunks))
      if (nchunks > sum(m)) {
        warning("requested number of chunks is greater than number of voxels in mask")
        nchunks <- sum(m)
      }
      data_chunks(dset, nchunks = nchunks, runwise = FALSE)
    }
  }
}

#' Collect all chunks from a chunk iterator
#'
#' This function collects all chunks from a chunk iterator into a list.
#'
#' @param chunk_iter A chunk iterator object created by chunk_iter()
#' @return A list containing all chunks from the iterator
#' @export
collect_chunks <- function(chunk_iter) {
  chunks <- list()
  for (i in seq_len(chunk_iter$nchunks)) {
    chunks[[i]] <- chunk_iter$nextElem()
  }
  chunks
}


#' @keywords internal
#' @noRd
#' @importFrom deflist deflist
arbitrary_chunks <- function(x, nchunks) {
  # print("arbitrary chunks")
  # browser()
  mask <- get_mask(x)
  # print(mask)
  indices <- as.integer(which(mask != 0))

  # If more chunks requested than voxels, cap to number of voxels
  if (nchunks > length(indices)) {
    warning("requested number of chunks (", nchunks, ") is greater than number of voxels (", length(indices), "). Using ", length(indices), " chunks instead.")
    nchunks <- length(indices)
  }

  chsize <- round(length(indices) / nchunks)
  # print(indices)

  assert_that(chsize > 0)
  chunkids <- sort(rep(1:nchunks, each = chsize, length.out = length(indices)))
  # print(chunkids)

  mfun <- function(i) indices[chunkids == i]
  # print(mfun)

  ret <- deflist::deflist(mfun, len = nchunks)
  # print(ret[[1]])
  return(ret)
}

#' @keywords internal
#' @noRd
slicewise_chunks <- function(x) {
  mask <- x$mask
  template <- neuroim2::NeuroVol(array(0, dim(mask)), neuroim2::space(mask))
  nchunks <- dim(mask)[3]

  maskSeq <- lapply(1:nchunks, function(i) {
    m <- template
    m[, , i] <- 1
    m
  })

  maskSeq
}

#' @keywords internal
#' @noRd
one_chunk <- function(x) {
  mask <- get_mask(x)
  voxel_ind <- which(mask > 0)
  list(voxel_ind)
}
</file>

<file path="tests/testthat/test_latent_backend.R">
test_that("latent_dataset errors when fmristore is absent", {
  with_mocked_bindings(
    requireNamespace = function(pkg, quietly = TRUE) FALSE,
    .package = "base",
    {
      expect_error(
        latent_dataset(list(), TR = 2, run_length = 10),
        "fmristore"
      )
    }
  )
})

if (!methods::isClass("MockLatentNeuroVec")) {
  setClass(
    "MockLatentNeuroVec",
    slots = c(basis = "matrix", loadings = "matrix", mask = "logical")
  )
  setMethod(
    "dim",
    "MockLatentNeuroVec",
    function(x) c(2, 2, 2, nrow(x@basis))
  )
}

create_mock_lvec <- function(n_time = 5, n_vox = 10, k = 4) {
  basis <- matrix(seq_len(n_time * k), nrow = n_time, ncol = k)
  loadings <- matrix(seq_len(n_vox * k), nrow = n_vox, ncol = k)
  mask <- rep(TRUE, n_vox)
  new("MockLatentNeuroVec", basis = basis, loadings = loadings, mask = mask)
}

test_that("latent_dataset constructs dataset from minimal latent object", {
  lvec <- create_mock_lvec()
  rl <- dim(lvec)[4]
  with_mocked_bindings(
    requireNamespace = function(pkg, quietly = TRUE) TRUE,
    .package = "base",
    {
      dset <- latent_dataset(lvec, TR = 1, run_length = rl)
      expect_s3_class(dset, "latent_dataset")
      expect_identical(get_data(dset), lvec@basis)
      expect_identical(get_mask(dset), lvec@mask)
      expect_equal(blocklens(dset), rl)
    }
  )
})

test_that("latent_dataset validates run_length sum", {
  lvec <- create_mock_lvec()
  with_mocked_bindings(
    requireNamespace = function(pkg, quietly = TRUE) TRUE,
    .package = "base",
    {
      expect_error(
        latent_dataset(lvec, TR = 1, run_length = dim(lvec)[4] - 1),
        "Sum of run lengths"
      )
    }
  )
})

test_that("latent_backend constructor works with validation", {
  skip_if_not_installed("fmristore")

  # Test validation - should fail for non-existent files
  expect_error(
    latent_backend(c("nonexistent1.lv.h5", "nonexistent2.lv.h5")),
    "All source files must exist"
  )

  # Test validation - should fail for non-HDF5 files
  temp_file <- tempfile(fileext = ".txt")
  writeLines("test", temp_file)
  on.exit(unlink(temp_file))

  expect_error(
    latent_backend(temp_file),
    "All source files must be HDF5 files"
  )

  # Test validation - mixed list with invalid items
  expect_error(
    latent_backend(list("nonexistent.lv.h5", 123)),
    "Source item 1 must be an existing file path"
  )
})

test_that("latent_backend works with mock LatentNeuroVec objects", {
  skip_if_not_installed("fmristore")

  # Create simple mock LatentNeuroVec objects for testing
  mock_lvec1 <- structure(list(), class = "LatentNeuroVec")
  mock_lvec2 <- structure(list(), class = "LatentNeuroVec")

  # Test backend creation with list of objects
  backend <- latent_backend(list(mock_lvec1, mock_lvec2))

  expect_s3_class(backend, "latent_backend")
  expect_s3_class(backend, "storage_backend")
  expect_equal(length(backend$source), 2)
  expect_false(backend$is_open)
  expect_false(backend$preload)
})

test_that("latent_backend constructor works with preload option", {
  skip_if_not_installed("fmristore")

  mock_lvec <- structure(list(), class = "LatentNeuroVec")

  # Test with preload = TRUE
  backend_preload <- latent_backend(list(mock_lvec), preload = TRUE)
  expect_true(backend_preload$preload)

  # Test with preload = FALSE (default)
  backend_lazy <- latent_backend(list(mock_lvec))
  expect_false(backend_lazy$preload)
})

test_that("latent_backend error handling works correctly", {
  # Test backend not open errors
  backend <- structure(list(is_open = FALSE), class = c("latent_backend", "storage_backend"))

  expect_error(backend_get_dims(backend), "Backend must be opened")
  expect_error(backend_get_mask(backend), "Backend must be opened")
  expect_error(backend_get_data(backend), "Backend must be opened")
  expect_error(backend_get_metadata(backend), "Backend must be opened")

  # Test no data errors
  backend$is_open <- TRUE
  backend$data <- list()

  expect_error(backend_get_dims(backend), "No data available")
  expect_error(backend_get_mask(backend), "No data available")
  expect_error(backend_get_data(backend), "No data available")
  expect_error(backend_get_metadata(backend), "No data available")
})

test_that("latent_backend class structure is correct", {
  skip_if_not_installed("fmristore")

  mock_lvec <- structure(list(), class = "LatentNeuroVec")
  backend <- latent_backend(list(mock_lvec))

  # Check class hierarchy
  expect_true(inherits(backend, "latent_backend"))
  expect_true(inherits(backend, "storage_backend"))

  # Check structure
  expect_named(backend, c("source", "mask_source", "preload", "data", "is_open"))
  expect_false(backend$is_open)
  expect_null(backend$mask_source)
  expect_null(backend$data)
})

test_that("latent_backend validates input types correctly", {
  skip_if_not_installed("fmristore")

  # Test invalid preload argument
  mock_lvec <- structure(list(), class = "LatentNeuroVec")
  expect_error(
    latent_backend(list(mock_lvec), preload = "invalid"),
    "is.logical\\(preload\\) is not TRUE"
  )

  # Test invalid source type
  expect_error(
    latent_backend(123),
    "source must be a character vector"
  )
})

test_that("fmri_latent_dataset constructor parameter validation", {
  skip_if_not_installed("fmristore")

  # Create a simple mock backend for testing parameter validation
  mock_lvec <- structure(list(), class = "LatentNeuroVec")
  backend <- latent_backend(list(mock_lvec))

  # Mock the required functions for validation
  mockery::stub(fmri_latent_dataset, "validate_backend", TRUE)
  mockery::stub(fmri_latent_dataset, "backend_open", function(b) {
    b$is_open <- TRUE
    b$data <- list(mock_lvec)
    b
  })
  mockery::stub(
    fmri_latent_dataset, "backend_get_dims",
    function(b) list(space = c(10, 10, 5), time = 100, n_runs = 1)
  )

  # Test successful creation
  dataset <- fmri_latent_dataset(backend, TR = 2, run_length = 100)

  expect_s3_class(dataset, "fmri_file_dataset")
  expect_s3_class(dataset, "volumetric_dataset")
  expect_s3_class(dataset, "fmri_dataset")
  expect_equal(dataset$nruns, 1)

  # Test dimension mismatch error
  mockery::stub(
    fmri_latent_dataset, "backend_get_dims",
    function(b) list(space = c(10, 10, 5), time = 100, n_runs = 1)
  )

  expect_error(
    fmri_latent_dataset(backend, TR = 2, run_length = 50), # Wrong run_length
    "Sum of run_length.*must equal total time points"
  )
})

test_that("latent backend behavior is documented correctly", {
  # Test that the documentation correctly states that latent backends
  # return latent scores, not voxel data

  # This is a documentation test to ensure the key difference is clear
  expect_true(TRUE) # The documentation has been updated to reflect this behavior
})

test_that("latent_backend concept validation", {
  skip_if_not_installed("fmristore")

  # Test that the key conceptual differences are understood:
  # 1. Data returned should be latent scores (time x components)
  # 2. Mask represents components, not spatial voxels
  # 3. Purpose is efficient analysis in compressed space

  # These are conceptual tests - the actual implementation would require
  # real LatentNeuroVec objects which are complex S4 objects
  expect_true(TRUE) # Implementation follows these principles
})
</file>

<file path="DESCRIPTION">
Package: fmridataset
Type: Package
Title: Unified Container for fMRI Datasets
Version: 0.1.0
Authors@R: person("Bradley", "Buchsbaum", 
                  email = "bbuchsbaum@gmail.com", 
                  role = c("aut", "cre"),
                  comment = c(ORCID = "0000-0001-5800-9890"))
Description: Provides a unified S3 class 'fmri_dataset' for representing 
    functional magnetic resonance imaging (fMRI) data from various sources 
    including raw NIfTI files, BIDS projects, pre-loaded NeuroVec objects, 
    and in-memory matrices. Features lazy loading, flexible data access 
    patterns, and integration with neuroimaging analysis workflows.
License: GPL (>= 3)
Encoding: UTF-8
LazyData: true
Roxygen: list(markdown = TRUE)
RoxygenNote: 7.3.2.9000
Depends: 
    R (>= 4.3.0)
Imports:
    assertthat,
    deflist,
    fmrihrf,
    memoise,
    neuroim2,
    purrr,
    tibble,
    utils
Suggests:
    bench,
    bidser,
    crayon,
    fmristore,
    foreach,
    testthat (>= 3.0.0),
    knitr,
    rmarkdown
VignetteBuilder: knitr
URL: https://github.com/bbuchsbaum/fmridataset, https://bbuchsbaum.github.io/fmridataset/
BugReports: https://github.com/bbuchsbaum/fmridataset/issues
Remotes:
    bbuchsbaum/fmristore,
    bbuchsbaum/bidser
</file>

<file path="R/dataset_constructors.R">
#' @importFrom assertthat assert_that
#' @importFrom purrr map_lgl
#' @importFrom tibble as_tibble
NULL

#' Matrix Dataset Constructor
#'
#' This function creates a matrix dataset object, which is a list containing
#' information about the data matrix, TR, number of runs, event table,
#' sampling frame, and mask.
#'
#' @param datamat A matrix where each column is a voxel time-series.
#' @param TR Repetition time (TR) of the fMRI acquisition.
#' @param run_length A numeric vector specifying the length of each run in the dataset.
#' @param event_table An optional data frame containing event information. Default is an empty data frame.
#'
#' @return A matrix dataset object of class c("matrix_dataset", "fmri_dataset", "list").
#' @export
#'
#' @examples
#' # A matrix with 100 rows and 100 columns (voxels)
#' X <- matrix(rnorm(100 * 100), 100, 100)
#' dset <- matrix_dataset(X, TR = 2, run_length = 100)
matrix_dataset <- function(datamat, TR, run_length, event_table = data.frame()) {
  if (is.vector(datamat)) {
    datamat <- as.matrix(datamat)
  }
  assert_that(sum(run_length) == nrow(datamat))

  frame <- fmrihrf::sampling_frame(blocklens = run_length, TR = TR)

  # For backward compatibility, keep the original structure
  # but could optionally add backend support in the future
  ret <- list(
    datamat = datamat,
    TR = TR,
    nruns = length(run_length),
    event_table = event_table,
    sampling_frame = frame,
    mask = rep(1, ncol(datamat))
  )

  class(ret) <- c("matrix_dataset", "fmri_dataset", "list")
  ret
}

#' Create an fMRI Memory Dataset Object
#'
#' This function creates an fMRI memory dataset object, which is a list containing information about the scans, mask, TR, number of runs, event table, base path, sampling frame, and censor.
#'
#' @param scans A list of objects of class \code{NeuroVec} from the neuroim2 package.
#' @param mask A binary mask of class \code{NeuroVol} from the neuroim2 package indicating the set of voxels to include in analyses.
#' @param TR Repetition time (TR) of the fMRI acquisition.
#' @param run_length A numeric vector specifying the length of each run in the dataset. Default is the length of the scans.
#' @param event_table An optional data frame containing event information. Default is an empty data frame.
#' @param base_path An optional base path for the dataset. Default is "." (current directory).
#' @param censor An optional numeric vector specifying which time points to censor. Default is NULL.
#'
#' @return An fMRI memory dataset object of class c("fmri_mem_dataset", "volumetric_dataset", "fmri_dataset", "list").
#' @export
#'
#' @examples
#' # Create a NeuroVec object
#' d <- c(10, 10, 10, 10)
#' nvec <- neuroim2::NeuroVec(array(rnorm(prod(d)), d), space = neuroim2::NeuroSpace(d))
#'
#' # Create a NeuroVol mask
#' mask <- neuroim2::NeuroVol(array(rnorm(10 * 10 * 10), d[1:3]), space = neuroim2::NeuroSpace(d[1:3]))
#' mask[mask < .5] <- 0
#'
#' # Create an fmri_mem_dataset
#' dset <- fmri_mem_dataset(list(nvec), mask, TR = 2)
fmri_mem_dataset <- function(scans, mask, TR,
                             run_length = sapply(scans, function(x) dim(x)[4]),
                             event_table = data.frame(),
                             base_path = ".",
                             censor = NULL) {
  assert_that(all(map_lgl(scans, function(x) inherits(x, "NeuroVec"))))
  assert_that(inherits(mask, "NeuroVol"))
  assert_that(all(dim(mask) == dim(scans[[1]][1:3])))

  ntotscans <- sum(sapply(scans, function(x) dim(x)[4]))
  # run_length <- map_dbl(scans, ~ dim(.)[4])
  assert_that(sum(run_length) == ntotscans)

  if (is.null(censor)) {
    censor <- rep(0, sum(run_length))
  }

  frame <- fmrihrf::sampling_frame(blocklens = run_length, TR = TR)

  ret <- list(
    scans = scans,
    mask = mask,
    nruns = length(run_length),
    event_table = event_table,
    base_path = base_path,
    sampling_frame = frame,
    censor = censor
  )

  class(ret) <- c("fmri_mem_dataset", "volumetric_dataset", "fmri_dataset", "list")
  ret
}

#' Create a Latent Dataset Object
#'
#' This function creates a latent dataset object, which encapsulates a dimension-reduced
#' subspace of "latent variables". The dataset is a list containing information about the latent
#' neuroimaging vector, TR, number of runs, event table, base path, sampling frame, and censor.
#'
#' @param lvec An instance of class \code{LatentNeuroVec}. (Typically, a \code{LatentNeuroVec} is
#'   created using the \code{fmristore} package.)
#' @param TR Repetition time (TR) of the fMRI acquisition.
#' @param run_length A numeric vector specifying the length of each run in the dataset.
#' @param event_table An optional data frame containing event information. Default is an empty data frame.
#'
#' @return A latent dataset object of class \code{c("latent_dataset", "matrix_dataset", "fmri_dataset", "list")}.
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # Create a matrix with 100 rows and 1000 columns (voxels)
#' X <- matrix(rnorm(100 * 1000), 100, 1000)
#' pres <- prcomp(X)
#' basis <- pres$x[, 1:25]
#' loadings <- pres$rotation[, 1:25]
#' offset <- colMeans(X)
#'
#' # Create a LatentNeuroVec object (requires the fmristore package)
#' lvec <- fmristore::LatentNeuroVec(basis, loadings,
#'   neuroim2::NeuroSpace(c(10, 10, 10, 100)),
#'   mask = rep(TRUE, 1000), offset = offset
#' )
#'
#' # Create a latent_dataset
#' dset <- latent_dataset(lvec, TR = 2, run_length = 100)
#' }
latent_dataset <- function(lvec, TR, run_length, event_table = data.frame()) {
  # Lazy check: make sure fmristore is installed (fmristore is not a hard dependency)
  if (!requireNamespace("fmristore", quietly = TRUE)) {
    stop("The 'fmristore' package is required to create a latent_dataset. Please install fmristore.",
      call. = FALSE
    )
  }

  # Ensure the total run length matches the number of time points in lvec
  assertthat::assert_that(
    sum(run_length) == dim(lvec)[4],
    msg = "Sum of run lengths must equal the 4th dimension of lvec"
  )

  frame <- fmrihrf::sampling_frame(blocklens = run_length, TR = TR)

  ret <- list(
    lvec = lvec,
    datamat = lvec@basis,
    TR = TR,
    nruns = length(run_length),
    event_table = event_table,
    sampling_frame = frame,
    mask = rep(1, ncol(lvec@basis))
  )

  class(ret) <- c("latent_dataset", "matrix_dataset", "fmri_dataset", "list")
  ret
}

#' Create an fMRI Dataset Object from LatentNeuroVec Files or Objects
#'
#' This function creates an fMRI dataset object from LatentNeuroVec files (.lv.h5) or objects
#' using the new backend architecture. LatentNeuroVec represents data in a compressed latent
#' space using basis functions and spatial loadings.
#'
#' @param latent_files A character vector of file paths to LatentNeuroVec HDF5 files (.lv.h5),
#'   or a list of LatentNeuroVec objects, or a pre-created latent_backend object.
#' @param mask_source Optional mask source. If NULL, the mask will be extracted from
#'   the first LatentNeuroVec object.
#' @param TR The repetition time in seconds of the scan-to-scan interval.
#' @param run_length A vector of one or more integers indicating the number of scans in each run.
#' @param event_table A data.frame containing the event onsets and experimental variables. Default is an empty data.frame.
#' @param base_path The file path to be prepended to relative file names. Default is "." (current directory).
#' @param censor A binary vector indicating which scans to remove. Default is NULL.
#' @param preload Read LatentNeuroVec objects eagerly rather than on first access. Default is FALSE.
#'
#' @return An fMRI dataset object of class c("fmri_file_dataset", "volumetric_dataset", "fmri_dataset", "list").
#'
#' @details
#' This function uses the latent_backend to handle LatentNeuroVec data efficiently.
#' LatentNeuroVec objects store fMRI data in a compressed format using:
#' - Basis functions (temporal components)
#' - Spatial loadings (voxel weights)
#' - Optional offset terms
#'
#' This is particularly efficient for data that can be well-represented by a
#' lower-dimensional basis (e.g., from PCA, ICA, or dictionary learning).
#'
#' **CRITICAL: Data Access in Latent Space**
#' Unlike standard fMRI datasets that return voxel-wise data, this dataset returns
#' **latent scores** (temporal basis components) rather than reconstructed voxel data.
#' The data matrix dimensions are (time × components), not (time × voxels). This is because:
#'
#' - Time-series analyses should be performed in the efficient latent space
#' - The latent scores capture temporal dynamics in the compressed representation
#' - Reconstructing to full voxel space defeats the compression benefits
#' - Most analysis workflows (GLM, connectivity, etc.) work directly with these temporal patterns
#'
#' Use this dataset when you want to analyze temporal dynamics in the latent space.
#' If you need full voxel reconstruction, use the reconstruction methods from fmristore directly.
#'
#' @export
#'
#' @examples
#' \dontrun{
#' # Create an fMRI dataset from LatentNeuroVec HDF5 files
#' dset <- fmri_latent_dataset(
#'   latent_files = c("run1.lv.h5", "run2.lv.h5", "run3.lv.h5"),
#'   TR = 2,
#'   run_length = c(150, 150, 150)
#' )
#'
#' # Create from pre-loaded LatentNeuroVec objects
#' lvec1 <- fmristore::read_vec("run1.lv.h5")
#' lvec2 <- fmristore::read_vec("run2.lv.h5")
#' dset <- fmri_latent_dataset(
#'   latent_files = list(lvec1, lvec2),
#'   TR = 2,
#'   run_length = c(100, 100)
#' )
#'
#' # Create from a latent_backend
#' backend <- latent_backend(c("run1.lv.h5", "run2.lv.h5"))
#' dset <- fmri_latent_dataset(backend, TR = 2, run_length = c(100, 100))
#' }
#'
#' @seealso
#' \code{\link{latent_backend}}, \code{\link{latent_dataset}}, \code{\link{fmri_h5_dataset}}
fmri_latent_dataset <- function(latent_files, mask_source = NULL, TR,
                                run_length,
                                event_table = data.frame(),
                                base_path = ".",
                                censor = NULL,
                                preload = FALSE) {
  # Check if latent_files is actually a backend object
  if (inherits(latent_files, "latent_backend")) {
    backend <- latent_files
  } else {
    # Create a latent_backend from the input
    if (is.character(latent_files)) {
      # File paths - prepend base_path if needed for relative paths
      latent_files <- ifelse(
        grepl("^(/|[A-Za-z]:)", latent_files), # Check if absolute path
        latent_files,
        file.path(base_path, latent_files)
      )
    }

    backend <- latent_backend(
      source = latent_files,
      mask_source = mask_source,
      preload = preload
    )
  }

  # Validate backend
  validate_backend(backend)

  # Open backend to initialize resources
  backend <- backend_open(backend)

  if (is.null(censor)) {
    censor <- rep(0, sum(run_length))
  }

  frame <- sampling_frame(run_length, TR)

  # Get dimensions to validate run_length
  dims <- backend_get_dims(backend)
  assert_that(sum(run_length) == dims$time,
    msg = sprintf(
      "Sum of run_length (%d) must equal total time points (%d)",
      sum(run_length), dims$time
    )
  )

  ret <- list(
    backend = backend,
    nruns = length(run_length),
    event_table = suppressMessages(tibble::as_tibble(event_table, .name_repair = "check_unique")),
    sampling_frame = frame,
    censor = censor
  )

  class(ret) <- c("fmri_file_dataset", "volumetric_dataset", "fmri_dataset", "list")
  ret
}

#' Create an fMRI Dataset Object from a Set of Scans
#'
#' This function creates an fMRI dataset object from a set of scans, design information, and other data.
#' The new implementation uses a pluggable backend architecture.
#'
#' @param scans A vector of one or more file names of the images comprising the dataset,
#'   or a pre-created storage backend object.
#' @param mask Name of the binary mask file indicating the voxels to include in the analysis.
#'   Ignored if scans is a backend object.
#' @param TR The repetition time in seconds of the scan-to-scan interval.
#' @param run_length A vector of one or more integers indicating the number of scans in each run.
#' @param event_table A data.frame containing the event onsets and experimental variables. Default is an empty data.frame.
#' @param base_path The file path to be prepended to relative file names. Default is "." (current directory).
#' @param censor A binary vector indicating which scans to remove. Default is NULL.
#' @param preload Read image scans eagerly rather than on first access. Default is FALSE.
#' @param mode The type of storage mode ('normal', 'bigvec', 'mmap', filebacked'). Default is 'normal'.
#'   Ignored if scans is a backend object.
#' @param backend Deprecated. Use scans parameter to pass a backend object.
#'
#' @return An fMRI dataset object of class c("fmri_file_dataset", "volumetric_dataset", "fmri_dataset", "list").
#' @export
#'
#' @examples
#' \dontrun{
#' # Create an fMRI dataset with 3 scans and a mask
#' dset <- fmri_dataset(c("scan1.nii", "scan2.nii", "scan3.nii"),
#'   mask = "mask.nii", TR = 2, run_length = rep(300, 3),
#'   event_table = data.frame(
#'     onsets = c(3, 20, 99, 3, 20, 99, 3, 20, 99),
#'     run = c(1, 1, 1, 2, 2, 2, 3, 3, 3)
#'   )
#' )
#'
#' # Create an fMRI dataset with 1 scan and a mask
#' dset <- fmri_dataset("scan1.nii",
#'   mask = "mask.nii", TR = 2,
#'   run_length = 300,
#'   event_table = data.frame(onsets = c(3, 20, 99), run = rep(1, 3))
#' )
#'
#' # Create an fMRI dataset with a backend
#' backend <- nifti_backend(c("scan1.nii", "scan2.nii"), mask_source = "mask.nii")
#' dset <- fmri_dataset(backend, TR = 2, run_length = c(150, 150))
#' }
fmri_dataset <- function(scans, mask = NULL, TR,
                         run_length,
                         event_table = data.frame(),
                         base_path = ".",
                         censor = NULL,
                         preload = FALSE,
                         mode = c("normal", "bigvec", "mmap", "filebacked"),
                         backend = NULL) {
  # Check if scans is actually a backend object
  if (inherits(scans, "storage_backend")) {
    backend <- scans
  } else if (!is.null(backend)) {
    warning("backend parameter is deprecated. Pass backend as first argument.")
  } else {
    # Legacy path: create a NiftiBackend from file paths
    assert_that(is.character(mask), msg = "'mask' should be the file name of the binary mask file")
    mode <- match.arg(mode)

    maskfile <- paste0(base_path, "/", mask)
    scan_files <- paste0(base_path, "/", scans)

    backend <- nifti_backend(
      source = scan_files,
      mask_source = maskfile,
      preload = preload,
      mode = mode
    )
  }

  # Validate backend
  validate_backend(backend)

  # Open backend to initialize resources
  backend <- backend_open(backend)

  if (is.null(censor)) {
    censor <- rep(0, sum(run_length))
  }

  frame <- fmrihrf::sampling_frame(blocklens = run_length, TR = TR)

  # Get dimensions to validate run_length
  dims <- backend_get_dims(backend)
  assert_that(sum(run_length) == dims$time,
    msg = sprintf(
      "Sum of run_length (%d) must equal total time points (%d)",
      sum(run_length), dims$time
    )
  )

  ret <- list(
    backend = backend,
    nruns = length(run_length),
    event_table = suppressMessages(tibble::as_tibble(event_table, .name_repair = "check_unique")),
    sampling_frame = frame,
    censor = censor
  )

  class(ret) <- c("fmri_file_dataset", "volumetric_dataset", "fmri_dataset", "list")
  ret
}

#' Create an fMRI Dataset Object from H5 Files
#'
#' This function creates an fMRI dataset object specifically from H5 files using the fmristore package.
#' Each scan is stored as an H5 file that loads to an H5NeuroVec object.
#'
#' @param h5_files A vector of one or more file paths to H5 files containing the fMRI data.
#' @param mask_source File path to H5 mask file, regular mask file, or in-memory NeuroVol object.
#' @param TR The repetition time in seconds of the scan-to-scan interval.
#' @param run_length A vector of one or more integers indicating the number of scans in each run.
#' @param event_table A data.frame containing the event onsets and experimental variables. Default is an empty data.frame.
#' @param base_path The file path to be prepended to relative file names. Default is "." (current directory).
#' @param censor A binary vector indicating which scans to remove. Default is NULL.
#' @param preload Read H5NeuroVec objects eagerly rather than on first access. Default is FALSE.
#' @param mask_dataset Character string specifying the dataset path within H5 file for mask (default: "data/elements").
#' @param data_dataset Character string specifying the dataset path within H5 files for data (default: "data").
#'
#' @return An fMRI dataset object of class c("fmri_file_dataset", "volumetric_dataset", "fmri_dataset", "list").
#' @export
#'
#' @examples
#' \dontrun{
#' # Create an fMRI dataset with H5NeuroVec files (standard fmristore format)
#' dset <- fmri_h5_dataset(
#'   h5_files = c("scan1.h5", "scan2.h5", "scan3.h5"),
#'   mask_source = "mask.h5",
#'   TR = 2,
#'   run_length = c(150, 150, 150)
#' )
#'
#' # Create an fMRI dataset with H5 files and NIfTI mask
#' dset <- fmri_h5_dataset(
#'   h5_files = "single_scan.h5",
#'   mask_source = "mask.nii",
#'   TR = 2,
#'   run_length = 300
#' )
#'
#' # Custom dataset paths (if using non-standard H5 structure)
#' dset <- fmri_h5_dataset(
#'   h5_files = "custom_scan.h5",
#'   mask_source = "custom_mask.h5",
#'   TR = 2,
#'   run_length = 200,
#'   data_dataset = "my_data_path",
#'   mask_dataset = "my_mask_path"
#' )
#' }
fmri_h5_dataset <- function(h5_files, mask_source, TR,
                            run_length,
                            event_table = data.frame(),
                            base_path = ".",
                            censor = NULL,
                            preload = FALSE,
                            mask_dataset = "data/elements",
                            data_dataset = "data") {
  # Prepare file paths
  h5_file_paths <- if (base_path != ".") {
    paste0(base_path, "/", h5_files)
  } else {
    h5_files
  }

  mask_file_path <- if (is.character(mask_source) && base_path != ".") {
    paste0(base_path, "/", mask_source)
  } else {
    mask_source
  }

  # Create H5 backend
  backend <- h5_backend(
    source = h5_file_paths,
    mask_source = mask_file_path,
    mask_dataset = mask_dataset,
    data_dataset = data_dataset,
    preload = preload
  )

  # Use the generic fmri_dataset constructor with the H5 backend
  fmri_dataset(
    scans = backend,
    TR = TR,
    run_length = run_length,
    event_table = event_table,
    censor = censor
  )
}
</file>

</files>
