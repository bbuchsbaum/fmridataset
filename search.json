[{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":null,"dir":"","previous_headings":"","what":"fmridataset API Safety and Usability Analysis","title":"fmridataset API Safety and Usability Analysis","text":"document analyzes fmridataset package API safety usability issues, providing specific examples backward-compatible improvement suggestions.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_11-multiple-incompatible-type-acceptance-without-clear-validation","dir":"","previous_headings":"1. Type Safety Issues","what":"1.1 Multiple Incompatible Type Acceptance Without Clear Validation","title":"fmridataset API Safety and Usability Analysis","text":"Issue: Several functions accept multiple types don’t validate provide clear error messages: Problems: - Silently coerces vectors matrices without warning - validation coercion produces expected dimensions - run_length can single value vector, clear documentation Suggested Improvement:","code":"# In dataset_constructors.R matrix_dataset <- function(datamat, TR, run_length, event_table = data.frame()) {   if (is.vector(datamat)) {     datamat <- as.matrix(datamat)  # Silent coercion   }   assert_that(sum(run_length) == nrow(datamat))   # ... } matrix_dataset <- function(datamat, TR, run_length, event_table = data.frame()) {   # Explicit type checking with informative messages   if (is.vector(datamat)) {     message(\"Converting vector to single-column matrix\")     datamat <- as.matrix(datamat)   }      if (!is.matrix(datamat)) {     stop_fmridataset(       fmridataset_error_config,       \"datamat must be a matrix or vector\",       parameter = \"datamat\",       value = class(datamat)     )   }      # Validate dimensions match   if (!is.numeric(run_length) || any(run_length <= 0)) {     stop_fmridataset(       fmridataset_error_config,       \"run_length must be positive numeric values\",       parameter = \"run_length\",       value = run_length     )   }      # Rest of function... }"},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_12-inconsistent-return-types","dir":"","previous_headings":"1. Type Safety Issues","what":"1.2 Inconsistent Return Types","title":"fmridataset API Safety and Usability Analysis","text":"Issue: get_mask() returns different types depending dataset type: Problems: - Returns 3D array, NeuroVol, numeric vector, logical vector - Users can’t predict return type without knowing dataset internals - Type inconsistency makes generic programming difficult Suggested Improvement: Add standardize parameter:","code":"# In data_access.R get_mask.fmri_file_dataset <- function(x, ...) {   if (!is.null(x$backend)) {     mask_vec <- backend_get_mask(x$backend)  # Returns logical vector     dims <- backend_get_dims(x$backend)$spatial     array(mask_vec, dims)  # Converts to 3D array   } else {     # Legacy path returns NeuroVol object     neuroim2::read_vol(x$mask_file)   } }  get_mask.matrix_dataset <- function(x, ...) {   x$mask  # Returns numeric vector (rep(1, ncol)) }  get_mask.latent_dataset <- function(x, ...) {   x$lvec@mask  # Returns logical vector from S4 slot } get_mask <- function(x, standardize = TRUE, ...) {   UseMethod(\"get_mask\") }  get_mask.fmri_file_dataset <- function(x, standardize = TRUE, ...) {   raw_mask <- # ... existing code      if (standardize) {     # Always return logical vector for consistency     return(as.logical(as.vector(raw_mask)))   }   return(raw_mask)  # Return native format if requested }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_21-cryptic-error-messages","dir":"","previous_headings":"2. Error Message Issues","what":"2.1 Cryptic Error Messages","title":"fmridataset API Safety and Usability Analysis","text":"Issue: Many errors don’t guide users solutions: Problems: - Doesn’t tell user actual values - Doesn’t suggest fix issue - context parameter wrong Suggested Improvement:","code":"# In dataset_constructors.R assert_that(sum(run_length) == nrow(datamat)) # Produces: \"sum(run_length) == nrow(datamat) is not TRUE\" if (sum(run_length) != nrow(datamat)) {   stop_fmridataset(     fmridataset_error_config,     sprintf(       \"Total run length (%d) must equal number of timepoints in data (%d). \",       \"Check that run_length values sum to your data's time dimension.\",       sum(run_length), nrow(datamat)     ),     parameter = \"run_length\",     value = run_length,     expected = nrow(datamat)   ) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_22-missing-input-validation-leading-to-downstream-errors","dir":"","previous_headings":"2. Error Message Issues","what":"2.2 Missing Input Validation Leading to Downstream Errors","title":"fmridataset API Safety and Usability Analysis","text":"Issue: fmri_dataset() doesn’t validate file existence early: Suggested Improvement: Add upfront validation:","code":"# Current code allows creation with non-existent files # Error occurs later during data access backend <- nifti_backend(   source = scan_files,  # No validation here   mask_source = maskfile,   preload = preload,   mode = mode ) # In fmri_dataset() if (!all(file.exists(scan_files))) {   missing <- scan_files[!file.exists(scan_files)]   stop_fmridataset(     fmridataset_error_backend_io,     sprintf(       \"Cannot find scan files: %s\\n\",       \"Please check file paths and ensure files exist.\",       paste(missing, collapse = \", \")     ),     file = missing,     operation = \"validate\"   ) }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_31-inconsistent-parameter-names","dir":"","previous_headings":"3. API Consistency Issues","what":"3.1 Inconsistent Parameter Names","title":"fmridataset API Safety and Usability Analysis","text":"Issue: Similar concepts use different parameter names: Problems: - Users must remember different parameter names concept - Can’t easily switch dataset types - Increases cognitive load Suggested Improvement: Add aliases backward compatibility:","code":"# Different names for mask across functions: fmri_dataset(scans, mask = NULL, ...)           # \"mask\"  fmri_mem_dataset(scans, mask, ...)              # \"mask\" (required) fmri_h5_dataset(h5_files, mask_source, ...)     # \"mask_source\" latent_backend(source, mask_source = NULL, ...)  # \"mask_source\" fmri_dataset <- function(scans, mask = NULL, mask_source = NULL, ...) {   # Support both parameter names   if (!is.null(mask_source) && is.null(mask)) {     mask <- mask_source   }   # Rest of function... }"},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_32-confusing-function-names","dir":"","previous_headings":"3. API Consistency Issues","what":"3.2 Confusing Function Names","title":"fmridataset API Safety and Usability Analysis","text":"Issue: Function names don’t clearly indicate purpose: Suggested Improvement: Add clearer aliases:","code":"# Unclear what these return: get_data()        # Returns native format (matrix, NeuroVec, etc.) get_data_matrix() # Always returns matrix samples()         # Returns 1:n_timepoints (just indices) blocklens()       # Returns run lengths # Add more descriptive aliases get_data_native <- get_data get_timepoint_indices <- samples get_run_lengths <- blocklens  # This already exists but isn't prominent"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_41-common-tasks-require-too-many-steps","dir":"","previous_headings":"4. Usability Issues","what":"4.1 Common Tasks Require Too Many Steps","title":"fmridataset API Safety and Usability Analysis","text":"Issue: Getting simple data matrix requires multiple operations: Suggested Improvement: Add convenience function:","code":"# Current approach for masked data extraction: dset <- fmri_dataset(scans, mask, TR, run_length) data <- get_data(dset)  # Returns NeuroVec mask <- get_mask(dset)  # Returns NeuroVol masked_data <- neuroim2::series(data, which(mask != 0)) # Add to API: get_masked_matrix <- function(x, ...) {   UseMethod(\"get_masked_matrix\") }  get_masked_matrix.fmri_dataset <- function(x, ...) {   # Direct path to masked matrix   if (!is.null(x$backend)) {     return(backend_get_data(x$backend, ...))   }   # Legacy path   get_data_matrix(x, ...) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_42-unclear-defaults","dir":"","previous_headings":"4. Usability Issues","what":"4.2 Unclear Defaults","title":"fmridataset API Safety and Usability Analysis","text":"Issue: matrix_dataset creates default mask 1s without documentation: Problems: - Users might realize voxels included default - Numeric 1s instead logical TRUE inconsistent Suggested Improvement:","code":"matrix_dataset <- function(datamat, TR, run_length, event_table = data.frame()) {   # ...   ret <- list(     # ...     mask = rep(1, ncol(datamat))  # Undocumented default   ) } matrix_dataset <- function(datamat, TR, run_length,                            event_table = data.frame(),                           mask = NULL) {   # ...   if (is.null(mask)) {     message(\"Using default mask (all voxels included)\")     mask <- rep(TRUE, ncol(datamat))   }   # Validate mask   if (length(mask) != ncol(datamat)) {     stop_fmridataset(       fmridataset_error_config,       sprintf(\"Mask length (%d) must match number of voxels (%d)\",               length(mask), ncol(datamat)),       parameter = \"mask\",       value = length(mask)     )   } }"},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_43-required-parameters-that-could-have-sensible-defaults","dir":"","previous_headings":"4. Usability Issues","what":"4.3 Required Parameters That Could Have Sensible Defaults","title":"fmridataset API Safety and Usability Analysis","text":"Issue: TR always required even needed: Suggested Improvement: Make TR optional warning:","code":"matrix_dataset <- function(datamat, TR, run_length, event_table = data.frame()) {   # TR is required but might not be used in some analyses } matrix_dataset <- function(datamat, TR = NULL, run_length,                            event_table = data.frame()) {   if (is.null(TR)) {     warning(\"No TR specified. Using default of 1.0 second. \",             \"Specify TR explicitly if timing matters for your analysis.\")     TR <- 1.0   }   # Rest of function... }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_51-missing-resource-cleanup","dir":"","previous_headings":"5. Safety Issues","what":"5.1 Missing Resource Cleanup","title":"fmridataset API Safety and Usability Analysis","text":"Issue: Backend resources aren’t automatically cleaned : Suggested Improvement: Add finalizers:","code":"# No automatic cleanup in dataset destructors # File handles or memory maps might leak backend <- backend_open(backend) # No corresponding close in error paths # In dataset constructors: backend <- backend_open(backend)  # Register cleanup reg.finalizer(ret, function(obj) {   if (!is.null(obj$backend) &&        inherits(obj$backend, \"storage_backend\")) {     try(backend_close(obj$backend), silent = TRUE)   } }, onexit = TRUE)"},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_52-side-effects-not-documented","dir":"","previous_headings":"5. Safety Issues","what":"5.2 Side Effects Not Documented","title":"fmridataset API Safety and Usability Analysis","text":"Issue: latent_backend returns latent scores instead voxel data, isn’t clear function names: Suggested Improvement: Add explicit documentation consider renaming:","code":"# This returns components, not voxels! data <- get_data_matrix(latent_dataset_obj) # Returns: time × components, not time × voxels #' @details #' IMPORTANT: For latent datasets, get_data_matrix() returns the latent #' scores (time × components), not reconstructed voxel data. Use #' get_latent_scores() for clarity.  get_latent_scores <- function(x, ...) {   if (!inherits(x, c(\"latent_dataset\", \"fmri_latent_dataset\"))) {     stop(\"get_latent_scores only works with latent datasets\")   }   get_data_matrix(x, ...) }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_61-add-input-validation-helper","dir":"","previous_headings":"6. Additional Recommendations","what":"6.1 Add Input Validation Helper","title":"fmridataset API Safety and Usability Analysis","text":"Create centralized validation function:","code":"validate_fmri_inputs <- function(TR = NULL, run_length = NULL,                                  mask = NULL, data_dims = NULL) {   errors <- list()      if (!is.null(TR)) {     if (!is.numeric(TR) || length(TR) != 1 || TR <= 0) {       errors$TR <- \"TR must be a single positive number\"     }   }      if (!is.null(run_length)) {     if (!is.numeric(run_length) || any(run_length <= 0)) {       errors$run_length <- \"run_length must contain positive numbers\"     }   }      if (length(errors) > 0) {     msg <- paste(names(errors), errors, sep = \": \", collapse = \"\\n\")     stop_fmridataset(       fmridataset_error_config,       paste(\"Invalid parameters:\\n\", msg)     )   } }"},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_62-add-type-checking-utilities","dir":"","previous_headings":"6. Additional Recommendations","what":"6.2 Add Type Checking Utilities","title":"fmridataset API Safety and Usability Analysis","text":"","code":"ensure_matrix <- function(x, name = \"data\") {   if (is.data.frame(x)) {     message(sprintf(\"Converting %s from data.frame to matrix\", name))     x <- as.matrix(x)   } else if (is.vector(x)) {     message(sprintf(\"Converting %s from vector to single-column matrix\", name))     x <- as.matrix(x)   } else if (!is.matrix(x)) {     stop_fmridataset(       fmridataset_error_config,       sprintf(\"%s must be a matrix, data.frame, or vector\", name),       parameter = name,       value = class(x)     )   }   x }"},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"id_63-improve-error-context","dir":"","previous_headings":"6. Additional Recommendations","what":"6.3 Improve Error Context","title":"fmridataset API Safety and Usability Analysis","text":"Add context parameter error functions:","code":"with_fmri_context <- function(expr, context) {   tryCatch(expr, error = function(e) {     if (inherits(e, \"fmridataset_error\")) {       e$context <- context     }     stop(e)   }) }  # Usage: with_fmri_context({   backend <- backend_open(backend) }, context = list(   operation = \"opening backend\",   dataset_type = class(x)[1] ))"},{"path":"https://bbuchsbaum.github.io/fmridataset/API_SAFETY_ANALYSIS.html","id":"summary","dir":"","previous_headings":"","what":"Summary","title":"fmridataset API Safety and Usability Analysis","text":"main safety usability issues fmridataset : Type coercion without warning - Functions silently convert types Inconsistent return types - generic returns different types Poor error messages - Don’t guide users solutions Inconsistent naming - Similar concepts different names Missing conveniences - Common tasks require multiple steps Resource management - automatic cleanup file handles Hidden behavior - functions don’t names suggest suggested improvements maintain backward compatibility improving safety usability : - Additional parameters defaults - Warning messages implicit behavior - Better error messages context - Convenience functions aliases - Automatic resource cleanup - Clearer documentation","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Backend Registry System Implementation","text":"successfully implemented comprehensive backend registry system fmridataset R package. system makes backends pluggable via registry pattern instead hardcoding types, allowing external packages register new backends without modifying fmridataset.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"id_1-core-registry-system-rbackend_registryr","dir":"","previous_headings":"What Was Implemented","what":"1. Core Registry System (R/backend_registry.R)","title":"Backend Registry System Implementation","text":"Registry Environment: Package-level environment store backend registrations register_backend(): Register new backend types unregister_backend(): Remove backend types is_backend_registered(): Check registration status get_backend_registry(): Get registry information list_backend_names(): List registered backends Backend Creation: create_backend() - Create backend instances name Validation: Enhanced validation system backend-specific validation functions","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"id_2-enhanced-backend-validation-rstorage_backendr","dir":"","previous_headings":"What Was Implemented","what":"2. Enhanced Backend Validation (R/storage_backend.R)","title":"Backend Registry System Implementation","text":"Added class inheritance checking Added S3 method availability checking Better error messages missing implementations Support custom validation functions per backend type","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"id_3-integration-with-existing-code","dir":"","previous_headings":"What Was Implemented","what":"3. Integration with Existing Code","title":"Backend Registry System Implementation","text":"Modified R/dataset_constructors.R use registry system Updated R/zzz.R register built-backends package load Maintained full backward compatibility","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"id_4-built-in-backend-registration","dir":"","previous_headings":"What Was Implemented","what":"4. Built-in Backend Registration","title":"Backend Registry System Implementation","text":"existing backends automatically registered: - nifti: NIfTI format backend using neuroim2 - h5: HDF5 format backend using fmristore - matrix: -memory matrix backend - latent: Latent space backend dimension-reduced data - study: Multi-subject study backend - zarr: Zarr format backend","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"id_5-documentation-and-examples","dir":"","previous_headings":"What Was Implemented","what":"5. Documentation and Examples","title":"Backend Registry System Implementation","text":"Vignette: vignettes/backend-registry.Rmd - Comprehensive guide creating custom backends Example Code: examples/backend_registry_example.R - Working examples custom backends Documentation: Full roxygen2 documentation new functions","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"id_6-comprehensive-testing","dir":"","previous_headings":"What Was Implemented","what":"6. Comprehensive Testing","title":"Backend Registry System Implementation","text":"Unit Tests: tests/testthat/test_backend_registry.R - 76 tests covering registry functionality Integration Tests: tests/testthat/test_backend_registry_integration.R - 31 tests verifying integration existing code tests pass successfully","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"extensibility","dir":"","previous_headings":"Key Features","what":"Extensibility","title":"Backend Registry System Implementation","text":"External packages can register new backends without modifying fmridataset Simple registration API: register_backend(name, factory, description) Support custom validation functions","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"backward-compatibility","dir":"","previous_headings":"Key Features","what":"Backward Compatibility","title":"Backend Registry System Implementation","text":"existing code continues work unchanged Direct backend constructor calls still work Existing dataset constructors enhanced use registry internally","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"validation","dir":"","previous_headings":"Key Features","what":"Validation","title":"Backend Registry System Implementation","text":"Comprehensive validation backend contracts Clear error messages missing implementations Custom validation functions backend-specific requirements","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"discoverability","dir":"","previous_headings":"Key Features","what":"Discoverability","title":"Backend Registry System Implementation","text":"list_backend_names() shows available backends get_backend_registry() provides detailed information Print method pretty-printed registry information","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"basic-backend-registration","dir":"","previous_headings":"Usage Examples","what":"Basic Backend Registration","title":"Backend Registry System Implementation","text":"","code":"# Register a custom backend my_backend_factory <- function(source, ...) {   # Implementation here   backend <- list(source = source, ...)   class(backend) <- c(\"my_backend\", \"storage_backend\")   backend }  register_backend(\"my_backend\", my_backend_factory, \"My custom backend\")  # Create instance backend <- create_backend(\"my_backend\", source = \"data.txt\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"using-in-dataset-creation","dir":"","previous_headings":"Usage Examples","what":"Using in Dataset Creation","title":"Backend Registry System Implementation","text":"","code":"# Create backend and use in dataset backend <- create_backend(\"nifti\",                           source = \"scan.nii\",                           mask_source = \"mask.nii\") dataset <- fmri_dataset(backend, TR = 2, run_length = 300)"},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"package-integration","dir":"","previous_headings":"Usage Examples","what":"Package Integration","title":"Backend Registry System Implementation","text":"","code":"# In external package's .onLoad() .onLoad <- function(libname, pkgname) {   if (requireNamespace(\"fmridataset\", quietly = TRUE)) {     fmridataset::register_backend(       name = \"myformat\",       factory = myformat_backend,       description = \"Backend for MyFormat files\"     )   } }"},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"benefits","dir":"","previous_headings":"","what":"Benefits","title":"Backend Registry System Implementation","text":"Pluggable Architecture: External packages can extend functionality without core changes Type Safety: Strong validation ensures backends implement required contracts Discoverability: Easy find use available backends Maintainability: Centralized registration system vs scattered hardcoded types Future-Proof: Easy add new backends data formats evolve","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"new-files","dir":"","previous_headings":"Files Added/Modified","what":"New Files","title":"Backend Registry System Implementation","text":"R/backend_registry.R - Core registry system tests/testthat/test_backend_registry.R - Registry unit tests tests/testthat/test_backend_registry_integration.R - Integration tests vignettes/backend-registry.Rmd - User guide examples/backend_registry_example.R - Working examples","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/BACKEND_REGISTRY_SUMMARY.html","id":"modified-files","dir":"","previous_headings":"Files Added/Modified","what":"Modified Files","title":"Backend Registry System Implementation","text":"R/zzz.R - Register built-backends load R/dataset_constructors.R - Use registry backend creation R/storage_backend.R - Enhanced validation NAMESPACE - Export new functions (auto-generated roxygen2) implementation successfully meets requirements: ✅ Registry system backend registration ✅ Backends discoverable name/type ✅ External package registration support ✅ Backward compatibility maintained ✅ Interface validation ✅ Documentation examples ✅ Comprehensive testing ✅ R best practices integration","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":null,"dir":"","previous_headings":"","what":"CLAUDE.md","title":"CLAUDE.md","text":"file provides guidance Claude Code (claude.ai/code) working code repository.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":"build-and-check","dir":"","previous_headings":"Common Development Commands","what":"Build and Check","title":"CLAUDE.md","text":"","code":"# Build package R CMD build . devtools::build()  # Check package (CRAN compliance) R CMD check devtools::check()  # Install package locally R CMD INSTALL . devtools::install()"},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":"testing","dir":"","previous_headings":"Common Development Commands","what":"Testing","title":"CLAUDE.md","text":"","code":"# Run testthat tests devtools::test() testthat::test_dir(\"tests/testthat\")  # Run a single test file testthat::test_file(\"tests/testthat/test-sampling-frame.R\")  # Run tests matching a pattern devtools::test(filter = \"backend\")  # Run with coverage covr::package_coverage()"},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":"documentation","dir":"","previous_headings":"Common Development Commands","what":"Documentation","title":"CLAUDE.md","text":"","code":"# Generate package documentation devtools::document() roxygen2::roxygenize()  # Build pkgdown website pkgdown::build_site()"},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":"development-workflow","dir":"","previous_headings":"Common Development Commands","what":"Development Workflow","title":"CLAUDE.md","text":"","code":"# Load all functions for interactive development devtools::load_all()  # Check for common issues devtools::check()  # Run specific checks rcmdcheck::rcmdcheck(args = \"--no-manual\")"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":"core-classes","dir":"","previous_headings":"High-Level Architecture","what":"Core Classes","title":"CLAUDE.md","text":"Supports multiple data sources: file paths, pre-loaded objects, matrices, BIDS datasets Lazy loading chunked iteration capabilities Main constructor: fmri_dataset() backend-specific helpers Built DelayedArray/DelayedMatrix memory efficiency Contains data spatial/temporal metadata Access via as_delayed_array(), as_tibble() methods Encapsulates TR, run lengths, temporal properties Bridges fmrireg fmridataset conventions Constructor: sampling_frame() matrix_backend - -memory matrix storage nifti_backend - NIfTI file access (optimized caching) h5_backend - HDF5 storage zarr_backend - Zarr array format (cloud-native, chunked storage) study_backend - Multi-subject study data","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":"key-design-patterns","dir":"","previous_headings":"High-Level Architecture","what":"Key Design Patterns","title":"CLAUDE.md","text":"Generic functions R/all_generic.R (loaded first alphabetically) Method dispatch pattern throughout Constructor pattern: new_*() (internal) → *() (user-facing) Validation pattern: constructors validate inputs Contract defined storage_backend.R Required methods: backend_open(), backend_close(), backend_get_dims(), backend_get_data() Backend validation via validate_backend() Lazy loading pattern efficient memory use data_chunks() memory-efficient processing Supports voxel-wise run-wise chunking strategies Iterator protocol sequential data access","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":"file-organization","dir":"","previous_headings":"High-Level Architecture","what":"File Organization","title":"CLAUDE.md","text":"Core Generic Functions: - R/all_generic.R - S3 generic function definitions (loaded first) Data Structures: - R/FmriSeries.R - S4 class lazy time series - R/fmri_dataset.R - Core dataset class - R/dataset_constructors.R - Dataset creation functions - R/sampling_frame_adapters.R - Temporal structure handling Storage Backends: - R/storage_backend.R - Backend interface definition - R/matrix_backend.R - -memory matrix storage - R/nifti_backend.R - NIfTI file backend (caching) - R/h5_backend.R - HDF5 storage backend - R/zarr_backend.R - Zarr array backend (cloud-native) - R/study_backend.R - Multi-subject study backend - R/study_backend_seed.R - Lazy evaluation study backend Latent Space Interface: - R/latent_dataset.R - Specialized interface latent space data Data Access & Processing: - R/data_access.R - Data retrieval methods - R/data_chunks.R - Chunking functionality - R/conversions.R - Type conversion methods - R/as_delayed_array.R - DelayedArray conversions Utilities: - R/config.R - Configuration management - R/errors.R - Custom error classes - R/print_methods.R - Display methods objects","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":"testing-strategy","dir":"","previous_headings":"High-Level Architecture","what":"Testing Strategy","title":"CLAUDE.md","text":"package uses comprehensive testthat testing 40+ test files: Test Organization: - test-*_backend.R - Backend-specific tests - test-dataset_constructors.R - Constructor validation - test-data_chunks*.R - Chunking functionality - test-fmri_series_*.R - FmriSeries class tests - test-integration.R - Cross-component integration - test-error_constructors.R - Error handling - test-backward_compatibility.R - Legacy API support Coverage Areas: - backend implementations - Data access patterns chunking strategies - Type conversions metadata handling - Edge cases error conditions - Performance optimizations (e.g., NIfTI caching)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CLAUDE.html","id":"integration-points","dir":"","previous_headings":"High-Level Architecture","what":"Integration Points","title":"CLAUDE.md","text":"Core Dependencies: - neuroim2: NeuroVec objects neuroimaging data structures - DelayedArray: Lazy array operations FmriSeries - Matrix: Sparse matrix support - iterators: Chunked data iteration Optional Dependencies: - bidser: BIDS dataset integration - fmristore: Advanced storage backends - arrow: Parquet file support - dplyr: Data manipulation architecture emphasizes loose coupling S3 generic functions, allowing backends components evolve independently maintaining stable APIs.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to fmridataset","title":"Contributing to fmridataset","text":"welcome contributions fmridataset package! document provides guidelines contributing.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"","what":"Code Style","title":"Contributing to fmridataset","text":"package follows tidyverse style guide modifications: Line length: Maximum 120 characters Naming: Use snake_case functions variables Documentation: exported functions must roxygen2 documentation S3 methods: Follow pattern method_name.class_name","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CONTRIBUTING.html","id":"development-setup","dir":"","previous_headings":"","what":"Development Setup","title":"Contributing to fmridataset","text":"Fork clone repository Install development dependencies: Load package development mode:","code":"install.packages(c(\"devtools\", \"testthat\", \"lintr\", \"roxygen2\")) devtools::load_all()"},{"path":"https://bbuchsbaum.github.io/fmridataset/CONTRIBUTING.html","id":"making-changes","dir":"","previous_headings":"","what":"Making Changes","title":"Contributing to fmridataset","text":"Create new branch feature bug fix Write tests new functionality Ensure tests pass: devtools::test() Check package: devtools::check() Run linter: lintr::lint_package() Update documentation needed: devtools::document()","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CONTRIBUTING.html","id":"backend-development","dir":"","previous_headings":"","what":"Backend Development","title":"Contributing to fmridataset","text":"’re adding new storage backend: Implement methods StorageBackend contract (see R/storage_backend.R) Validate backend validate_backend() Add comprehensive tests Document backend extending vignette","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CONTRIBUTING.html","id":"testing","dir":"","previous_headings":"","what":"Testing","title":"Contributing to fmridataset","text":"Write tests using testthat Place test files tests/testthat/ Name test files test-<functionality>.R Use descriptive test names","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CONTRIBUTING.html","id":"pull-request-process","dir":"","previous_headings":"","what":"Pull Request Process","title":"Contributing to fmridataset","text":"Update NEWS.md changes Ensure CI checks pass Request review maintainers Address feedback","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CONTRIBUTING.html","id":"questions","dir":"","previous_headings":"","what":"Questions?","title":"Contributing to fmridataset","text":"Feel free open issue questions contributing.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":null,"dir":"","previous_headings":"","what":"CRAN Documentation Compliance Strategy (roxygen2)","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"comprehensive guide creating CRAN-compliant documentation using roxygen2. Following practices significantly smooth CRAN submission process.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"core-principle","dir":"","previous_headings":"","what":"Core Principle","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"exported functions, datasets, classes must documented. exceptions.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"required-tags-for-exported-functions","dir":"","previous_headings":"I. Essential Function Documentation","what":"Required Tags for Exported Functions","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Title & Description: first sentence title (sentence case, period end ’s phrase). Subsequent paragraphs form description. Parameters (@param): Document every argument, specifying type purpose. Return Value (@return): Clearly describe object returned. complex objects, use \\item{} lists describe data frame columns. Examples (@examples): Provide runnable examples. See “VII. Examples Best Practices” details. Export (@export): tag makes function available users signals roxygen2 generate documentation.","code":"#' Compute Summary Statistics #' #' Calculates descriptive statistics for numeric data, handling missing #' values and providing multiple summary measures in a consistent format. #' @param x A numeric vector or matrix. #' @param na.rm Logical; if TRUE, NA values are removed before computation. #' @param method Character string specifying computation method: \"fast\" or \"robust\". #' @return A named list with elements: #'   \\item{mean}{Arithmetic mean.} #'   \\item{median}{Median value.} #'   \\item{sd}{Standard deviation.} #'   Returns NULL if input is empty after NA removal. #' @examples #' # Basic usage #' x <- c(1, 2, 3, NA, 5) #' compute_stats(x, na.rm = TRUE) #' #' # With matrix input #' m <- matrix(1:12, nrow = 3) #' compute_stats(m) #' #' \\donttest{ #'   # Slower example (>5 seconds or requires special conditions) #'   if (requireNamespace(\"largedata\", quietly = TRUE)) { #'     # Simulating data generation for example #'     # big_data <- largedata::generate_data(1e6) #'     # compute_stats(big_data, method = \"robust\") #'   } #' } #' @export"},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"optional-but-recommended-tags","dir":"","previous_headings":"I. Essential Function Documentation","what":"Optional But Recommended Tags","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Details (@details): elaborate explanations, algorithms, technical specifics. See Also (@seealso): Link related functions using \\code{\\link{function_name}} \\code{\\link[package]{function_name}}. References (@references): Cite relevant publications sources. Use \\doi{} DOIs.","code":"#' @details #' Uses Welford's online algorithm for numerically stable computation. #' For matrices, statistics are computed column-wise. #' @seealso #' \\code{\\link{base_stats}} for basic statistics, #' \\code{\\link[stats]{summary}} for R's built-in summary. #' @references #' Welford, B. P. (1962). Note on a method for calculating corrected #' sums of squares and products. Technometrics, 4(3), 419-420. #' \\doi{10.1080/00401706.1962.10490022}"},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"inheriting-documentation-use-sparingly","dir":"","previous_headings":"I. Essential Function Documentation","what":"Inheriting Documentation (Use Sparingly)","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"avoid duplication across related functions methods: - @inheritParams function_name: Inherits @param tags another function. - @inheritDotParams function_name arg1 arg2 ...: Inherits documentation specific arguments passed via .... - @inheritSection function_name SectionTitle: Inherits whole section (e.g., “Details”) another function’s documentation. - @inherit allgemein_generic_function: S4 methods, can inherit documentation generic. Use inherited documentation exactly appropriate.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"ii-package-level-documentation","dir":"","previous_headings":"","what":"II. Package-Level Documentation","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Create dedicated file, typically R/yourpackage-package.R (R/package_name.R), package-level documentation. file generally contain function definitions. Key Points: - \"_PACKAGE\" @keywords internal: recommended way anchor package-level documentation using roxygen2. \"_PACKAGE\" documented. - @docType package @name yourpackage-package (@name yourpackage) essential. - @aliases yourpackage: Allows ?yourpackage work. - ## usethis namespace: comments usethis manage @importFrom tags choose colocate .","code":"#' @keywords internal \"_PACKAGE\"  ## usethis namespace: start ## (Place @importFrom directives here if desired, or manage them elsewhere) ## usethis namespace: end NULL  #' yourpackage: A Brief Package Title #' #' A paragraph (or more) describing what the package does, its main purpose, #' and its key features. This is often the first thing users see. #' Point users to key functions to get started, e.g., #' \"For an overview of the main functionalities, see \\code{\\link{main_function}} #' and the package vignette: \\code{vignette(\"your-vignette-name\", package = \"yourpackage\")}\". #' #' @section Main Functions: #' An optional section to highlight key functions. #' \\itemize{ #'   \\item \\code{\\link{function1}}: Performs task X, crucial for getting started. #'   \\item \\code{\\link{function2}}: Implements advanced feature Y. #' } #' #' @docType package #' @name yourpackage-package # Or just yourpackage #' @aliases yourpackage #' @author Your Name <your.email@example.com> [cre, aut] (See DESCRIPTION for full author list) NULL"},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"iii-s3-methods-documentation","dir":"","previous_headings":"","what":"III. S3 Methods Documentation","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Generic Function: Document generic fully, including @param, @return, @examples. Methods (grouped documentation using @rdname): Use @rdname link method documentation generic’s documentation file. Important: - Use @method generic_name class_name properly register S3 methods NAMESPACE file. roxygen2 handles . - method identical parameters return value structure generic, might need repeat @param @return. Use @describeIn generic_name Short description method. brevity.","code":"#' Summarize Data Objects #' #' Generic function for creating summaries of various data types. #' #' @param x Object to summarize. #' @param ... Additional arguments passed to methods. #' @return A summary object (class depends on input type). #' @export #' @examples #' # Example for a numeric vector #' summarize_data(c(1, 2, 3, NA, 5), na.rm = TRUE) #' #' # Example for a data frame (will dispatch to data.frame method) #' df <- data.frame(a = 1:3, b = letters[1:3]) #' summarize_data(df) summarize_data <- function(x, ...) {   UseMethod(\"summarize_data\") } #' @rdname summarize_data #' @method summarize_data numeric #' @param na.rm Logical; if TRUE, NAs are removed (specific to numeric method). #' @export summarize_data.numeric <- function(x, na.rm = TRUE, ...) {   # Implementation   list(mean = mean(x, na.rm = na.rm), n = length(na.omit(x))) }  #' @rdname summarize_data #' @method summarize_data data.frame #' @param columns Character vector of column names to summarize (default: all numeric). #' @export summarize_data.data.frame <- function(x, columns = NULL, ...) {   # Implementation   # Select numeric columns if 'columns' is NULL   if(is.null(columns)) columns <- names(x)[sapply(x, is.numeric)]   lapply(x[, columns, drop = FALSE], summarize_data.numeric, ...) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"iv-dataset-documentation","dir":"","previous_headings":"","what":"IV. Dataset Documentation","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Place data (e.g., survey_data.rda) data/ directory. Create documentation file (e.g., R/data.R):","code":"#' Example Survey Data #' #' Survey responses from 500 participants collected in 2023 for #' demonstrating statistical analysis techniques. Contains demographic #' information and Likert scale responses. #' #' @format A data frame with 500 rows and 8 columns: #' \\describe{ #'   \\item{id}{Participant ID (integer).} #'   \\item{age}{Age in years (numeric, range 18-65).} #'   \\item{gender}{Gender identity (factor: \"Male\", \"Female\", \"Other\").} #'   \\item{response}{Survey response score (numeric, 1-10 scale).} #'   \\item{group}{Treatment group (factor: \"Control\", \"Treatment\").} #'   \\item{date}{Response date (Date object).} #' } #' @source Simulated data based on typical survey patterns. Generated using `scripts/generate_survey_data.R`. #' @keywords datasets #' @examples #' data(survey_data) #' str(survey_data) #' summary(survey_data$response) #' if (requireNamespace(\"ggplot2\", quietly = TRUE)) { #'   ggplot2::ggplot(survey_data, ggplot2::aes(x = age, y = response, color = gender)) + #'     ggplot2::geom_point() #' } \"survey_data\" # The name must match the .rda file (without extension)"},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"v-internal-functions","dir":"","previous_headings":"","what":"V. Internal Functions","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"True Helper Functions (@noRd): functions exported .Rd file generated. invisible users. Internal Functions Might Useful Power Users (@keywords internal): functions exported default (unless @export also added, rare internals) .Rd file generated. documentation typically hidden main help index can accessed via ?yourpackage:::advanced_internal help(\"advanced_internal\", help_type = \"html\") (known).","code":"#' Internal helper for data validation #' #' Checks input data format and throws informative errors. Not intended #' for direct user calls. #' @param data Input data to validate. #' @param required_cols Required column names. #' @return TRUE if valid, throws error otherwise. #' @noRd validate_input <- function(data, required_cols) {   # Implementation } #' Advanced internal processing function #' #' This function performs a specialized internal task. It is exposed for #' debugging or advanced usage by developers familiar with the package internals, #' but it is not part of the stable, public API and may change without notice. #' @param x Input parameter for advanced processing. #' @return Processed result. #' @keywords internal # @export # Typically NOT exported advanced_internal_processor <- function(x) {   # Implementation }"},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"vi-namespace-management-with-roxygen2","dir":"","previous_headings":"","what":"VI. NAMESPACE Management with roxygen2","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Manage imports directly .R files, often top package-level documentation file. Specific Imports: Best Practices: - ✅ Prefer specific imports: @importFrom pkg function1 function2. clearest safest. - ⚠️ Use whole package imports cautiously: @import pkg avoided unless use many functions pkg frequently across multiple files. increases risk namespace conflicts. - ✅ Explicit namespacing code: Using dplyr::filter() stats::lm() code often robust approach, even also @importFrom. makes dependencies obvious.","code":"#' @importFrom stats median sd lm #' @importFrom utils head tail"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"general-guidelines","dir":"","previous_headings":"VII. Examples Best Practices (@examples)","what":"General Guidelines","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"@examples must run without errors warnings R CMD check ---cran. Even code wrapped \\donttest{} still run CRAN checks (e.g., generating pkg-Ex.Rout). \\donttest{} primarily means specific example block fails, won’t cause check fail reason alone, CRAN might still flag issues found. \\dontrun{} avoided unless absolutely necessary (e.g., functions open GUIs, modify global options irreversibly, require authentication, write files outside tempdir()). Justify use cran-comments.md. \\dontshow{} code run whose output illustrative user (e.g., setup like old_opts <- options(...), cleanup like options(old_opts), setting temporary directories).","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"timing-guidelines","dir":"","previous_headings":"VII. Examples Best Practices (@examples)","what":"Timing Guidelines","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Target: Strive individual example (certainly example file) run quickly. CRAN sensitive total example runtime (e.g., aiming <5-10 seconds per file, ideally much less). Take longer seconds (e.g., > 5s). Require internet connection. Depend suggested packages (always guard (requireNamespace(...))). Need special system conditions universally available.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"file-system-safety-in-examples","dir":"","previous_headings":"VII. Examples Best Practices (@examples)","what":"File System Safety in Examples","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"examples need write files, always use tempdir() clean .","code":"#' @examples #' \\dontshow{ #' # CRAN-safe temporary directory for example output #' old_wd <- setwd(tempdir()) #' old_dir_to_create <- \"my_temp_example_dir\" #' if (!dir.exists(old_dir_to_create)) dir.create(old_dir_to_create) #' } #' #' # Example that writes a file #' my_data_frame <- data.frame(a = 1:3, b = letters[1:3]) #' # write.csv(my_data_frame, file.path(old_dir_to_create, \"temp_file.csv\")) #' # message(\"File written to: \", file.path(tempdir(), old_dir_to_create, \"temp_file.csv\")) #' #' \\dontshow{ #' # Cleanup: remove created files/directories and restore working directory #' # unlink(file.path(old_dir_to_create, \"temp_file.csv\")) #' # unlink(old_dir_to_create, recursive = TRUE) #' setwd(old_wd) #' }"},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"conditional-examples","dir":"","previous_headings":"VII. Examples Best Practices (@examples)","what":"Conditional Examples","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"","code":"#' @examples #' # Basic example (always runs) #' basic_function_call(1:10) #' #' \\donttest{ #'   # Example depending on a suggested package #'   if (requireNamespace(\"ggplot2\", quietly = TRUE)) { #'     # data_for_plot <- ... #'     # ggplot2::ggplot(data_for_plot) + ggplot2::geom_point() #'   } #' #'   # Example requiring internet (and ideally interactive session) #'   if (interactive() && curl::has_internet()) { #'     # downloaded_content <- download_function_from_web(\"https://example.com/data.txt\") #'   } #' }"},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"viii-description-file-essentials","dir":"","previous_headings":"","what":"VIII. DESCRIPTION File Essentials","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"DESCRIPTION file critical contents used CRAN help pages. Key Fields: - Title: Sentence case, concise. - Description: detailed, 1-2 paragraphs. - Authors@R: Canonical way list authors roles. - License: Must OSI-approved open-source license accepted CRAN. Include actual LICENSE (LICENSE.md MIT + file LICENSE). - Encoding: UTF-8: Essential using non-ASCII characters code docs. - LazyData: false: Generally preferred CRAN. true, consider Compression: xz methods keep package size small. - URL / BugReports: Links development site/issue tracker.","code":"Package: yourpackage Type: Package Title: Your Package Title in Sentence Case (No Period at End) Version: 0.1.0 Authors@R:     person(given = \"First\",            family = \"Last\",            role = c(\"aut\", \"cre\"),  # Author and Creator/Maintainer            email = \"first.last@example.com\",            comment = c(ORCID = \"0000-0000-0000-0000\")) Description: A comprehensive, multi-sentence description of what the     package does. Explain its main purpose, key functionalities, and     primary use cases. This can span multiple indented lines. License: MIT + file LICENSE  # Or GPL-3, Apache 2.0, etc. Must be CRAN-compatible. Encoding: UTF-8 Language: en-US LazyData: false # Recommended; if true, consider data compression. RoxygenNote: 7.3.1 # Updated by roxygen2 URL: https://github.com/yourname/yourpackage, https://yourpackage.r-universe.dev BugReports: https://github.com/yourname/yourpackage/issues Depends:     R (>= 3.5.0) # Specify minimum R version Imports:     stats,         # For median, sd, etc.     utils          # For head, tail Suggests:     knitr,     rmarkdown,     # For vignettes     testthat (>= 3.0.0), # For tests     ggplot2        # For examples or vignettes # VignetteBuilder: knitr # Added if you have vignettes (see Section XII) # Compression: xz # Consider if LazyData: true or large data files"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"essential-local-checks","dir":"","previous_headings":"IX. Quality Control Workflow","what":"Essential Local Checks","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"","code":"# 1. Generate documentation and NAMESPACE devtools::document() # or roxygen2::roxygenise()  # 2. Run comprehensive CRAN checks (THE GOLD STANDARD) # This will also build and check vignettes if present. devtools::check(cran = TRUE)  # 3. Check for spelling errors in documentation # Add a WORDLIST file for technical terms. devtools::spell_check()  # 4. Validate all URLs in documentation, DESCRIPTION, etc. urlchecker::url_check()"},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"multi-platform-testing-crucial-before-submission","dir":"","previous_headings":"IX. Quality Control Workflow","what":"Multi-Platform Testing (Crucial Before Submission)","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Use services check platforms don’t access :","code":"# Windows (devel and release) devtools::check_win_devel() devtools::check_win_release() # If available  # macOS # devtools::check_mac_release() # (Often relies on external services or specific local setup)  # R-hub builder (various Linux distributions, Windows, macOS) # devtools::check_rhub() or rhub::check_for_cran() rhub::check_for_cran() # Simpler interface for CRAN-like checks  # Check results from these services thoroughly."},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"target-0-errors-0-warnings-0-notes","dir":"","previous_headings":"IX. Quality Control Workflow","what":"Target: 0 errors, 0 warnings, 0 notes","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Address every issue. NOTE unavoidable justified (rare), explain comprehensively cran-comments.md file submitted package.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"xi-pro-tips-for-smooth-submissions","dir":"","previous_headings":"","what":"XI. Pro Tips for Smooth Submissions","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"usethis::use_package_doc(): Sets package-level documentation file. usethis::use_data(): Prepares datasets inclusion data/ optionally creates R/data.R. usethis::use_vignette(\"-vignette-title\"): Creates vignette template. usethis::use_mit_license(), usethis::use_gpl3_license(), etc.: Sets license files. usethis::use_testthat(): Sets testing infrastructure. Use active voice clear, concise language. specific @param descriptions (e.g., “numeric vector.” just “Input.”). Ensure @return clearly describes output’s class structure. Include examples common use cases important edge cases. Run devtools::check(cran = TRUE) frequently development, just submission. Address issues arise; don’t let accumulate. Treat documentation care R code. Keep documentation comments (#') physically close code describe. Update documentation immediately change function’s behavior, arguments, return value. Use version control (Git) documentation changes just code. “Writing R Extensions” manual ultimate authority. Refer definitive answers.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"xii-vignettes-highly-recommended","dir":"","previous_headings":"","what":"XII. Vignettes (Highly Recommended)","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Vignettes longer-form documents illustrate use package, often narrative extended examples. CRAN reviewers value well-written vignettes. Purpose: Show practical applications, workflows, delve specific functionalities deeply function examples allow. Creation: Use usethis::use_vignette(\"-vignette-title\"). creates .Rmd file inst/doc/. Format: R Markdown (.Rmd) standard. Include introduction explaining vignette’s purpose. Use mix narrative text runnable R code chunks. Ensure code chunks efficient don’t produce excessive output. Make self-contained possible. DESCRIPTION file entries: include vignettes, must declare builder (usually knitr) list necessary packages Suggests: dcf     VignetteBuilder: knitr     Suggests:         knitr,         rmarkdown # packages used vignettes Building Checking: devtools::check() attempt build vignettes. Ensure build without errors reasonably quickly.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/CRAN_guidance.html","id":"resources","dir":"","previous_headings":"","what":"Resources","title":"CRAN Documentation Compliance Strategy (roxygen2)","text":"Writing R Extensions (official manual – indispensable) CRAN Repository Policy (Essential reading) R Packages (2e) Hadley Wickham Jennifer Bryan (Excellent practical guide) roxygen2 documentation (roxygen2 specifics) CRAN Incoming Feasibility (CRAN’s checklist) Remember: devtools::check(cran = TRUE) best friend. Aim zero errors, warnings, notes submitting CRAN.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/SECURITY.html","id":"supported-versions","dir":"","previous_headings":"","what":"Supported Versions","title":"Security Policy","text":"project currently experimental development. Security updates provided recent version.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/SECURITY.html","id":"reporting-a-vulnerability","dir":"","previous_headings":"","what":"Reporting a Vulnerability","title":"Security Policy","text":"discover security vulnerability project, please report emailing maintainer directly rather opening public issue. Please report security vulnerabilities public GitHub issues. Include following information report: - description vulnerability - Steps reproduce issue - Potential impact - Suggested fix (available) can expect: - Acknowledgment report within 48 hours - Regular updates progress addressing vulnerability - Credit responsible disclosure (desired) Thank helping keep fmridataset package secure!","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"motivation-architectural-design-for-neuroimaging-data","dir":"Articles","previous_headings":"","what":"Motivation: Architectural Design for Neuroimaging Data","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"Neuroimaging analyses frequently involve data multiple sources different storage formats: NIfTI files, HDF5 archives, preprocessed matrices, BIDS-organized datasets. format typically requires format-specific loading code, memory management strategies, temporal organization schemes. fmridataset architecture addresses challenges abstraction layer separates analysis operations storage implementations. design pattern provides: Format independence: Analysis code works uniformly across supported formats Memory efficiency: Backend-specific optimizations without changing analysis code Extensibility: New formats can added without modifying existing code Performance optimization: backend implements format-specific optimizations document describes architectural principles, design patterns, extension mechanisms enable capabilities.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"a-real-example-architecture-in-action","dir":"Articles","previous_headings":"","what":"A Real Example: Architecture in Action","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"Let’s see architectural principles work practice creating datasets different sources demonstrating unified behavior: Note: unified interface works identically across different storage mechanisms (-memory matrices, file-based storage, multi-subject containers). abstraction enables code reuse across diverse data sources.","code":"library(fmridataset)  # Example 1: Matrix backend for in-memory data set.seed(123) matrix_data <- matrix(rnorm(1000 * 100), nrow = 100, ncol = 1000) matrix_ds <- matrix_dataset(   datamat = matrix_data,   TR = 2.0,   run_length = c(50, 50) )  # Example 2: File backend for NIfTI data (simulated paths) file_paths <- c(\"run1.nii.gz\", \"run2.nii.gz\") # Would be real paths # file_ds <- fmri_file_dataset( #   scans = file_paths, #   mask = \"mask.nii.gz\", #   TR = 2.0, #   run_length = c(50, 50) # )  # Example 3: Study backend for multi-subject data subject_datasets <- list(matrix_ds) # In practice, multiple datasets study_ds <- fmri_study_dataset(   datasets = subject_datasets,   subject_ids = \"sub-001\" )  # Demonstrate unified interface cat(\"Matrix dataset class:\", class(matrix_ds)[1], \"\\n\") cat(\"Study dataset class:\", class(study_ds)[1], \"\\n\")  # Same methods work on all dataset types cat(\"Matrix dataset TR:\", get_TR(matrix_ds), \"seconds\\n\") cat(\"Study dataset TR:\", get_TR(study_ds), \"seconds\\n\")  # Same chunking interface matrix_chunks <- data_chunks(matrix_ds, nchunks = 3) study_chunks <- data_chunks(study_ds, nchunks = 3)  cat(\"Matrix chunks created:\", length(matrix_chunks), \"\\n\") cat(\"Study chunks created:\", length(study_chunks), \"\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"understanding-the-architectural-layers","dir":"Articles","previous_headings":"","what":"Understanding the Architectural Layers","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"fmridataset architecture consists three primary layers work together provide flexibility maintaining simplicity. layer distinct responsibilities clean interfaces enable independent evolution extension.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"the-dataset-layer-user-facing-interface","dir":"Articles","previous_headings":"Understanding the Architectural Layers","what":"The Dataset Layer: User-Facing Interface","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"dataset layer provides primary interface users interact . layer defines operations possible ensures consistent behavior regardless underlying data source. call get_data_matrix() n_timepoints(), ’re working dataset layer. layer implements “facade” pattern, presenting simplified interface hides complexity different storage formats temporal organizations. dataset layer also handles cross-cutting concerns like event table management, chunking coordination, temporal structure validation. standardizing operations dataset level, ensure data sources behave consistently end users. dataset layer also format-specific optimizations can implemented transparently. example, matrix datasets can provide immediate data access, file datasets can implement sophisticated caching strategies. Users don’t need know implementation details; just experience different performance characteristics.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"the-backend-layer-storage-abstraction","dir":"Articles","previous_headings":"Understanding the Architectural Layers","what":"The Backend Layer: Storage Abstraction","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"dataset layer lies backend layer, handles storage-specific operations. layer implements “strategy” pattern, different backends provide alternative implementations set operations. backend knows efficiently read specific data format presenting uniform interface dataset layer. Backends responsible resource management, including opening closing files, managing memory allocations, handling format-specific error conditions. also provide metadata extraction capabilities, enabling datasets query dimensions, spatial information, acquisition parameters without loading actual data. backend contract carefully designed support eager lazy loading strategies. Backends can choose load data immediately fast access defer loading explicitly requested memory efficiency. flexibility enables optimal performance across different usage patterns dataset sizes.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"the-temporal-layer-time-structure-modeling","dir":"Articles","previous_headings":"Understanding the Architectural Layers","what":"The Temporal Layer: Time Structure Modeling","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"temporal layer captures rich time structure fMRI data sampling frame abstraction. Unlike simple time series, fMRI data complex temporal organization multiple runs, variable run lengths, specific timing relationships must preserved proper analysis. Sampling frames provide unified model temporal structure works across dataset types. handle conversions different time representations (seconds, TRs, sample indices), maintain run boundary information, enable sophisticated temporal queries. abstraction allows temporal analysis code work whether data comes single long session multiple shorter runs. temporal layer also integrates experimental design event table management. understanding acquisition timing experimental events, system can provide sophisticated event-related analysis capabilities maintaining temporal integrity across different data sources.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"deep-dive-core-design-patterns","dir":"Articles","previous_headings":"","what":"Deep Dive: Core Design Patterns","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"architecture leverages several key design patterns provide flexibility maintainability. Understanding patterns helps use package effectively provides foundation extensions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"delegation-pattern-distributed-responsibility","dir":"Articles","previous_headings":"Deep Dive: Core Design Patterns","what":"Delegation Pattern: Distributed Responsibility","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"delegation pattern central fmridataset manages complexity. Rather implementing functionality monolithic classes, architecture delegates specific responsibilities specialized components: delegation enables clean separation concerns makes system modular. component can evolve independently long maintains interface contract.","code":"# Datasets delegate temporal queries to sampling frames sf <- matrix_ds$sampling_frame direct_tr <- get_TR(sf) delegated_tr <- get_TR(matrix_ds) # Delegates to sampling frame  cat(\"Direct TR query:\", direct_tr, \"\\n\") cat(\"Delegated TR query:\", delegated_tr, \"\\n\")  # Datasets delegate storage operations to backends backend <- matrix_ds$backend backend_dims <- backend_get_dims(backend) dataset_dims <- dim(get_data_matrix(matrix_ds))  cat(\"Backend reports dimensions:\", backend_dims$time, \"x\", sum(backend_dims$spatial), \"\\n\") cat(\"Dataset provides dimensions:\", dataset_dims, \"\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"factory-pattern-automatic-backend-selection","dir":"Articles","previous_headings":"Deep Dive: Core Design Patterns","what":"Factory Pattern: Automatic Backend Selection","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"factory pattern enables automatic selection appropriate backends based input types. Users don’t need explicitly choose backends; system selects optimal one based data characteristics: automatic selection reduces cognitive load users ensuring optimal performance data type.","code":"# Matrix input automatically creates matrix backend matrix_input <- matrix(rnorm(500), nrow = 50, ncol = 10) auto_dataset1 <- fmri_dataset(scans = matrix_input, TR = 2.0, run_length = 50) cat(\"Matrix input created:\", class(auto_dataset1)[1], \"\\n\")  # File paths would automatically create file backend # file_input <- c(\"scan1.nii\", \"scan2.nii\") # auto_dataset2 <- fmri_dataset(scans = file_input, TR = 2.0, run_length = c(100, 100)) # cat(\"File input created:\", class(auto_dataset2)[1], \"\\n\")  # The factory hides complexity while providing optimal performance"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"observer-pattern-event-integration","dir":"Articles","previous_headings":"Deep Dive: Core Design Patterns","what":"Observer Pattern: Event Integration","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"observer pattern enables loose coupling temporal structure experimental events. Event tables can updated independently maintaining consistency underlying temporal organization: pattern ensures experimental design information remains consistent acquisition timing across different operations.","code":"# Create dataset with temporal structure ds <- matrix_dataset(   datamat = matrix(rnorm(800), nrow = 80, ncol = 10),   TR = 2.0,   run_length = c(40, 40) )  # Add events that automatically align with temporal structure events <- data.frame(   onset = c(10, 30, 90, 110), # seconds   duration = c(2, 2, 2, 2),   trial_type = c(\"A\", \"B\", \"A\", \"B\"),   run = c(1, 1, 2, 2) )  ds$event_table <- events  # Events automatically validate against temporal structure sf <- ds$sampling_frame total_duration <- get_total_duration(sf) cat(\"Total scan duration:\", total_duration, \"seconds\\n\") cat(\"Last event ends at:\", max(events$onset + events$duration), \"seconds\\n\")  # System can detect temporal inconsistencies if (max(events$onset + events$duration) > total_duration) {   warning(\"Events extend beyond scan duration\") }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"strategy-pattern-flexible-chunking","dir":"Articles","previous_headings":"Deep Dive: Core Design Patterns","what":"Strategy Pattern: Flexible Chunking","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"strategy pattern enables different chunking strategies based analysis requirements. interface supports multiple approaches data subdivision: Different strategies optimize different analysis patterns maintaining programming interface.","code":"# Strategy 1: Voxel-based chunking (default) voxel_chunks <- data_chunks(matrix_ds, nchunks = 4) cat(\"Voxel chunking: \", length(voxel_chunks), \"chunks\\n\")  # Strategy 2: Run-based chunking run_chunks <- data_chunks(matrix_ds, runwise = TRUE) cat(\"Run chunking: \", length(run_chunks), \"chunks\\n\")  # Strategy 3: Single chunk (no subdivision) single_chunk <- data_chunks(matrix_ds, nchunks = 1) cat(\"Single chunking: \", length(single_chunk), \"chunks\\n\")  # Each strategy optimizes for different use cases for (i in 1:min(2, length(voxel_chunks))) {   chunk <- voxel_chunks[[i]]   cat(\"Voxel chunk\", i, \":\", ncol(chunk$data), \"voxels\\n\") }  for (i in 1:length(run_chunks)) {   chunk <- run_chunks[[i]]   cat(\"Run chunk\", i, \":\", nrow(chunk$data), \"timepoints\\n\") }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"extension-points-customizing-the-architecture","dir":"Articles","previous_headings":"","what":"Extension Points: Customizing the Architecture","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"architecture provides several well-defined extension points allow customize behavior add new capabilities without modifying core code.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"custom-dataset-types","dir":"Articles","previous_headings":"Extension Points: Customizing the Architecture","what":"Custom Dataset Types","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"Creating new dataset types allows support specialized data sources add domain-specific functionality: Custom dataset types inherit standard functionality adding specialized capabilities.","code":"# Example: Create a dataset type for ROI (Region of Interest) time series roi_dataset <- function(roi_timeseries, roi_labels, roi_coordinates = NULL, TR, ...) {   # Validate inputs   if (!is.matrix(roi_timeseries)) {     stop(\"roi_timeseries must be a matrix\")   }    if (ncol(roi_timeseries) != length(roi_labels)) {     stop(\"Number of ROIs must match label length\")   }    # Create underlying matrix dataset with actual time series data   base_dataset <- matrix_dataset(     datamat = roi_timeseries, # timepoints × regions matrix     TR = TR,     ...   )    # Add ROI-specific metadata   base_dataset$roi_labels <- roi_labels   base_dataset$roi_coordinates <- roi_coordinates   base_dataset$data_type <- \"roi_timeseries\"    # Set class for method dispatch   class(base_dataset) <- c(\"roi_dataset\", class(base_dataset))    return(base_dataset) }  # Create ROI-specific methods get_roi_labels <- function(dataset) {   UseMethod(\"get_roi_labels\") }  get_roi_labels.roi_dataset <- function(dataset) {   dataset$roi_labels }  get_roi_coordinates <- function(dataset) {   UseMethod(\"get_roi_coordinates\") }  get_roi_coordinates.roi_dataset <- function(dataset) {   dataset$roi_coordinates }  # Usage: Create ROI time series data (100 timepoints × 8 brain regions) set.seed(42) roi_timeseries <- matrix(rnorm(800), nrow = 100, ncol = 8) roi_coords <- matrix(c(   -45, -65, 30, # Left angular gyrus   45, -65, 30, # Right angular gyrus   -40, -85, 15, # Left middle occipital   40, -85, 15, # Right middle occipital   -50, -25, 45, # Left supramarginal   50, -25, 45, # Right supramarginal   0, 10, 50, # Medial frontal   0, -50, 25 # Posterior cingulate ), nrow = 8, ncol = 3, byrow = TRUE)  roi_ds <- roi_dataset(   roi_timeseries = roi_timeseries,   roi_labels = c(     \"L_Angular\", \"R_Angular\", \"L_MOG\", \"R_MOG\",     \"L_Supramarginal\", \"R_Supramarginal\", \"Med_Frontal\", \"PCC\"   ),   roi_coordinates = roi_coords,   TR = 2.0,   run_length = 100 )  cat(\"Created ROI dataset with\", length(get_roi_labels(roi_ds)), \"regions\\n\") cat(\"First ROI:\", get_roi_labels(roi_ds)[1], \"at coordinates\", get_roi_coordinates(roi_ds)[1, ], \"\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"custom-backend-implementation","dir":"Articles","previous_headings":"Extension Points: Customizing the Architecture","what":"Custom Backend Implementation","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"New backends enable support additional data formats storage systems: backend registered used throughout fmridataset ecosystem.","code":"# Example: Simple CSV backend implementation csv_backend <- function(csv_file, ...) {   # Validate file exists   if (!file.exists(csv_file)) {     stop(\"CSV file not found: \", csv_file)   }    # Create backend object   backend <- list(     csv_file = csv_file,     data_cache = NULL,     is_open = FALSE   )    class(backend) <- c(\"csv_backend\", \"storage_backend\")   backend }  # Implement required backend methods backend_open.csv_backend <- function(backend) {   if (!backend$is_open) {     # Load data when opened     backend$data_cache <- as.matrix(read.csv(backend$csv_file, header = FALSE))     backend$is_open <- TRUE   }   backend }  backend_close.csv_backend <- function(backend) {   backend$data_cache <- NULL   backend$is_open <- FALSE   invisible(NULL) }  backend_get_dims.csv_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Backend must be opened before querying dimensions\")   }   list(     spatial = c(ncol(backend$data_cache), 1, 1),     time = nrow(backend$data_cache)   ) }  backend_get_data.csv_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) {     stop(\"Backend must be opened before accessing data\")   }    data <- backend$data_cache   if (!is.null(rows)) data <- data[rows, , drop = FALSE]   if (!is.null(cols)) data <- data[, cols, drop = FALSE]    data }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"custom-temporal-abstractions","dir":"Articles","previous_headings":"Extension Points: Customizing the Architecture","what":"Custom Temporal Abstractions","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"sampling frame system can extended support specialized temporal organizations: Custom temporal abstractions enable support specialized acquisition protocols.","code":"# Example: Variable TR sampling frame for multi-echo data variable_tr_sampling_frame <- function(tr_sequence, run_boundaries, ...) {   # Validate inputs   if (length(tr_sequence) != sum(run_boundaries)) {     stop(\"TR sequence length must match total timepoints\")   }    # Create base sampling frame   base_sf <- sampling_frame(     run_lengths = run_boundaries,     TR = mean(tr_sequence), # Use mean TR for compatibility     ...   )    # Add variable TR information   base_sf$tr_sequence <- tr_sequence   base_sf$run_boundaries <- cumsum(c(0, run_boundaries))    class(base_sf) <- c(\"variable_tr_sampling_frame\", class(base_sf))   base_sf }  # Add specialized methods get_tr_at_timepoint <- function(sf, timepoint) {   UseMethod(\"get_tr_at_timepoint\") }  get_tr_at_timepoint.variable_tr_sampling_frame <- function(sf, timepoint) {   if (timepoint < 1 || timepoint > length(sf$tr_sequence)) {     stop(\"Timepoint out of range\")   }   sf$tr_sequence[timepoint] }  # Example usage tr_seq <- rep(c(2.0, 2.5, 1.5), times = c(20, 20, 20)) # Variable TRs var_sf <- variable_tr_sampling_frame(   tr_sequence = tr_seq,   run_boundaries = c(30, 30) )  cat(\"TR at timepoint 10:\", get_tr_at_timepoint(var_sf, 10), \"seconds\\n\") cat(\"TR at timepoint 25:\", get_tr_at_timepoint(var_sf, 25), \"seconds\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"advanced-topics","dir":"Articles","previous_headings":"","what":"Advanced Topics","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"understand basic architectural patterns, advanced concepts help leverage full power system optimize complex use cases.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"performance-architecture","dir":"Articles","previous_headings":"Advanced Topics","what":"Performance Architecture","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"architecture designed performance primary consideration. Understanding performance implications different architectural choices helps optimize analyses: performance characteristics guide backend selection based specific use case. lazy evaluation architecture enables working datasets larger memory deferring expensive operations absolutely necessary.","code":"# Benchmark different backend types benchmark_backends <- function(n_timepoints = 100, n_voxels = 1000) {   # Generate test data   test_data <- generate_example_fmri_data(n_timepoints, n_voxels)    # Benchmark matrix backend   start_time <- Sys.time()   matrix_ds <- matrix_dataset(     datamat = test_data,     TR = 2.0,     run_length = n_timepoints   )   matrix_create_time <- difftime(Sys.time(), start_time, units = \"secs\")    start_time <- Sys.time()   matrix_data <- get_data_matrix(matrix_ds)   matrix_access_time <- difftime(Sys.time(), start_time, units = \"secs\")    cat(\"Performance Comparison:\\n\")   cat(\"====================\\n\")   cat(sprintf(\"Dataset size: %d timepoints × %d voxels\\n\", n_timepoints, n_voxels))   cat(sprintf(\"Memory size: %.1f MB\\n\\n\", object.size(test_data) / 1024^2))    cat(\"Matrix Backend:\\n\")   cat(sprintf(\"  Creation time: %.3f seconds\\n\", matrix_create_time))   cat(sprintf(\"  Data access time: %.3f seconds\\n\", matrix_access_time))   cat(\"  Memory model: All data in RAM\\n\")   cat(\"  Best for: Small datasets, repeated access\\n\\n\")    # Simulate file backend performance   cat(\"File Backend (simulated):\\n\")   cat(\"  Creation time: ~0.001 seconds (lazy)\\n\")   cat(\"  First access time: ~1-5 seconds (disk I/O)\\n\")   cat(\"  Memory model: Load on demand\\n\")   cat(\"  Best for: Large datasets, sequential access\\n\\n\")    # Simulate study backend performance   cat(\"Study Backend (simulated):\\n\")   cat(\"  Creation time: ~0.01 seconds (metadata only)\\n\")   cat(\"  Access time: Varies by subject\\n\")   cat(\"  Memory model: Per-subject lazy loading\\n\")   cat(\"  Best for: Multi-subject analyses\\n\") }  # Run benchmark benchmark_backends(n_timepoints = 200, n_voxels = 5000) # Benchmark chunking strategies benchmark_chunking <- function(dataset, chunk_sizes = c(1, 5, 10, 20)) {   cat(\"\\nChunking Performance Analysis:\\n\")   cat(\"============================\\n\")    results <- data.frame(     chunks = chunk_sizes,     time = numeric(length(chunk_sizes)),     memory_peak = numeric(length(chunk_sizes))   )    for (i in seq_along(chunk_sizes)) {     n_chunks <- chunk_sizes[i]      start_time <- Sys.time()     chunks <- data_chunks(dataset, nchunks = n_chunks)      # Simulate processing     for (chunk in chunks) {       # Simple operation on each chunk       chunk_mean <- mean(chunk$data)     }      results$time[i] <- as.numeric(difftime(Sys.time(), start_time, units = \"secs\"))     results$memory_peak[i] <- n_timepoints(dataset) * ncol(dataset$datamat) / n_chunks * 8 / 1024^2   }    print(results)    cat(\"\\nRecommendations:\\n\")   cat(\"- More chunks = Lower memory usage\\n\")   cat(\"- Fewer chunks = Better performance (less overhead)\\n\")   cat(\"- Optimal chunk size depends on available RAM\\n\") }  # Create test dataset and benchmark test_ds <- matrix_dataset(   generate_example_fmri_data(100, 1000),   TR = 2.0,   run_length = 100 )  benchmark_chunking(test_ds)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"memory-management-architecture","dir":"Articles","previous_headings":"Advanced Topics","what":"Memory Management Architecture","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"architecture provides sophisticated memory management backend-specific strategies: Different backends implement different memory strategies optimized use cases.","code":"# Memory usage patterns for different backends analyze_memory_patterns <- function() {   # Matrix backend: immediate memory allocation   matrix_data <- matrix(rnorm(10000), nrow = 100, ncol = 100)   matrix_size <- object.size(matrix_data)    cat(\"Matrix backend:\\n\")   cat(\"  Data size:\", format(matrix_size, units = \"Mb\"), \"\\n\")   cat(\"  Memory allocation: immediate\\n\")   cat(\"  Access pattern: O(1) random access\\n\\n\")    # File backend simulation   cat(\"File backend:\\n\")   cat(\"  Data size: 0 bytes (until accessed)\\n\")   cat(\"  Memory allocation: on-demand\\n\")   cat(\"  Access pattern: O(n) sequential preferred\\n\\n\")    # Study backend simulation   cat(\"Study backend:\\n\")   cat(\"  Data size: per-subject allocation\\n\")   cat(\"  Memory allocation: lazy per subject\\n\")   cat(\"  Access pattern: optimized for chunking\\n\") }  analyze_memory_patterns()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"extensibility-architecture","dir":"Articles","previous_headings":"Advanced Topics","what":"Extensibility Architecture","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"architecture’s extensibility comes layered design well-defined interfaces: extensibility architecture enables ecosystem grow maintaining stability.","code":"# Demonstrate interface consistency across extensions demonstrate_interface_consistency <- function() {   # All datasets implement the same core interface   core_methods <- c(\"get_data_matrix\", \"get_TR\", \"n_runs\", \"n_timepoints\", \"data_chunks\")    cat(\"Core interface methods:\\n\")   for (method in core_methods) {     cat(\"  -\", method, \"\\n\")   }    cat(\"\\nInterface guarantees:\\n\")   cat(\"  - Same method signatures across all dataset types\\n\")   cat(\"  - Consistent return value formats\\n\")   cat(\"  - Predictable error handling\\n\")   cat(\"  - Backward compatibility preservation\\n\") }  demonstrate_interface_consistency()  # Extensions can add new methods without breaking existing code add_custom_methods <- function() {   cat(\"\\nExtension pattern:\\n\")   cat(\"  1. Inherit from base classes\\n\")   cat(\"  2. Implement required interface methods\\n\")   cat(\"  3. Add specialized functionality\\n\")   cat(\"  4. Register with appropriate systems\\n\") }  add_custom_methods()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"tips-and-best-practices","dir":"Articles","previous_headings":"","what":"Tips and Best Practices","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"architectural insights help use fmridataset effectively build robust extensions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"performance-considerations","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Performance Considerations","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"Backend Selection Guidelines: - Matrix backends: Suitable datasets 8GB frequent random access patterns - File backends: Optimal large datasets (>8GB) sequential access patterns - Study backends: Required multi-subject analyses subject-level lazy loading","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"implementation-requirements","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Implementation Requirements","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"Backend Interface Compliance: Custom backends must implement six required methods backend contract. Partial implementations cause failures chunking operations, study-level analyses, advanced features depend complete interface.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"extension-patterns","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Extension Patterns","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"Delegation Strategy: extending functionality, delegate existing components rather reimplementing core features. approach maintains consistency, reduces code duplication, ensures compatibility future updates.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"architectural-decision-guidelines","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Architectural Decision Guidelines","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"designing extensions choosing implementation approaches:","code":"# Decision framework for extensions evaluate_extension_approach <- function(requirement) {   cat(\"Extension decision framework:\\n\\n\")    cat(\"1. Data source extension:\\n\")   cat(\"   - New file format? → Custom backend\\n\")   cat(\"   - New data organization? → Custom dataset\\n\")   cat(\"   - New temporal structure? → Custom sampling frame\\n\\n\")    cat(\"2. Analysis extension:\\n\")   cat(\"   - New processing pattern? → Custom chunking strategy\\n\")   cat(\"   - New metadata needs? → Dataset subclass\\n\")   cat(\"   - New access pattern? → Custom methods\\n\\n\")    cat(\"3. Performance extension:\\n\")   cat(\"   - Memory optimization? → Backend specialization\\n\")   cat(\"   - I/O optimization? → Custom caching\\n\")   cat(\"   - Parallel processing? → Chunking extensions\\n\") }  evaluate_extension_approach()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"interface-design-principles","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Interface Design Principles","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"extending architecture, follow interface design principles: principles ensure extensions integrate smoothly existing ecosystem.","code":"demonstrate_interface_principles <- function() {   cat(\"Interface design principles:\\n\\n\")    cat(\"1. Consistency:\\n\")   cat(\"   - Same method names across similar components\\n\")   cat(\"   - Predictable parameter patterns\\n\")   cat(\"   - Uniform error handling\\n\\n\")    cat(\"2. Composability:\\n\")   cat(\"   - Components work together seamlessly\\n\")   cat(\"   - Clear separation of concerns\\n\")   cat(\"   - Minimal coupling between layers\\n\\n\")    cat(\"3. Extensibility:\\n\")   cat(\"   - Well-defined extension points\\n\")   cat(\"   - Backward compatibility guarantees\\n\")   cat(\"   - Future-proof interface design\\n\") }  demonstrate_interface_principles()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"troubleshooting-architecture-issues","dir":"Articles","previous_headings":"","what":"Troubleshooting Architecture Issues","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"Understanding architecture helps diagnose resolve complex issues span multiple components.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"layer-specific-debugging","dir":"Articles","previous_headings":"Troubleshooting Architecture Issues","what":"Layer-Specific Debugging","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"Different types issues typically originate specific architectural layers: Dataset Layer Issues Method dispatch problems, interface inconsistencies, temporal validation errors Backend Layer Issues File /O problems, memory allocation failures, format-specific errors Temporal Layer Issues Run boundary mismatches, timing calculation errors, event alignment problems","code":"# Debugging strategy by architectural layer debug_by_layer <- function(error_type) {   cat(\"Debugging strategy by layer:\\n\\n\")    cat(\"Dataset layer debugging:\\n\")   cat(\"  - Check class hierarchy: class(dataset)\\n\")   cat(\"  - Verify method dispatch: methods(class = class(dataset)[1])\\n\")   cat(\"  - Validate temporal structure: dataset$sampling_frame\\n\\n\")    cat(\"Backend layer debugging:\\n\")   cat(\"  - Check backend status: dataset$backend\\n\")   cat(\"  - Test direct backend calls: backend_get_dims(backend)\\n\")   cat(\"  - Verify resource state: backend state variables\\n\\n\")    cat(\"Temporal layer debugging:\\n\")   cat(\"  - Check sampling frame: get_run_lengths(dataset)\\n\")   cat(\"  - Verify timing: get_TR(dataset) * n_timepoints(dataset)\\n\")   cat(\"  - Test event alignment: validate event onsets\\n\") }  debug_by_layer()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"performance-troubleshooting","dir":"Articles","previous_headings":"Troubleshooting Architecture Issues","what":"Performance Troubleshooting","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"Architecture-aware performance debugging: Understanding architecture helps identify root cause performance issues.","code":"diagnose_performance_issues <- function() {   cat(\"Performance diagnosis by component:\\n\\n\")    cat(\"Slow data access:\\n\")   cat(\"  - Backend type: file vs. matrix vs. study\\n\")   cat(\"  - Chunking strategy: voxel vs. run vs. custom\\n\")   cat(\"  - Memory pressure: check available RAM\\n\\n\")    cat(\"High memory usage:\\n\")   cat(\"  - Lazy loading: ensure file backends stay lazy\\n\")   cat(\"  - Chunk sizing: reduce chunk size for large datasets\\n\")   cat(\"  - Garbage collection: explicit gc() calls\\n\\n\")    cat(\"Interface inconsistencies:\\n\")   cat(\"  - Method dispatch: verify S3 method registration\\n\")   cat(\"  - Class hierarchy: check inheritance patterns\\n\")   cat(\"  - Extension conflicts: check for method overrides\\n\") }  diagnose_performance_issues()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"integration-with-other-vignettes","dir":"Articles","previous_headings":"","what":"Integration with Other Vignettes","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"architectural overview connects several aspects fmridataset ecosystem: Prerequisites: Start Getting Started understand basic usage patterns diving architectural details. Implementation Guides: - Backend Registry - Practical guide creating registering custom backends - Extending Backends - Deep dive backend development patterns - Study-Level Analysis - Understand architecture scales multi-subject studies Applied Examples: - H5 Backend Usage - See advanced backend features practice Design Philosophy: architecture reflects broader principles modular design, separation concerns, extensibility common scientific computing frameworks. Understanding patterns help work effectively packages neuroimaging ecosystem.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/architecture-overview.html","id":"session-information","dir":"Articles","previous_headings":"","what":"Session Information","title":"Architecture Deep Dive: Design Principles and Extensibility","text":"","code":"sessionInfo()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"motivation-why-create-custom-backends","dir":"Articles","previous_headings":"","what":"Motivation: Why Create Custom Backends?","title":"Backend Development Basics: Creating Custom Storage Backends","text":"research group using custom MATLAB pipeline exports preprocessed fMRI data JSON files separate metadata. data format optimized specific analyses, includes custom quality metrics, integrates lab’s database system. Rather converting data standard formats writing custom loading code analysis, can create backend makes format work seamlessly fmridataset. Creating custom backend means specialized data format immediately gains access fmridataset features: unified interfaces, efficient chunking, study-level operations, compatibility entire ecosystem. vignette teaches essentials backend development practical examples, showing implement required interface optimize specific use case.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"required-methods","dir":"Articles","previous_headings":"Backend Contract Specifications","what":"Required Methods","title":"Backend Development Basics: Creating Custom Storage Backends","text":"storage backends must implement six core methods:","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_openbackend","dir":"Articles","previous_headings":"Backend Contract Specifications > Required Methods","what":"backend_open(backend)","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Purpose: Initialize backend acquire necessary resources. Requirements: - Must idempotent (safe call multiple times) - Must set internal state track open status - Must validate data source accessibility - Must return modified backend object Implementation Requirements:","code":"backend_open.my_backend <- function(backend) {   if (backend$is_open) return(backend)   # Acquire resources, validate data source   backend$is_open <- TRUE   return(backend) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_closebackend","dir":"Articles","previous_headings":"Backend Contract Specifications > Required Methods","what":"backend_close(backend)","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Purpose: Release resources clean state. Requirements: - Must idempotent (safe call closed backends) - Must release held resources (files, connections, memory) - Must set internal state indicate closed status - Must return invisibly Implementation Requirements:","code":"backend_close.my_backend <- function(backend) {   if (!backend$is_open) return(invisible(backend))   # Release resources   backend$is_open <- FALSE   return(invisible(backend)) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_get_dimsbackend","dir":"Articles","previous_headings":"Backend Contract Specifications > Required Methods","what":"backend_get_dims(backend)","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Purpose: Return data dimensions canonical format. Requirements: - Must return list spatial time elements - spatial must integer vector c(n_voxels, n_spatial_dim2, n_spatial_dim3) - time must integer scalar indicating number timepoints - Must work open backends Implementation Requirements:","code":"backend_get_dims.my_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend must be opened first\")   list(spatial = c(n_voxels, 1, 1), time = n_timepoints) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_get_databackend-rows-null-cols-null","dir":"Articles","previous_headings":"Backend Contract Specifications > Required Methods","what":"backend_get_data(backend, rows = NULL, cols = NULL)","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Purpose: Extract data matrix optional subsetting. Requirements: - Must return matrix dimensions (timepoints × voxels) - rows parameter: integer vector specifying timepoint indices (1-based) - cols parameter: integer vector specifying voxel indices (1-based) - Must handle NULL parameters (return data) - Must preserve matrix format single row/column selections","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_get_maskbackend","dir":"Articles","previous_headings":"Backend Contract Specifications > Required Methods","what":"backend_get_mask(backend)","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Purpose: Return logical mask indicating valid voxels. Requirements: - Must return logical vector length equal number spatial elements - TRUE indicates valid voxel, FALSE indicates invalid/missing voxel - Must consistent data returned backend_get_data()","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_validatebackend-optional","dir":"Articles","previous_headings":"Backend Contract Specifications > Required Methods","what":"backend_validate(backend) (Optional)","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Purpose: Validate backend state data integrity. Requirements: - Return logical scalar: TRUE valid, FALSE invalid - check internal consistency, data availability, format compliance","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"performance-requirements","dir":"Articles","previous_headings":"Backend Contract Specifications","what":"Performance Requirements","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Lazy Loading: Avoid loading large datasets constructor backend_open() Memory Efficiency: Load requested data subsets backend_get_data() Idempotency: backend_open() backend_close() must safely repeatable Error Handling: Provide informative error messages common failure modes","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"state-management-requirements","dir":"Articles","previous_headings":"Backend Contract Specifications","what":"State Management Requirements","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Open State Tracking: Maintain internal is_open flag Resource Management: Clean resource acquisition release patterns Thread Safety: Consider concurrent access backend used parallel contexts","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"quick-start-json-backend-example","dir":"Articles","previous_headings":"","what":"Quick Start: JSON Backend Example","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Let’s create working backend JSON-formatted fMRI data demonstrates essential concepts: Interface Specification: Backends require implementation six core methods integrate fmridataset ecosystem. minimal interface enables full compatibility package features.","code":"# Create a complete JSON backend implementation json_backend <- function(json_file, metadata_file = NULL) {   # Input validation   if (!file.exists(json_file)) {     stop(\"JSON file not found: \", json_file)   }    # Initialize backend structure   backend <- list(     json_file = json_file,     metadata_file = metadata_file,     data_cache = NULL,     dims_cache = NULL,     is_open = FALSE   )    class(backend) <- c(\"json_backend\", \"storage_backend\")   backend }  # Implement the open method backend_open.json_backend <- function(backend) {   if (backend$is_open) {     return(backend) # Already open   }    # Simulate reading JSON data (in practice, use jsonlite::fromJSON)   # For demonstration, create synthetic data   set.seed(123)   n_time <- 100   n_voxels <- 500    backend$data_cache <- matrix(     rnorm(n_time * n_voxels),     nrow = n_time,     ncol = n_voxels   )    backend$dims_cache <- list(     spatial = c(n_voxels, 1, 1), # Flat spatial structure     time = n_time   )    backend$is_open <- TRUE   backend }  # Implement the close method backend_close.json_backend <- function(backend) {   backend$data_cache <- NULL   backend$dims_cache <- NULL   backend$is_open <- FALSE   invisible(backend) }  # Implement dimension query backend_get_dims.json_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Backend must be opened first\")   }   backend$dims_cache }  # Implement data access backend_get_data.json_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) {     stop(\"Backend must be opened first\")   }    data <- backend$data_cache    # Handle subsetting   if (!is.null(rows)) {     data <- data[rows, , drop = FALSE]   }   if (!is.null(cols)) {     data <- data[, cols, drop = FALSE]   }    data }  # Implement mask generation backend_get_mask.json_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Backend must be opened first\")   }    # All voxels are valid in our JSON format   rep(TRUE, backend$dims_cache$spatial[1]) }  # Test the backend json_file <- tempfile(fileext = \".json\") writeLines(\"{}\", json_file) # Create dummy file  backend <- json_backend(json_file) backend <- backend_open(backend)  dims <- backend_get_dims(backend) cat(\"Backend dimensions - Time:\", dims$time, \"Spatial:\", dims$spatial[1], \"\\n\") #> Backend dimensions - Time: 100 Spatial: 500  # Get some data sample_data <- backend_get_data(backend, rows = 1:10, cols = 1:50) cat(\"Retrieved data shape:\", dim(sample_data), \"\\n\") #> Retrieved data shape: 10 50  backend_close(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"understanding-the-backend-contract","dir":"Articles","previous_headings":"","what":"Understanding the Backend Contract","title":"Backend Development Basics: Creating Custom Storage Backends","text":"backend contract defines minimal interface backends must implement. Understanding contract essential creating compatible backends.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"required-methods-1","dir":"Articles","previous_headings":"Understanding the Backend Contract","what":"Required Methods","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Every backend must implement five S3 methods:","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_open","dir":"Articles","previous_headings":"Understanding the Backend Contract > Required Methods","what":"1. backend_open()","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Opens backend acquires resources (file handles, connections, memory). method idempotent - calling multiple times open backend safe.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_close","dir":"Articles","previous_headings":"Understanding the Backend Contract > Required Methods","what":"2. backend_close()","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Releases resources cleans . closing, backend hold external resources.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_get_dims","dir":"Articles","previous_headings":"Understanding the Backend Contract > Required Methods","what":"3. backend_get_dims()","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Returns dimensions list spatial (3-element vector) time (single integer) elements. must work without loading data.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_get_data","dir":"Articles","previous_headings":"Understanding the Backend Contract > Required Methods","what":"4. backend_get_data()","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Returns data timepoints × voxels orientation. Must support optional row column subsetting efficient partial loading.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend_get_mask","dir":"Articles","previous_headings":"Understanding the Backend Contract > Required Methods","what":"5. backend_get_mask()","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Returns logical vector indicating valid voxels. Length must equal product spatial dimensions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"optional-methods","dir":"Articles","previous_headings":"Understanding the Backend Contract","what":"Optional Methods","title":"Backend Development Basics: Creating Custom Storage Backends","text":"methods enhance functionality aren’t required:","code":"# Optional: Metadata extraction backend_get_metadata.json_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Backend must be opened first\")   }    list(     format = \"json\",     compression = \"none\",     creation_date = Sys.Date(),     custom_metrics = list(       quality_score = 0.95,       motion_level = \"low\"     )   ) }  # Optional: Validation backend_validate.json_backend <- function(backend) {   # Check data integrity   if (!backend$is_open) {     return(FALSE)   }    # Validate dimensions   dims <- backend$dims_cache   expected_size <- dims$time * dims$spatial[1]   actual_size <- length(backend$data_cache)    if (expected_size != actual_size) {     warning(\"Data size mismatch\")     return(FALSE)   }    TRUE }  # Test optional methods backend <- backend_open(json_backend(json_file)) metadata <- backend_get_metadata(backend) cat(\"Format:\", metadata$format, \"\\n\") cat(\"Quality score:\", metadata$custom_metrics$quality_score, \"\\n\")  is_valid <- backend_validate(backend) cat(\"Backend valid:\", is_valid, \"\\n\") backend_close(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"deep-dive-implementation-patterns","dir":"Articles","previous_headings":"","what":"Deep Dive: Implementation Patterns","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Let’s explore common patterns make backends robust efficient.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"state-management-pattern","dir":"Articles","previous_headings":"Deep Dive: Implementation Patterns","what":"State Management Pattern","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Backends need track whether ’re open manage resources appropriately:","code":"# Robust state management example stateful_backend <- function(source) {   backend <- list(     source = source,     # State flags     is_open = FALSE,     is_validated = FALSE,     has_error = FALSE,     # Resource tracking     resources = list(),     # Error information     last_error = NULL   )    class(backend) <- c(\"stateful_backend\", \"storage_backend\")   backend }  # Safe resource acquisition backend_open.stateful_backend <- function(backend) {   if (backend$is_open) {     return(backend) # Idempotent   }    tryCatch(     {       # Acquire resources       backend$resources$data <- matrix(rnorm(1000), 100, 10)       backend$is_open <- TRUE       backend$has_error <- FALSE     },     error = function(e) {       backend$has_error <- TRUE       backend$last_error <- conditionMessage(e)       stop(\"Failed to open backend: \", conditionMessage(e))     }   )    backend }  # Safe resource cleanup backend_close.stateful_backend <- function(backend) {   if (!backend$is_open) {     return(invisible(backend)) # Already closed   }    # Release all resources   backend$resources <- list()   backend$is_open <- FALSE    invisible(backend) }  # Implement other required methods... backend_get_dims.stateful_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")   list(spatial = c(10, 1, 1), time = 100) }  backend_get_data.stateful_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) stop(\"Backend not open\")   data <- backend$resources$data   if (!is.null(rows)) data <- data[rows, , drop = FALSE]   if (!is.null(cols)) data <- data[, cols, drop = FALSE]   data }  backend_get_mask.stateful_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")   rep(TRUE, 10) }  # Test state management backend <- stateful_backend(\"dummy_source\") cat(\"Initial state - is_open:\", backend$is_open, \"\\n\") #> Initial state - is_open: FALSE  backend <- backend_open(backend) cat(\"After open - is_open:\", backend$is_open, \"\\n\") #> After open - is_open: TRUE  backend <- backend_close(backend) cat(\"After close - is_open:\", backend$is_open, \"\\n\") #> After close - is_open: FALSE"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"lazy-loading-pattern","dir":"Articles","previous_headings":"Deep Dive: Implementation Patterns","what":"Lazy Loading Pattern","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Implement lazy loading defer expensive operations:","code":"# Lazy loading backend lazy_backend <- function(data_source) {   backend <- list(     data_source = data_source,     is_open = FALSE,     # Lazy caches     dims_cache = NULL,     data_cache = NULL,     mask_cache = NULL   )    class(backend) <- c(\"lazy_backend\", \"storage_backend\")   backend }  backend_open.lazy_backend <- function(backend) {   backend$is_open <- TRUE   # Don't load data yet!   backend }  backend_get_dims.lazy_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")    # Load dimensions only when first requested   if (is.null(backend$dims_cache)) {     # In practice, read just headers/metadata     backend$dims_cache <- list(       spatial = c(100, 1, 1),       time = 50     )   }    backend$dims_cache }  backend_get_data.lazy_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) stop(\"Backend not open\")    # Load data only when first accessed   if (is.null(backend$data_cache)) {     cat(\"Loading data (lazy)...\\n\")     backend$data_cache <- matrix(rnorm(5000), 50, 100)   }    data <- backend$data_cache   if (!is.null(rows)) data <- data[rows, , drop = FALSE]   if (!is.null(cols)) data <- data[, cols, drop = FALSE]   data }  backend_get_mask.lazy_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")    if (is.null(backend$mask_cache)) {     dims <- backend_get_dims(backend)     backend$mask_cache <- rep(TRUE, dims$spatial[1])   }    backend$mask_cache }  backend_close.lazy_backend <- function(backend) {   backend$dims_cache <- NULL   backend$data_cache <- NULL   backend$mask_cache <- NULL   backend$is_open <- FALSE   invisible(backend) }  # Demonstrate lazy loading backend <- lazy_backend(\"source\") backend <- backend_open(backend)  cat(\"Getting dimensions...\\n\") #> Getting dimensions... dims <- backend_get_dims(backend) # No data loading  cat(\"\\nGetting mask...\\n\") #>  #> Getting mask... mask <- backend_get_mask(backend) # Still no data loading  cat(\"\\nGetting data...\\n\") #>  #> Getting data... data <- backend_get_data(backend, rows = 1:10) # NOW data loads #> Loading data (lazy)...  backend_close(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"validation-pattern","dir":"Articles","previous_headings":"Deep Dive: Implementation Patterns","what":"Validation Pattern","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Implement validation ensure data integrity:","code":"# Create validation utilities validate_backend_contract <- function(backend_class) {   required_methods <- c(     \"backend_open\",     \"backend_close\",     \"backend_get_dims\",     \"backend_get_data\",     \"backend_get_mask\"   )    missing_methods <- character()    for (method in required_methods) {     full_method <- paste0(method, \".\", backend_class)     if (!exists(full_method)) {       missing_methods <- c(missing_methods, method)     }   }    if (length(missing_methods) > 0) {     stop(       \"Backend class '\", backend_class, \"' missing required methods: \",       paste(missing_methods, collapse = \", \")     )   }    cat(\"✓ Backend class '\", backend_class, \"' implements all required methods\\n\")   TRUE }  # Test our backends validate_backend_contract(\"json_backend\") #> ✓ Backend class ' json_backend ' implements all required methods #> [1] TRUE validate_backend_contract(\"lazy_backend\") #> ✓ Backend class ' lazy_backend ' implements all required methods #> [1] TRUE"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"caching-strategies","dir":"Articles","previous_headings":"Advanced Topics","what":"Caching Strategies","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Implement intelligent caching better performance:","code":"# Advanced caching backend cached_backend <- function(source, cache_size_mb = 100) {   backend <- list(     source = source,     cache_size_mb = cache_size_mb,     is_open = FALSE,     # Multi-level cache     cache = list(       dims = NULL,       mask = NULL,       data_blocks = list(),       access_times = list()     ),     # Cache statistics     stats = list(       hits = 0,       misses = 0,       evictions = 0     )   )    class(backend) <- c(\"cached_backend\", \"storage_backend\")   backend }  # Implement cache management cache_get_or_load <- function(backend, key, loader_fn) {   if (!is.null(backend$cache[[key]])) {     backend$stats$hits <- backend$stats$hits + 1     cat(\"Cache hit for\", key, \"\\n\")     return(backend$cache[[key]])   }    backend$stats$misses <- backend$stats$misses + 1   cat(\"Cache miss for\", key, \"- loading...\\n\")    value <- loader_fn()   backend$cache[[key]] <- value   backend$cache$access_times[[key]] <- Sys.time()    value }  backend_open.cached_backend <- function(backend) {   backend$is_open <- TRUE   backend }  backend_get_dims.cached_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")    cache_get_or_load(backend, \"dims\", function() {     list(spatial = c(100, 1, 1), time = 50)   }) }  backend_get_data.cached_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) stop(\"Backend not open\")    # Create cache key based on request   cache_key <- paste0(     \"data_\",     paste(range(rows %||% 1:50), collapse = \"_\"),     \"_\",     paste(range(cols %||% 1:100), collapse = \"_\")   )    data <- cache_get_or_load(backend, cache_key, function() {     matrix(rnorm(5000), 50, 100)   })    if (!is.null(rows)) data <- data[rows, , drop = FALSE]   if (!is.null(cols)) data <- data[, cols, drop = FALSE]   data }  backend_get_mask.cached_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")    cache_get_or_load(backend, \"mask\", function() {     rep(TRUE, 100)   }) }  backend_close.cached_backend <- function(backend) {   # Report cache statistics   cat(\"\\nCache statistics:\\n\")   cat(\"  Hits:\", backend$stats$hits, \"\\n\")   cat(\"  Misses:\", backend$stats$misses, \"\\n\")   cat(     \"  Hit rate:\",     round(100 * backend$stats$hits /       (backend$stats$hits + backend$stats$misses), 1), \"%\\n\"   )    backend$cache <- list()   backend$is_open <- FALSE   invisible(backend) }  # Demonstrate caching `%||%` <- function(x, y) if (is.null(x)) y else x  backend <- cached_backend(\"source\") backend <- backend_open(backend)  # First access - cache miss data1 <- backend_get_data(backend, rows = 1:10) #> Cache miss for data_1_10_1_100 - loading...  # Second access - cache hit data2 <- backend_get_data(backend, rows = 1:10) #> Cache miss for data_1_10_1_100 - loading...  # Different subset - cache miss data3 <- backend_get_data(backend, rows = 11:20) #> Cache miss for data_11_20_1_100 - loading...  backend_close(backend) #>  #> Cache statistics: #>   Hits: 0  #>   Misses: 0  #>   Hit rate: NaN %"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"error-handling","dir":"Articles","previous_headings":"Advanced Topics","what":"Error Handling","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Robust error handling makes backends production-ready:","code":"# Create a backend with comprehensive error handling robust_backend <- function(source) {   backend <- list(     source = source,     is_open = FALSE,     error_log = list()   )    class(backend) <- c(\"robust_backend\", \"storage_backend\")   backend }  # Helper to log errors log_error <- function(backend, operation, error) {   backend$error_log[[length(backend$error_log) + 1]] <- list(     timestamp = Sys.time(),     operation = operation,     message = conditionMessage(error)   )   backend }  backend_open.robust_backend <- function(backend) {   tryCatch(     {       if (backend$is_open) {         warning(\"Backend already open\")         return(backend)       }        # Simulate potential failures       if (runif(1) > 0.8) {         stop(\"Simulated connection failure\")       }        backend$is_open <- TRUE       cat(\"Successfully opened backend\\n\")       backend     },     error = function(e) {       backend <- log_error(backend, \"open\", e)       stop(\"Failed to open backend: \", conditionMessage(e))     }   ) }  backend_get_data.robust_backend <- function(backend, rows = NULL, cols = NULL) {   tryCatch(     {       if (!backend$is_open) {         stop(\"Backend not open\")       }        # Validate indices       if (!is.null(rows) && any(rows < 1)) {         stop(\"Invalid row indices\")       }        if (!is.null(cols) && any(cols < 1)) {         stop(\"Invalid column indices\")       }        # Return data       matrix(rnorm(5000), 50, 100)[rows %||% 1:50, cols %||% 1:100, drop = FALSE]     },     error = function(e) {       backend <- log_error(backend, \"get_data\", e)       stop(\"Data access failed: \", conditionMessage(e))     }   ) }  # Implement other methods... backend_get_dims.robust_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")   list(spatial = c(100, 1, 1), time = 50) }  backend_get_mask.robust_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")   rep(TRUE, 100) }  backend_close.robust_backend <- function(backend) {   if (length(backend$error_log) > 0) {     cat(\"\\nError log:\\n\")     for (error in backend$error_log) {       cat(         \"  -\", error$operation, \"at\", format(error$timestamp),         \":\", error$message, \"\\n\"       )     }   }   backend$is_open <- FALSE   invisible(backend) }  # Test error handling set.seed(123) backend <- robust_backend(\"source\")  # May fail randomly result <- tryCatch(   {     backend <- backend_open(backend)     data <- backend_get_data(backend, rows = 1:10)     cat(\"Data retrieved successfully\\n\")     backend_close(backend)   },   error = function(e) {     cat(\"Caught error:\", conditionMessage(e), \"\\n\")   } ) #> Successfully opened backend #> Data retrieved successfully"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"tips-and-best-practices","dir":"Articles","previous_headings":"","what":"Tips and Best Practices","title":"Backend Development Basics: Creating Custom Storage Backends","text":"essential guidelines creating robust, efficient backends.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"lazy-loading-requirements","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Lazy Loading Requirements","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Implementation Pattern: - Load metadata dimensions backend_open() - Defer data loading backend_get_data() invocation - Cache frequently accessed metadata - Implement partial loading subset requests","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"idempotency-requirements","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Idempotency Requirements","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Method Behavior Specifications: - backend_open() open backend: Return without modification - backend_close() closed backend: Return silently - query methods: Return consistent results repeated calls - State modifications: Implement checks prevent duplicate operations","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"caching-strategy","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Caching Strategy","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Recommended Cache Targets: - Spatial masks (computed , used frequently) - Dimension calculations - Metadata extractions - Subset indices repeated queries Implement cache invalidation underlying data changes.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"backend-development-checklist","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Backend Development Checklist","title":"Backend Development Basics: Creating Custom Storage Backends","text":"considering backend complete:","code":"backend_checklist <- function() {   cat(\"Backend Development Checklist:\\n\\n\")    cat(\"Required Functionality:\\n\")   cat(\"  ☐ Implements all 5 required methods\\n\")   cat(\"  ☐ Returns correct data orientations\\n\")   cat(\"  ☐ Handles NULL rows/cols in get_data\\n\")   cat(\"  ☐ Returns valid dimension structure\\n\")   cat(\"  ☐ Mask length matches spatial dimensions\\n\\n\")    cat(\"Robustness:\\n\")   cat(\"  ☐ Validates inputs in constructor\\n\")   cat(\"  ☐ Checks is_open state in all methods\\n\")   cat(\"  ☐ Handles errors gracefully\\n\")   cat(\"  ☐ Cleans up resources in close\\n\")   cat(\"  ☐ Methods are idempotent\\n\\n\")    cat(\"Performance:\\n\")   cat(\"  ☐ Implements lazy loading\\n\")   cat(\"  ☐ Caches frequently accessed values\\n\")   cat(\"  ☐ Minimizes memory footprint\\n\")   cat(\"  ☐ Supports partial data loading\\n\\n\")    cat(\"Documentation:\\n\")   cat(\"  ☐ Constructor documented\\n\")   cat(\"  ☐ Error messages are informative\\n\")   cat(\"  ☐ Usage examples provided\\n\")   cat(\"  ☐ Performance characteristics noted\\n\") }  backend_checklist() #> Backend Development Checklist: #>  #> Required Functionality: #>   ☐ Implements all 5 required methods #>   ☐ Returns correct data orientations #>   ☐ Handles NULL rows/cols in get_data #>   ☐ Returns valid dimension structure #>   ☐ Mask length matches spatial dimensions #>  #> Robustness: #>   ☐ Validates inputs in constructor #>   ☐ Checks is_open state in all methods #>   ☐ Handles errors gracefully #>   ☐ Cleans up resources in close #>   ☐ Methods are idempotent #>  #> Performance: #>   ☐ Implements lazy loading #>   ☐ Caches frequently accessed values #>   ☐ Minimizes memory footprint #>   ☐ Supports partial data loading #>  #> Documentation: #>   ☐ Constructor documented #>   ☐ Error messages are informative #>   ☐ Usage examples provided #>   ☐ Performance characteristics noted"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"testing-your-backend","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Testing Your Backend","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Comprehensive testing ensures reliability:","code":"# Test suite for backends test_backend <- function(backend_constructor, test_source) {   cat(\"Testing backend implementation...\\n\\n\")    # Test construction   cat(\"Testing construction...\")   backend <- backend_constructor(test_source)   cat(\" ✓\\n\")    # Test opening   cat(\"Testing open...\")   backend <- backend_open(backend)   cat(\" ✓\\n\")    # Test dimensions   cat(\"Testing dimensions...\")   dims <- backend_get_dims(backend)   stopifnot(is.list(dims))   stopifnot(all(c(\"spatial\", \"time\") %in% names(dims)))   stopifnot(length(dims$spatial) == 3)   cat(\" ✓\\n\")    # Test mask   cat(\"Testing mask...\")   mask <- backend_get_mask(backend)   stopifnot(is.logical(mask))   stopifnot(length(mask) == prod(dims$spatial))   cat(\" ✓\\n\")    # Test data access   cat(\"Testing data access...\")   data <- backend_get_data(backend)   stopifnot(is.matrix(data))   stopifnot(nrow(data) == dims$time)   cat(\" ✓\\n\")    # Test subsetting   cat(\"Testing subsetting...\")   subset_data <- backend_get_data(backend, rows = 1:10, cols = 1:20)   stopifnot(dim(subset_data)[1] == 10)   stopifnot(dim(subset_data)[2] == 20)   cat(\" ✓\\n\")    # Test closing   cat(\"Testing close...\")   backend_close(backend)   cat(\" ✓\\n\")    cat(\"\\nAll tests passed.\\n\") }  # Test our JSON backend test_backend(json_backend, json_file) #> Testing backend implementation... #>  #> Testing construction... ✓ #> Testing open... ✓ #> Testing dimensions... ✓ #> Testing mask... ✓ #> Testing data access... ✓ #> Testing subsetting... ✓ #> Testing close... ✓ #>  #> All tests passed."},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"troubleshooting","dir":"Articles","previous_headings":"","what":"Troubleshooting","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Common issues developing backends solutions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"dimension-mismatches","dir":"Articles","previous_headings":"Troubleshooting","what":"Dimension Mismatches","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Problem: “Error: Mask length match spatial dimensions” Solution: Ensure length(backend_get_mask(backend)) equals prod(backend_get_dims(backend)$spatial)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"memory-issues","dir":"Articles","previous_headings":"Troubleshooting","what":"Memory Issues","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Problem: Large datasets cause memory errors Solution: Implement lazy loading support partial data access row/column subsetting","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"state-management","dir":"Articles","previous_headings":"Troubleshooting","what":"State Management","title":"Backend Development Basics: Creating Custom Storage Backends","text":"Problem: “Error: Backend open” methods Solution: Always check is_open flag provide informative error messages","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"integration-with-other-vignettes","dir":"Articles","previous_headings":"","what":"Integration with Other Vignettes","title":"Backend Development Basics: Creating Custom Storage Backends","text":"backend development guide connects : Prerequisites: - Getting Started - Understand backends fit ecosystem - Architecture Overview - Learn design principles Next Steps: - Backend Registry - Register backend automatic selection - Advanced Backend Patterns - Sophisticated techniques production backends Applications: - H5 Backend Usage - See production backend action","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-development-basics.html","id":"session-information","dir":"Articles","previous_headings":"","what":"Session Information","title":"Backend Development Basics: Creating Custom Storage Backends","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] fmridataset_0.8.9 #>  #> loaded via a namespace (and not attached): #>  [1] gtable_0.3.6          xfun_0.56             bslib_0.9.0           #>  [4] ggplot2_4.0.1         lattice_0.22-7        bigassertr_0.1.7      #>  [7] numDeriv_2016.8-1.1   vctrs_0.7.0           tools_4.5.2           #> [10] generics_0.1.4        stats4_4.5.2          parallel_4.5.2        #> [13] tibble_3.3.1          pkgconfig_2.0.3       Matrix_1.7-4          #> [16] RColorBrewer_1.1-3    bigstatsr_1.6.2       S4Vectors_0.48.0      #> [19] S7_0.2.1              desc_1.4.3            RcppParallel_5.1.11-1 #> [22] assertthat_0.2.1      lifecycle_1.0.5       compiler_4.5.2        #> [25] neuroim2_0.8.3        farver_2.1.2          stringr_1.6.0         #> [28] textshaping_1.0.4     RNifti_1.9.0          bigparallelr_0.3.2    #> [31] codetools_0.2-20      htmltools_0.5.9       sass_0.4.10           #> [34] yaml_2.3.12           deflist_0.2.0         pillar_1.11.1         #> [37] pkgdown_2.2.0         crayon_1.5.3          jquerylib_0.1.4       #> [40] RNiftyReg_2.8.4       cachem_1.1.0          DelayedArray_0.36.0   #> [43] dbscan_1.2.4          iterators_1.0.14      abind_1.4-8           #> [46] foreach_1.5.2         tidyselect_1.2.1      digest_0.6.39         #> [49] stringi_1.8.7         dplyr_1.1.4           purrr_1.2.1           #> [52] splines_4.5.2         cowplot_1.2.0         fastmap_1.2.0         #> [55] grid_4.5.2            mmap_0.6-23           SparseArray_1.10.8    #> [58] cli_3.6.5             magrittr_2.0.4        S4Arrays_1.10.1       #> [61] fmrihrf_0.1.0.9000    scales_1.4.0          XVector_0.50.0        #> [64] rmarkdown_2.30        matrixStats_1.5.0     rmio_0.4.0            #> [67] ragg_1.5.0            memoise_2.0.1         evaluate_1.0.5        #> [70] knitr_1.51            IRanges_2.44.0        doParallel_1.0.17     #> [73] rlang_1.1.7           Rcpp_1.1.1            glue_1.8.0            #> [76] BiocGenerics_0.56.0   jsonlite_2.0.0        R6_2.6.1              #> [79] MatrixGenerics_1.22.0 systemfonts_1.3.1     fs_1.6.6              #> [82] flock_0.7"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"motivation-the-format-fragmentation-problem","dir":"Articles","previous_headings":"","what":"Motivation: The Format Fragmentation Problem","title":"Backend Registry: Extending Data Format Support","text":"Imagine ’re collaborating multi-site neuroimaging study lab uses different tools data formats. Site stores preprocessed data custom HDF5 files specialized metadata, Site B uses BIDS-organized NIfTI files, Site C provides CSV matrices exported MATLAB, Site D data proprietary format analysis software. Traditional approaches require write completely separate loading processing code format, manually handle different metadata conventions, constantly switch different programming interfaces. fmridataset backend registry system eliminates complexity providing pluggable architecture new data formats can seamlessly integrated existing ecosystem. backend registered, data format immediately works existing analysis code, chunking systems, study-level operations. approach transforms format fragmentation problem major barrier simple extension task, enabling true format independence neuroimaging research.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"a-real-example-creating-and-using-custom-backends","dir":"Articles","previous_headings":"","what":"A Real Example: Creating and Using Custom Backends","title":"Backend Registry: Extending Data Format Support","text":"Let’s dive concrete example shows create custom backend integrate seamlessly fmridataset ecosystem. ’ll create backend CSV files demonstrates key concepts: Now let’s see backend action: Technical Note: registration, custom backends integrate fmridataset functionality. Analysis code written one backend works identically newly registered backends without modification.","code":"library(fmridataset)  # Step 1: Create a custom CSV backend csv_backend <- function(csv_file, mask_file = NULL, ...) {   # Validate inputs   if (!file.exists(csv_file)) {     stop(\"CSV file does not exist: \", csv_file)   }    # Read and validate data (simulate for vignette)   # In practice: data_matrix <- read.csv(csv_file, header = FALSE)   # For demo, create synthetic data   set.seed(123)   data_matrix <- matrix(rnorm(1000), nrow = 100, ncol = 10)    # Handle mask   if (is.null(mask_file)) {     # Default mask: all columns are valid     mask <- rep(TRUE, ncol(data_matrix))   } else {     # mask <- as.logical(read.csv(mask_file, header = FALSE)[[1]])     mask <- rep(TRUE, ncol(data_matrix)) # Simplified for demo   }    # Create backend object   backend <- list(     csv_file = csv_file,     mask_file = mask_file,     data_matrix = data_matrix,     mask = mask,     spatial_dims = c(ncol(data_matrix), 1, 1), # Flat 3D space     is_open = FALSE   )    class(backend) <- c(\"csv_backend\", \"storage_backend\")   backend }  # Step 2: Implement required S3 methods backend_open.csv_backend <- function(backend) {   # CSV backend stores data in memory, so opening just marks state   backend$is_open <- TRUE   backend }  backend_close.csv_backend <- function(backend) {   backend$is_open <- FALSE   invisible(NULL) }  backend_get_dims.csv_backend <- function(backend) {   list(     spatial = backend$spatial_dims,     time = nrow(backend$data_matrix)   ) }  backend_get_mask.csv_backend <- function(backend) {   backend$mask }  backend_get_data.csv_backend <- function(backend, rows = NULL, cols = NULL) {   # Apply mask first   data <- backend$data_matrix[, backend$mask, drop = FALSE]    # Apply row subsetting   if (!is.null(rows)) {     data <- data[rows, , drop = FALSE]   }    # Apply column subsetting (after masking)   if (!is.null(cols)) {     data <- data[, cols, drop = FALSE]   }    data }  backend_get_metadata.csv_backend <- function(backend) {   list(     format = \"CSV\",     source_file = backend$csv_file,     mask_file = backend$mask_file,     data_loaded = backend$is_open   ) }  # Step 3: Register the backend register_backend(   name = \"csv\",   factory = csv_backend,   description = \"CSV file backend for simple text-based fMRI data\" )  cat(\"CSV backend registered successfully\\n\") # Step 4: Use the registered backend # Create a backend instance (would use real file path) csv_backend_instance <- create_backend(\"csv\", csv_file = \"example_data.csv\")  # Open the backend csv_backend_instance <- backend_open(csv_backend_instance)  # Query metadata without loading data dims <- backend_get_dims(csv_backend_instance) cat(\"Data dimensions:\", dims$time, \"timepoints ×\", sum(dims$spatial), \"voxels\\n\")  metadata <- backend_get_metadata(csv_backend_instance) cat(\"Data format:\", metadata$format, \"\\n\")  # Use in dataset creation dataset <- fmri_dataset(   scans = csv_backend_instance,   TR = 2.0,   run_length = 100 )  cat(\"Created dataset using CSV backend\\n\") print(dataset)  # All standard operations work data_matrix <- get_data_matrix(dataset) cat(\"Retrieved data matrix with dimensions:\", dim(data_matrix), \"\\n\")  # Chunking works automatically chunks <- data_chunks(dataset, nchunks = 3) cat(\"Created\", length(chunks), \"chunks for processing\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"understanding-the-backend-registry-system","dir":"Articles","previous_headings":"","what":"Understanding the Backend Registry System","title":"Backend Registry: Extending Data Format Support","text":"backend registry system foundation fmridataset’s extensibility. provides clean separation data storage formats analysis operations, enabling unlimited format support maintaining consistent user interface.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"the-backend-contract","dir":"Articles","previous_headings":"Understanding the Backend Registry System","what":"The Backend Contract","title":"Backend Registry: Extending Data Format Support","text":"Every backend must implement standardized contract consisting six core methods. contract ensures backends behave consistently can used interchangeably rest system. contract defines just methods must exist, also expected behavior error handling patterns. backend contract designed support eager lazy loading strategies. backends (like matrix backends) can provide immediate data access, others (like file backends) can defer loading absolutely necessary. flexibility enables optimal performance characteristics different data sources maintaining programming interface. Understanding backend contract crucial creating reliable extensions. method specific responsibilities expected return formats must followed exactly. contract also defines error conditions communicated rest system.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"registry-architecture","dir":"Articles","previous_headings":"Understanding the Backend Registry System","what":"Registry Architecture","title":"Backend Registry: Extending Data Format Support","text":"registry sophisticated system manages backend discovery, validation, instantiation. call register_backend(), system performs validation ensure backend meets contract requirements. also handles method dispatch, ensuring correct backend implementation called operation. registry supports runtime registration, meaning backends can added external packages without modifying core fmridataset code. enables vibrant ecosystem specialized packages can provide backends niche formats leveraging existing analysis infrastructure. registry also provides introspection capabilities, allowing users developers discover available backends, query capabilities, understand specific requirements. transparency makes system approachable helps debugging things go wrong.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"validation-and-error-handling","dir":"Articles","previous_headings":"Understanding the Backend Registry System","what":"Validation and Error Handling","title":"Backend Registry: Extending Data Format Support","text":"registry system includes comprehensive validation catch common errors backend development. backend registered, system checks required methods implemented follow expected patterns. validation helps developers catch issues early rather encountering mysterious errors analysis. validation system also includes runtime checks ensure backends continue behave correctly actual use. checks help identify issues like resource leaks, inconsistent data formats, unexpected error conditions might caught initial development. Error handling registry system designed informative actionable. something goes wrong, system provides detailed error messages help identify went wrong fix . approach reduces debugging time required developing custom backends.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"deep-dive-creating-robust-backends","dir":"Articles","previous_headings":"","what":"Deep Dive: Creating Robust Backends","title":"Backend Registry: Extending Data Format Support","text":"basic concepts established, let’s explore create production-quality backends handle real-world complexities edge cases.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"complete-backend-implementation","dir":"Articles","previous_headings":"Deep Dive: Creating Robust Backends","what":"Complete Backend Implementation","title":"Backend Registry: Extending Data Format Support","text":"robust backend implementation goes beyond basic contract handle edge cases, provide good error messages, optimize performance: enhanced backend demonstrates proper error handling, input validation, resource management.","code":"# Advanced backend with comprehensive features advanced_csv_backend <- function(csv_file, mask_file = NULL,                                  delimiter = \",\", has_header = FALSE, ...) {   # Input validation   if (!is.character(csv_file) || length(csv_file) != 1) {     stop(\"csv_file must be a single character string\")   }    if (!file.exists(csv_file)) {     stop(\"CSV file does not exist: \", csv_file)   }    # Check file size for memory planning   file_info <- file.info(csv_file)   if (file_info$size > 1e9) { # 1GB     warning(       \"Large CSV file detected (\",       round(file_info$size / 1e6, 1), \"MB). Consider chunked loading.\"     )   }    # Validate delimiter   if (!delimiter %in% c(\",\", \";\", \"\\t\", \"|\")) {     warning(\"Unusual delimiter '\", delimiter, \"' - ensure it's correct\")   }    # Create backend object with metadata   backend <- list(     csv_file = csv_file,     mask_file = mask_file,     delimiter = delimiter,     has_header = has_header,     file_size = file_info$size,     file_modified = file_info$mtime,     data_cache = NULL,     mask_cache = NULL,     spatial_dims = NULL,     is_open = FALSE,     read_count = 0   )    class(backend) <- c(\"advanced_csv_backend\", \"storage_backend\")   backend }  # Enhanced backend methods with error handling backend_open.advanced_csv_backend <- function(backend) {   if (backend$is_open) {     return(backend) # Already open   }    tryCatch(     {       # Read data with proper error handling       # data <- read.csv(backend$csv_file,       #                  header = backend$has_header,       #                  sep = backend$delimiter)        # Simulate data loading for vignette       set.seed(123)       data <- matrix(rnorm(2000), nrow = 200, ncol = 10)        # Validate data format       if (!is.numeric(data)) {         stop(\"CSV data must be numeric for fMRI analysis\")       }        if (any(is.na(data))) {         na_prop <- mean(is.na(data))         if (na_prop > 0.1) {           stop(             \"Too many missing values in CSV data (\",             round(na_prop * 100, 1), \"%)\"           )         }         warning(\"CSV contains \", sum(is.na(data)), \" missing values\")       }        # Handle mask       if (!is.null(backend$mask_file)) {         if (!file.exists(backend$mask_file)) {           stop(\"Mask file not found: \", backend$mask_file)         }         # mask <- read.csv(backend$mask_file, header = FALSE)[[1]]         mask <- rep(TRUE, ncol(data)) # Simplified for demo       } else {         mask <- rep(TRUE, ncol(data))       }        # Validate mask       if (length(mask) != ncol(data)) {         stop(           \"Mask length (\", length(mask),           \") does not match data columns (\", ncol(data), \")\"         )       }        # Cache data and metadata       backend$data_cache <- data       backend$mask_cache <- as.logical(mask)       backend$spatial_dims <- c(sum(backend$mask_cache), 1, 1)       backend$is_open <- TRUE       backend$read_count <- backend$read_count + 1        cat(         \"Opened CSV backend: \", nrow(data), \" timepoints, \",         sum(backend$mask_cache), \" voxels\\n\"       )        return(backend)     },     error = function(e) {       stop(\"Failed to open CSV backend: \", conditionMessage(e))     }   ) }  backend_close.advanced_csv_backend <- function(backend) {   # Clear cached data to free memory   backend$data_cache <- NULL   backend$mask_cache <- NULL   backend$is_open <- FALSE    cat(\"Closed CSV backend, freed cached data\\n\")   invisible(NULL) }  backend_get_dims.advanced_csv_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Backend must be opened before querying dimensions\")   }    list(     spatial = backend$spatial_dims,     time = nrow(backend$data_cache)   ) }  backend_get_mask.advanced_csv_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Backend must be opened before accessing mask\")   }    backend$mask_cache }  backend_get_data.advanced_csv_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) {     stop(\"Backend must be opened before accessing data\")   }    # Apply mask first   data <- backend$data_cache[, backend$mask_cache, drop = FALSE]    # Apply subsetting with validation   if (!is.null(rows)) {     if (any(rows < 1 | rows > nrow(data))) {       stop(         \"Row indices out of range: \",         paste(range(rows), collapse = \"-\"),         \" (data has \", nrow(data), \" rows)\"       )     }     data <- data[rows, , drop = FALSE]   }    if (!is.null(cols)) {     if (any(cols < 1 | cols > ncol(data))) {       stop(         \"Column indices out of range: \",         paste(range(cols), collapse = \"-\"),         \" (masked data has \", ncol(data), \" columns)\"       )     }     data <- data[, cols, drop = FALSE]   }    return(data) }  backend_get_metadata.advanced_csv_backend <- function(backend) {   list(     format = \"Advanced CSV\",     source_file = backend$csv_file,     mask_file = backend$mask_file,     delimiter = backend$delimiter,     file_size_mb = round(backend$file_size / 1e6, 2),     file_modified = backend$file_modified,     is_open = backend$is_open,     read_count = backend$read_count,     has_cached_data = !is.null(backend$data_cache)   ) }  # Register the enhanced backend register_backend(   name = \"advanced_csv\",   factory = advanced_csv_backend,   description = \"Enhanced CSV backend with comprehensive error handling and validation\" )  cat(\"Advanced CSV backend registered\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"backend-validation-and-testing","dir":"Articles","previous_headings":"Deep Dive: Creating Robust Backends","what":"Backend Validation and Testing","title":"Backend Registry: Extending Data Format Support","text":"crucial aspect backend development comprehensive testing ensure reliability: Systematic testing ensures backends work correctly across different scenarios.","code":"# Comprehensive backend testing framework test_backend_contract <- function(backend_name, test_params) {   cat(\"Testing backend contract for:\", backend_name, \"\\n\")    # Test 1: Backend creation   tryCatch(     {       backend <- do.call(create_backend, c(list(backend_name), test_params))       cat(\"✓ Backend creation successful\\n\")     },     error = function(e) {       cat(\"✗ Backend creation failed:\", conditionMessage(e), \"\\n\")       return(FALSE)     }   )    # Test 2: Backend opening   tryCatch(     {       backend <- backend_open(backend)       cat(\"✓ Backend opening successful\\n\")     },     error = function(e) {       cat(\"✗ Backend opening failed:\", conditionMessage(e), \"\\n\")       return(FALSE)     }   )    # Test 3: Dimension queries   tryCatch(     {       dims <- backend_get_dims(backend)       if (!is.list(dims) || !all(c(\"spatial\", \"time\") %in% names(dims))) {         stop(\"Invalid dimension format\")       }       cat(\"✓ Dimension queries successful\\n\")     },     error = function(e) {       cat(\"✗ Dimension queries failed:\", conditionMessage(e), \"\\n\")       return(FALSE)     }   )    # Test 4: Mask access   tryCatch(     {       mask <- backend_get_mask(backend)       if (!is.logical(mask)) {         stop(\"Mask must be logical vector\")       }       cat(\"✓ Mask access successful\\n\")     },     error = function(e) {       cat(\"✗ Mask access failed:\", conditionMessage(e), \"\\n\")       return(FALSE)     }   )    # Test 5: Data access   tryCatch(     {       full_data <- backend_get_data(backend)       if (!is.matrix(full_data) || !is.numeric(full_data)) {         stop(\"Data must be numeric matrix\")       }        # Test partial data access       subset_data <- backend_get_data(backend, rows = 1:5, cols = 1:3)       if (nrow(subset_data) != 5 || ncol(subset_data) != 3) {         stop(\"Subsetting failed\")       }        cat(\"✓ Data access successful\\n\")     },     error = function(e) {       cat(\"✗ Data access failed:\", conditionMessage(e), \"\\n\")       return(FALSE)     }   )    # Test 6: Metadata   tryCatch(     {       metadata <- backend_get_metadata(backend)       if (!is.list(metadata)) {         stop(\"Metadata must be a list\")       }       cat(\"✓ Metadata access successful\\n\")     },     error = function(e) {       cat(\"✗ Metadata access failed:\", conditionMessage(e), \"\\n\")       return(FALSE)     }   )    # Test 7: Backend closing   tryCatch(     {       backend_close(backend)       cat(\"✓ Backend closing successful\\n\")     },     error = function(e) {       cat(\"✗ Backend closing failed:\", conditionMessage(e), \"\\n\")       return(FALSE)     }   )    cat(\"All backend contract tests passed!\\n\")   return(TRUE) }  # Test our advanced CSV backend test_params <- list(csv_file = \"example_data.csv\") # test_result <- test_backend_contract(\"advanced_csv\", test_params)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"performance-optimization","dir":"Articles","previous_headings":"Deep Dive: Creating Robust Backends","what":"Performance Optimization","title":"Backend Registry: Extending Data Format Support","text":"Backend performance can significantly impact analysis speed, especially large datasets: Performance monitoring helps identify bottlenecks optimization opportunities.","code":"# Performance-optimized backend strategies demonstrate_performance_patterns <- function() {   cat(\"Backend performance optimization patterns:\\n\\n\")    cat(\"1. Lazy loading:\\n\")   cat(\"   - Defer data loading until absolutely necessary\\n\")   cat(\"   - Cache metadata for quick queries\\n\")   cat(\"   - Implement partial loading for subsetting\\n\\n\")    cat(\"2. Memory management:\\n\")   cat(\"   - Clear caches when backend is closed\\n\")   cat(\"   - Monitor memory usage during operations\\n\")   cat(\"   - Implement intelligent caching strategies\\n\\n\")    cat(\"3. I/O optimization:\\n\")   cat(\"   - Minimize file system operations\\n\")   cat(\"   - Use memory-mapped files for large datasets\\n\")   cat(\"   - Implement efficient partial reading\\n\\n\")    cat(\"4. Error handling:\\n\")   cat(\"   - Fail fast with informative messages\\n\")   cat(\"   - Validate inputs before expensive operations\\n\")   cat(\"   - Provide recovery suggestions\\n\") }  demonstrate_performance_patterns()  # Example of performance monitoring monitor_backend_performance <- function(backend, operations = 100) {   if (requireNamespace(\"microbenchmark\", quietly = TRUE)) {     cat(\"Benchmarking backend operations:\\n\")      # Benchmark dimension queries     dim_benchmark <- microbenchmark::microbenchmark(       dims = backend_get_dims(backend),       times = operations     )      cat(       \"Dimension queries: \",       round(mean(dim_benchmark$time) / 1e6, 2), \"ms average\\n\"     )      # Benchmark data access     data_benchmark <- microbenchmark::microbenchmark(       full_data = backend_get_data(backend),       partial_data = backend_get_data(backend, rows = 1:10),       times = min(operations, 10) # Fewer iterations for data access     )      cat(       \"Full data access: \",       round(mean(data_benchmark$time[data_benchmark$expr == \"full_data\"]) / 1e6, 2),       \"ms average\\n\"     )     cat(       \"Partial data access: \",       round(mean(data_benchmark$time[data_benchmark$expr == \"partial_data\"]) / 1e6, 2),       \"ms average\\n\"     )   } }  # Example usage (commented out for vignette) # backend_instance <- create_backend(\"advanced_csv\", csv_file = \"example.csv\") # backend_instance <- backend_open(backend_instance) # monitor_backend_performance(backend_instance)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"advanced-topics","dir":"Articles","previous_headings":"","what":"Advanced Topics","title":"Backend Registry: Extending Data Format Support","text":"’re comfortable basic backend development, advanced concepts help create sophisticated, production-ready backends.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"package-integration-strategies","dir":"Articles","previous_headings":"Advanced Topics","what":"Package Integration Strategies","title":"Backend Registry: Extending Data Format Support","text":"package developers, proper integration fmridataset ecosystem requires careful planning: Proper package integration ensures backends work reliably across different environments.","code":"# Example package integration pattern demonstrate_package_integration <- function() {   cat(\"Package integration best practices:\\n\\n\")    cat(\"1. Package initialization (.onLoad):\\n\")   cat(\"   .onLoad <- function(libname, pkgname) {\\n\")   cat(\"     if (requireNamespace('fmridataset', quietly = TRUE)) {\\n\")   cat(\"       fmridataset::register_backend(\\n\")   cat(\"         name = 'myformat',\\n\")   cat(\"         factory = myformat_backend,\\n\")   cat(\"         description = 'Backend for MyFormat files'\\n\")   cat(\"       )\\n\")   cat(\"     }\\n\")   cat(\"   }\\n\\n\")    cat(\"2. Package cleanup (.onUnload):\\n\")   cat(\"   .onUnload <- function(libpath) {\\n\")   cat(\"     if (requireNamespace('fmridataset', quietly = TRUE)) {\\n\")   cat(\"       fmridataset::unregister_backend('myformat')\\n\")   cat(\"     }\\n\")   cat(\"   }\\n\\n\")    cat(\"3. Conditional functionality:\\n\")   cat(\"   - Check for fmridataset availability\\n\")   cat(\"   - Gracefully handle missing dependencies\\n\")   cat(\"   - Provide standalone functionality when possible\\n\\n\")    cat(\"4. Documentation:\\n\")   cat(\"   - Document backend-specific parameters\\n\")   cat(\"   - Provide usage examples\\n\")   cat(\"   - Explain integration with fmridataset workflows\\n\") }  demonstrate_package_integration()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"advanced-validation-systems","dir":"Articles","previous_headings":"Advanced Topics","what":"Advanced Validation Systems","title":"Backend Registry: Extending Data Format Support","text":"Sophisticated backends can provide custom validation beyond basic contract: Custom validation helps ensure data quality catch format-specific issues.","code":"# Advanced validation for specialized backends create_validation_system <- function(backend_name) {   # Custom validator function   validate_specialized_backend <- function(backend) {     validation_results <- list()      # Check 1: Data format validation     tryCatch(       {         if (backend$is_open) {           data <- backend_get_data(backend)            # Check for NaN or infinite values           if (any(is.nan(data)) || any(is.infinite(data))) {             validation_results$data_quality <- \"FAIL: Contains NaN or infinite values\"           } else {             validation_results$data_quality <- \"PASS: Data values are valid\"           }            # Check data range           data_range <- range(data, na.rm = TRUE)           if (diff(data_range) == 0) {             validation_results$data_range <- \"WARN: All data values are identical\"           } else if (abs(data_range[1]) > 1000 || abs(data_range[2]) > 1000) {             validation_results$data_range <- \"WARN: Data values seem unusually large\"           } else {             validation_results$data_range <- \"PASS: Data range appears reasonable\"           }            # Check for temporal correlation structure           if (nrow(data) > 10 && ncol(data) > 1) {             temporal_corr <- cor(data[1:(nrow(data) - 1), ], data[2:nrow(data), ])             mean_temporal_corr <- mean(diag(temporal_corr), na.rm = TRUE)              if (mean_temporal_corr < 0.1) {               validation_results$temporal_structure <-                 \"WARN: Low temporal correlation (may indicate noise)\"             } else if (mean_temporal_corr > 0.95) {               validation_results$temporal_structure <-                 \"WARN: Very high temporal correlation (may indicate processing artifact)\"             } else {               validation_results$temporal_structure <-                 \"PASS: Temporal correlation structure appears normal\"             }           }         }       },       error = function(e) {         validation_results$data_access <- paste(\"ERROR:\", conditionMessage(e))       }     )      # Check 2: Metadata consistency     tryCatch(       {         metadata <- backend_get_metadata(backend)         dims <- backend_get_dims(backend)          # Validate metadata completeness         required_fields <- c(\"format\", \"source_file\")         missing_fields <- setdiff(required_fields, names(metadata))          if (length(missing_fields) > 0) {           validation_results$metadata_completeness <-             paste(\"WARN: Missing metadata fields:\", paste(missing_fields, collapse = \", \"))         } else {           validation_results$metadata_completeness <- \"PASS: All required metadata present\"         }       },       error = function(e) {         validation_results$metadata_access <- paste(\"ERROR:\", conditionMessage(e))       }     )      return(validation_results)   }    # Register custom validator   cat(\"Custom validation system created for:\", backend_name, \"\\n\")   cat(\"Validation checks:\\n\")   cat(\"  - Data quality (NaN, infinite values)\\n\")   cat(\"  - Data range reasonableness\\n\")   cat(\"  - Temporal correlation structure\\n\")   cat(\"  - Metadata completeness\\n\")    return(validate_specialized_backend) }  # Example usage csv_validator <- create_validation_system(\"advanced_csv\")  # Apply validation to a backend validate_backend_instance <- function(backend) {   cat(\"Running custom validation...\\n\")    validation_results <- csv_validator(backend)    for (check_name in names(validation_results)) {     result <- validation_results[[check_name]]     status <- substr(result, 1, 4)      if (status == \"PASS\") {       cat(\"✓\", check_name, \":\", result, \"\\n\")     } else if (status == \"WARN\") {       cat(\"⚠\", check_name, \":\", result, \"\\n\")     } else {       cat(\"✗\", check_name, \":\", result, \"\\n\")     }   } }  # validate_backend_instance(backend_instance)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"backend-composition-and-chaining","dir":"Articles","previous_headings":"Advanced Topics","what":"Backend Composition and Chaining","title":"Backend Registry: Extending Data Format Support","text":"Advanced scenarios may require composing multiple backends creating backend chains: Composite backends enable sophisticated data integration scenarios.","code":"# Example: Composite backend that combines multiple sources create_composite_backend <- function(backend_list, combination_strategy = \"concatenate\") {   composite_backend <- function(...) {     # Validate all component backends     for (i in seq_along(backend_list)) {       if (!inherits(backend_list[[i]], \"storage_backend\")) {         stop(\"Component \", i, \" is not a valid backend\")       }     }      backend <- list(       component_backends = backend_list,       combination_strategy = combination_strategy,       is_open = FALSE,       combined_dims = NULL,       combined_mask = NULL     )      class(backend) <- c(\"composite_backend\", \"storage_backend\")     backend   }    return(composite_backend) }  # Implement composite backend methods backend_open.composite_backend <- function(backend) {   # Open all component backends   for (i in seq_along(backend$component_backends)) {     backend$component_backends[[i]] <- backend_open(backend$component_backends[[i]])   }    # Compute combined dimensions and masks   backend$is_open <- TRUE    cat(\"Opened composite backend with\", length(backend$component_backends), \"components\\n\")   backend }  backend_get_dims.composite_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Composite backend must be opened first\")   }    # Combine dimensions based on strategy   component_dims <- lapply(backend$component_backends, backend_get_dims)    if (backend$combination_strategy == \"concatenate\") {     # Concatenate timepoints, ensure spatial dimensions match     total_time <- sum(sapply(component_dims, function(x) x$time))     spatial_dims <- component_dims[[1]]$spatial      # Validate spatial consistency     for (dims in component_dims[-1]) {       if (!identical(dims$spatial, spatial_dims)) {         stop(\"Spatial dimensions must match for concatenation\")       }     }      return(list(spatial = spatial_dims, time = total_time))   }    # Add other combination strategies as needed   stop(\"Unsupported combination strategy: \", backend$combination_strategy) }  # Additional composite backend methods would follow similar patterns...  cat(\"Composite backend framework created\\n\") cat(\"Supports combination strategies for multiple data sources\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"tips-and-best-practices","dir":"Articles","previous_headings":"","what":"Tips and Best Practices","title":"Backend Registry: Extending Data Format Support","text":"practical guidelines learned developing maintaining production backends help create robust, reliable extensions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"performance-requirements","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Performance Requirements","title":"Backend Registry: Extending Data Format Support","text":"Lazy Loading Implementation: Backends must defer data loading explicitly requested backend_get_data(). Loading data backend creation causes unnecessary memory allocation performance degradation, particularly large datasets.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"error-handling-standards","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Error Handling Standards","title":"Backend Registry: Extending Data Format Support","text":"Input Validation: Implement comprehensive validation backend creation descriptive error messages include: - specific validation failed - expected format range - Suggestions correction","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"registry-introspection","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Registry Introspection","title":"Backend Registry: Extending Data Format Support","text":"Capability Detection: Query registry backend availability invoking format-specific features: pattern ensures code portability across environments different backend configurations.","code":"if (backend_registry$has_backend(\"csv\")) {   # CSV-specific operations }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"development-workflow","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Development Workflow","title":"Backend Registry: Extending Data Format Support","text":"Effective backend development follows systematic workflow:","code":"backend_development_checklist <- function() {   cat(\"Backend development checklist:\\n\\n\")    cat(\"1. Planning phase:\\n\")   cat(\"   □ Understand the data format thoroughly\\n\")   cat(\"   □ Identify performance requirements\\n\")   cat(\"   □ Plan for edge cases and error conditions\\n\")   cat(\"   □ Design the user interface (constructor parameters)\\n\\n\")    cat(\"2. Implementation phase:\\n\")   cat(\"   □ Create constructor function with input validation\\n\")   cat(\"   □ Implement all six required contract methods\\n\")   cat(\"   □ Add comprehensive error handling\\n\")   cat(\"   □ Implement resource management (open/close)\\n\\n\")    cat(\"3. Testing phase:\\n\")   cat(\"   □ Test with valid inputs\\n\")   cat(\"   □ Test with invalid inputs (error handling)\\n\")   cat(\"   □ Test edge cases (empty files, large files, etc.)\\n\")   cat(\"   □ Test performance characteristics\\n\\n\")    cat(\"4. Integration phase:\\n\")   cat(\"   □ Register backend with descriptive name\\n\")   cat(\"   □ Test integration with existing workflows\\n\")   cat(\"   □ Validate chunking behavior\\n\")   cat(\"   □ Test with study-level operations\\n\\n\")    cat(\"5. Documentation phase:\\n\")   cat(\"   □ Document constructor parameters\\n\")   cat(\"   □ Provide usage examples\\n\")   cat(\"   □ Document known limitations\\n\")   cat(\"   □ Create troubleshooting guide\\n\") }  backend_development_checklist()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"error-handling-strategies","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Error Handling Strategies","title":"Backend Registry: Extending Data Format Support","text":"Robust error handling crucial production backends:","code":"demonstrate_error_handling <- function() {   cat(\"Error handling best practices:\\n\\n\")    cat(\"1. Fail fast principle:\\n\")   cat(\"   - Validate inputs immediately\\n\")   cat(\"   - Check file existence before attempting operations\\n\")   cat(\"   - Verify data format early in the process\\n\\n\")    cat(\"2. Informative error messages:\\n\")   cat(\"   - Explain what went wrong\\n\")   cat(\"   - Suggest corrective actions\\n\")   cat(\"   - Include relevant context (file paths, data dimensions)\\n\\n\")    cat(\"3. Graceful degradation:\\n\")   cat(\"   - Handle partial failures when possible\\n\")   cat(\"   - Provide warnings for non-critical issues\\n\")   cat(\"   - Clean up resources on failure\\n\\n\")    cat(\"4. Consistent error types:\\n\")   cat(\"   - Use appropriate error classes\\n\")   cat(\"   - Follow R error handling conventions\\n\")   cat(\"   - Provide machine-readable error information\\n\") }  demonstrate_error_handling()  # Example of good error handling robust_error_handling_example <- function(file_path) {   # Good: Check file existence with helpful message   if (!file.exists(file_path)) {     stop(       \"File not found: '\", file_path,       \"'. Please check the path and ensure the file exists.\"     )   }    # Good: Check file permissions   if (!file.access(file_path, mode = 4) == 0) {     stop(       \"Cannot read file: '\", file_path,       \"'. Please check file permissions.\"     )   }    # Good: Validate file format before processing   file_ext <- tools::file_ext(file_path)   if (!file_ext %in% c(\"csv\", \"txt\")) {     stop(       \"Unsupported file format: '.\", file_ext,       \"'. Expected .csv or .txt files.\"     )   }    cat(\"File validation passed for:\", file_path, \"\\n\") }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"testing-and-validation","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Testing and Validation","title":"Backend Registry: Extending Data Format Support","text":"Comprehensive testing ensures backend reliability:","code":"create_backend_test_suite <- function(backend_name) {   cat(\"Creating test suite for:\", backend_name, \"\\n\\n\")    test_scenarios <- list(     \"normal_operation\" = list(       description = \"Test normal backend operations\",       test_data = \"valid_file.csv\",       expected_result = \"success\"     ),     \"missing_file\" = list(       description = \"Test handling of missing files\",       test_data = \"nonexistent_file.csv\",       expected_result = \"error\"     ),     \"corrupted_data\" = list(       description = \"Test handling of corrupted data\",       test_data = \"corrupted_file.csv\",       expected_result = \"error\"     ),     \"large_file\" = list(       description = \"Test performance with large files\",       test_data = \"large_file.csv\",       expected_result = \"success_with_warning\"     ),     \"edge_cases\" = list(       description = \"Test edge cases (empty files, single values)\",       test_data = \"edge_case_file.csv\",       expected_result = \"success_or_documented_limitation\"     )   )    cat(\"Test scenarios defined:\\n\")   for (scenario_name in names(test_scenarios)) {     scenario <- test_scenarios[[scenario_name]]     cat(\"  -\", scenario_name, \":\", scenario$description, \"\\n\")   }    cat(\"\\nRun these tests systematically during development\\n\")   return(test_scenarios) }  # create_test_suite <- create_backend_test_suite(\"advanced_csv\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"troubleshooting-backend-issues","dir":"Articles","previous_headings":"","what":"Troubleshooting Backend Issues","title":"Backend Registry: Extending Data Format Support","text":"developing using custom backends, certain issues common can systematically diagnosed resolved.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"common-development-issues","dir":"Articles","previous_headings":"Troubleshooting Backend Issues","what":"Common Development Issues","title":"Backend Registry: Extending Data Format Support","text":"“Error: Backend must implement method X” indicates missing required methods backend implementation. Ensure six contract methods implemented: backend_open, backend_close, backend_get_dims, backend_get_mask, backend_get_data, backend_get_metadata. “Error: Backend registration failed” registry validation detected issues backend. Check factory function returns object correct class inheritance methods properly defined. Memory leaks backend operations Ensure backend_close method properly cleans cached data releases resources. Implement explicit memory management long-running operations.","code":"# Diagnostic tools for backend development diagnose_backend_issues <- function(backend_name) {   cat(\"Diagnosing backend issues for:\", backend_name, \"\\n\\n\")    # Check if backend is registered   if (!is_backend_registered(backend_name)) {     cat(\"✗ Backend not registered\\n\")     cat(\"  Solution: Call register_backend() with your backend\\n\")     return()   } else {     cat(\"✓ Backend is registered\\n\")   }    # Check backend info   tryCatch(     {       backend_info <- get_backend_registry(backend_name)       cat(\"✓ Backend info accessible\\n\")       cat(\"  Description:\", backend_info$description, \"\\n\")     },     error = function(e) {       cat(\"✗ Cannot access backend info:\", conditionMessage(e), \"\\n\")     }   )    # Test backend creation with minimal parameters   cat(\"\\nTesting backend creation...\\n\")   # This would test actual backend creation in practice    cat(\"Diagnosis complete\\n\") }  # Example troubleshooting function troubleshoot_backend_errors <- function(error_message) {   cat(\"Troubleshooting backend error:\\n\")   cat(\"Error:\", error_message, \"\\n\\n\")    if (grepl(\"not found\", error_message, ignore.case = TRUE)) {     cat(\"Likely cause: File path issue\\n\")     cat(\"Solutions:\\n\")     cat(\"  - Check file exists with file.exists()\\n\")     cat(\"  - Use absolute paths\\n\")     cat(\"  - Verify file permissions\\n\")   } else if (grepl(\"implement.*method\", error_message, ignore.case = TRUE)) {     cat(\"Likely cause: Missing backend method\\n\")     cat(\"Solutions:\\n\")     cat(\"  - Implement all required contract methods\\n\")     cat(\"  - Check method naming (backend_open, backend_close, etc.)\\n\")     cat(\"  - Verify S3 method registration\\n\")   } else if (grepl(\"dimensions\", error_message, ignore.case = TRUE)) {     cat(\"Likely cause: Dimension mismatch\\n\")     cat(\"Solutions:\\n\")     cat(\"  - Validate data dimensions in backend_open\\n\")     cat(\"  - Check mask length matches data columns\\n\")     cat(\"  - Ensure consistent spatial dimensions\\n\")   } else {     cat(\"General debugging steps:\\n\")     cat(\"  - Check backend registration\\n\")     cat(\"  - Validate input parameters\\n\")     cat(\"  - Test with minimal example\\n\")     cat(\"  - Check error logs for more details\\n\")   } }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"performance-issues","dir":"Articles","previous_headings":"Troubleshooting Backend Issues","what":"Performance Issues","title":"Backend Registry: Extending Data Format Support","text":"Backend performance problems often stem inefficient /O memory management:","code":"# Performance diagnostic tools profile_backend_performance <- function(backend_name, test_file) {   cat(\"Profiling backend performance:\", backend_name, \"\\n\")    if (requireNamespace(\"profvis\", quietly = TRUE)) {     cat(\"Using profvis for detailed profiling\\n\")     # In practice, you would use profvis::profvis() here   }    # Basic timing measurements   backend_creation_time <- system.time({     # backend <- create_backend(backend_name, csv_file = test_file)   })    cat(\"Backend creation time:\", backend_creation_time[\"elapsed\"], \"seconds\\n\")    # Memory usage tracking   if (requireNamespace(\"pryr\", quietly = TRUE)) {     cat(\"Memory usage tracking available\\n\")     # Track memory during operations   }    cat(\"Performance profiling complete\\n\") }  # Identify common performance bottlenecks identify_performance_bottlenecks <- function() {   cat(\"Common backend performance bottlenecks:\\n\\n\")    cat(\"1. Eager data loading:\\n\")   cat(\"   - Loading all data during backend_open\\n\")   cat(\"   - Solution: Implement lazy loading\\n\\n\")    cat(\"2. Inefficient file I/O:\\n\")   cat(\"   - Reading entire files for partial access\\n\")   cat(\"   - Solution: Implement partial reading\\n\\n\")    cat(\"3. Memory leaks:\\n\")   cat(\"   - Not clearing caches on close\\n\")   cat(\"   - Solution: Explicit memory management\\n\\n\")    cat(\"4. Repeated operations:\\n\")   cat(\"   - Re-reading metadata on each access\\n\")   cat(\"   - Solution: Intelligent caching\\n\") }  identify_performance_bottlenecks()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"integration-with-other-vignettes","dir":"Articles","previous_headings":"","what":"Integration with Other Vignettes","title":"Backend Registry: Extending Data Format Support","text":"backend registry guide connects several aspects fmridataset ecosystem: Prerequisites: Understanding Architecture Overview helps appreciate backends fit overall system design. Practical Application: Getting Started guide shows different backends work user perspective. Advanced Usage: - Extending Backends - Deep dive sophisticated backend development patterns - H5 Backend Usage - Example production-quality backend implementation - Study-Level Analysis - See custom backends work multi-subject studies Development Context: backend registry system exemplifies key principles modular software design common throughout neuroimaging ecosystem. Understanding patterns help work effectively extensible packages. Package Development: ’re developing packages work neuroimaging data, backend registry pattern provides template creating extensible, interoperable systems.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/backend-registry.html","id":"session-information","dir":"Articles","previous_headings":"","what":"Session Information","title":"Backend Registry: Extending Data Format Support","text":"","code":"sessionInfo()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"motivation-beyond-basic-backends","dir":"Articles","previous_headings":"","what":"Motivation: Beyond Basic Backends","title":"Advanced Backend Development: Storage Extensions","text":"scenario requires backends supporting proprietary formats complex metadata, hierarchical organization, advanced compression. Data includes fMRI time series, physiological recordings, eye tracking data, quality metrics. Standard file-based backends handle structure, requiring streaming access, caching, integration data management systems. vignette covers advanced backend development techniques extending basic contract production storage systems. Topics include caching strategies, streaming data access, error handling, performance optimization, integration patterns complex data sources.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"quick-start-production-backend-example","dir":"Articles","previous_headings":"","what":"Quick Start: Production Backend Example","title":"Advanced Backend Development: Storage Extensions","text":"example implements NeuroStream backend demonstrating advanced techniques including streaming capabilities, metadata handling, caching: Now let’s demonstrate advanced backend action: Implementation Summary: Advanced backends implement streaming protocols, multi-tier caching, error recovery, performance monitoring maintaining interface compatibility standard backend contract.","code":"library(fmridataset)  # Step 1: Create a sophisticated NeuroStream backend neurostream_backend <- function(stream_url, cache_dir = NULL,                                 chunk_size_mb = 64, compression = \"auto\", ...) {   # Advanced input validation   if (!is.character(stream_url) || length(stream_url) != 1) {     stop(\"stream_url must be a single character string\")   }    if (!grepl(\"^(http|file|neurostream)://\", stream_url)) {     stop(\"Invalid stream URL format. Expected protocol prefix (http://, file://, or neurostream://)\")   }    # Validate cache configuration   if (!is.null(cache_dir)) {     if (!dir.exists(cache_dir)) {       tryCatch(         {           dir.create(cache_dir, recursive = TRUE)         },         error = function(e) {           stop(\"Cannot create cache directory: \", cache_dir, \" - \", conditionMessage(e))         }       )     }   }    # Validate chunk size   if (!is.numeric(chunk_size_mb) || chunk_size_mb <= 0 || chunk_size_mb > 1024) {     stop(\"chunk_size_mb must be between 1 and 1024 MB\")   }    # Initialize connection metadata   connection_id <- paste0(     \"ns_\", format(Sys.time(), \"%Y%m%d_%H%M%S\"), \"_\",     sample(1000:9999, 1)   )    # Create advanced backend object   backend <- list(     # Core configuration     stream_url = stream_url,     cache_dir = cache_dir,     chunk_size_mb = chunk_size_mb,     compression = compression,     connection_id = connection_id,      # State management     is_open = FALSE,     is_streaming = FALSE,     connection_handle = NULL,      # Data caching     metadata_cache = NULL,     spatial_cache = NULL,     temporal_cache = NULL,     data_chunks_cache = list(),      # Performance tracking     bytes_read = 0,     cache_hits = 0,     cache_misses = 0,     last_access_time = NULL,      # Advanced features     streaming_buffer = NULL,     compression_ratio = NULL,     error_recovery_attempts = 0,     max_error_recovery_attempts = 3   )    class(backend) <- c(\"neurostream_backend\", \"storage_backend\")   backend }  # Step 2: Implement sophisticated backend methods backend_open.neurostream_backend <- function(backend) {   if (backend$is_open) {     return(backend) # Already open   }    cat(\"Opening NeuroStream connection:\", backend$connection_id, \"\\n\")    # Simulate connection establishment with error recovery   attempt <- 1   while (attempt <= backend$max_error_recovery_attempts) {     tryCatch(       {         # Simulate connection process         backend$connection_handle <- list(           url = backend$stream_url,           established_at = Sys.time(),           protocol_version = \"2.1\",           server_capabilities = c(\"streaming\", \"compression\", \"metadata_queries\")         )          # Fetch and cache metadata         backend$metadata_cache <- list(           format_version = \"NeuroStream-2.1\",           spatial_dims = c(64, 64, 40),           temporal_length = 300,           acquisition_params = list(             TR = 2.0,             TE = 30,             flip_angle = 90,             voxel_size = c(3, 3, 3)           ),           quality_metrics = list(             snr_estimate = 45.2,             motion_max = 0.8,             temporal_variance = 12.3           )         )          # Initialize spatial structures         backend$spatial_cache <- list(           mask = rep(TRUE, prod(backend$metadata_cache$spatial_dims)),           roi_labels = paste0(\"region_\", 1:prod(backend$metadata_cache$spatial_dims)),           coordinates = expand.grid(             x = 1:backend$metadata_cache$spatial_dims[1],             y = 1:backend$metadata_cache$spatial_dims[2],             z = 1:backend$metadata_cache$spatial_dims[3]           )         )          # Setup temporal structures         backend$temporal_cache <- list(           timepoints = 1:backend$metadata_cache$temporal_length,           acquisition_times = (1:backend$metadata_cache$temporal_length - 1) *             backend$metadata_cache$acquisition_params$TR,           run_boundaries = c(1, 151, 301), # Example run structure           quality_flags = rep(\"good\", backend$metadata_cache$temporal_length)         )          # Initialize streaming if supported         if (\"streaming\" %in% backend$connection_handle$server_capabilities) {           backend$is_streaming <- TRUE           backend$streaming_buffer <- list(             buffer_size_mb = backend$chunk_size_mb,             current_buffer = NULL,             buffer_range = NULL           )           cat(\"Streaming mode enabled\\n\")         }          backend$is_open <- TRUE         backend$last_access_time <- Sys.time()         cat(\"NeuroStream connection established successfully\\n\")          return(backend)       },       error = function(e) {         cat(\"Connection attempt\", attempt, \"failed:\", conditionMessage(e), \"\\n\")         attempt <- attempt + 1          if (attempt <= backend$max_error_recovery_attempts) {           cat(\"Retrying in\", attempt, \"seconds...\\n\")           Sys.sleep(attempt) # Exponential backoff         } else {           stop(             \"Failed to establish NeuroStream connection after \",             backend$max_error_recovery_attempts, \" attempts: \", conditionMessage(e)           )         }       }     )   } }  backend_close.neurostream_backend <- function(backend) {   if (!backend$is_open) {     return(invisible(NULL))   }    cat(\"Closing NeuroStream connection:\", backend$connection_id, \"\\n\")    # Report performance statistics   if (backend$bytes_read > 0) {     cache_hit_rate <- backend$cache_hits / (backend$cache_hits + backend$cache_misses) * 100     cat(\"Performance summary:\\n\")     cat(\"  Bytes read:\", format(backend$bytes_read, units = \"auto\"), \"\\n\")     cat(\"  Cache hit rate:\", round(cache_hit_rate, 1), \"%\\n\")     cat(       \"  Compression ratio:\",       ifelse(is.null(backend$compression_ratio), \"N/A\",         paste0(round(backend$compression_ratio, 2), \":1\")       ), \"\\n\"     )   }    # Clear caches and release resources   backend$data_chunks_cache <- list()   backend$streaming_buffer <- NULL   backend$connection_handle <- NULL   backend$is_open <- FALSE   backend$is_streaming <- FALSE    cat(\"NeuroStream connection closed\\n\")   invisible(NULL) }  backend_get_dims.neurostream_backend <- function(backend) {   if (!backend$is_open) {     stop(\"NeuroStream backend must be opened before querying dimensions\")   }    # Use cached metadata for fast response   list(     spatial = backend$metadata_cache$spatial_dims,     time = backend$metadata_cache$temporal_length   ) }  backend_get_mask.neurostream_backend <- function(backend) {   if (!backend$is_open) {     stop(\"NeuroStream backend must be opened before accessing mask\")   }    # Return cached mask   backend$spatial_cache$mask }  backend_get_data.neurostream_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) {     stop(\"NeuroStream backend must be opened before accessing data\")   }    backend$last_access_time <- Sys.time()    # Determine data requirements   total_timepoints <- backend$metadata_cache$temporal_length   total_voxels <- sum(backend$spatial_cache$mask)    requested_rows <- if (is.null(rows)) 1:total_timepoints else rows   requested_cols <- if (is.null(cols)) 1:total_voxels else cols    # Check cache first   cache_key <- paste0(     \"data_\", min(requested_rows), \"_\", max(requested_rows),     \"_\", min(requested_cols), \"_\", max(requested_cols)   )    if (cache_key %in% names(backend$data_chunks_cache)) {     backend$cache_hits <- backend$cache_hits + 1     cat(\"Cache hit for data request\\n\")     return(backend$data_chunks_cache[[cache_key]])   }    backend$cache_misses <- backend$cache_misses + 1   cat(\"Cache miss - fetching data from stream\\n\")    # Simulate intelligent data fetching   tryCatch(     {       # For demo, create synthetic data with realistic characteristics       set.seed(42) # Reproducible for vignette        # Simulate streaming data with temporal autocorrelation       n_rows <- length(requested_rows)       n_cols <- length(requested_cols)        # Create base signal with temporal structure       base_signal <- matrix(rnorm(n_rows * n_cols), nrow = n_rows, ncol = n_cols)        # Add temporal autocorrelation       for (col in 1:n_cols) {         for (row in 2:n_rows) {           base_signal[row, col] <- 0.7 * base_signal[row - 1, col] +             0.3 * base_signal[row, col]         }       }        # Add spatial correlation structure       if (n_cols > 1) {         spatial_kernel <- exp(-as.matrix(dist(1:n_cols)) / 5)         for (row in 1:n_rows) {           base_signal[row, ] <- base_signal[row, ] %*% spatial_kernel / sum(spatial_kernel)         }       }        # Cache the result       backend$data_chunks_cache[[cache_key]] <- base_signal        # Update performance metrics       estimated_bytes <- n_rows * n_cols * 8 # 8 bytes per double       backend$bytes_read <- backend$bytes_read + estimated_bytes        # Simulate compression ratio       if (backend$compression != \"none\") {         backend$compression_ratio <- runif(1, 2.5, 4.0) # Typical fMRI compression       }        cat(\"Fetched\", n_rows, \"×\", n_cols, \"data matrix from NeuroStream\\n\")       return(base_signal)     },     error = function(e) {       backend$error_recovery_attempts <- backend$error_recovery_attempts + 1        if (backend$error_recovery_attempts <= backend$max_error_recovery_attempts) {         cat(\"Data fetch error, attempting recovery:\", conditionMessage(e), \"\\n\")         Sys.sleep(1)         return(backend_get_data(backend, rows, cols)) # Recursive retry       } else {         stop(\"Failed to fetch data after multiple attempts: \", conditionMessage(e))       }     }   ) }  backend_get_metadata.neurostream_backend <- function(backend) {   base_metadata <- list(     format = \"NeuroStream\",     stream_url = backend$stream_url,     connection_id = backend$connection_id,     is_open = backend$is_open,     is_streaming = backend$is_streaming   )    if (backend$is_open) {     # Include rich metadata when connection is active     c(base_metadata, list(       acquisition_params = backend$metadata_cache$acquisition_params,       quality_metrics = backend$metadata_cache$quality_metrics,       performance_stats = list(         bytes_read = backend$bytes_read,         cache_hits = backend$cache_hits,         cache_misses = backend$cache_misses,         cache_hit_rate = if ((backend$cache_hits + backend$cache_misses) > 0) {           backend$cache_hits / (backend$cache_hits + backend$cache_misses)         } else {           0         }       ),       server_info = backend$connection_handle[c(\"protocol_version\", \"server_capabilities\")]     ))   } else {     base_metadata   } }  # Step 3: Register the advanced backend register_backend(   name = \"neurostream\",   factory = neurostream_backend,   description = \"Advanced NeuroStream backend with streaming, caching, and error recovery\" )  cat(\"NeuroStream backend registered with advanced features\\n\") # Create and use the advanced backend ns_backend <- create_backend(\"neurostream\",   stream_url = \"neurostream://example.server.edu/study123\",   cache_dir = \"/tmp/neurostream_cache\",   chunk_size_mb = 32 )  # Open with sophisticated connection management ns_backend <- backend_open(ns_backend)  # Query rich metadata metadata <- backend_get_metadata(ns_backend) cat(\"Connected to:\", metadata$stream_url, \"\\n\") cat(\"Protocol version:\", metadata$server_info$protocol_version, \"\\n\") cat(\"Server capabilities:\", paste(metadata$server_info$server_capabilities, collapse = \", \"), \"\\n\")  # Use in dataset creation dataset <- fmri_dataset(   scans = ns_backend,   TR = metadata$acquisition_params$TR,   run_length = c(150, 150) # Two 150-timepoint runs )  cat(\"Created advanced dataset with NeuroStream backend\\n\") print(dataset)  # Demonstrate intelligent caching cat(\"First data access (cache miss):\\n\") data1 <- get_data_matrix(dataset, run_id = 1) cat(\"Data dimensions:\", dim(data1), \"\\n\")  cat(\"Second data access (cache hit):\\n\") data2 <- get_data_matrix(dataset, run_id = 1) cat(\"Data dimensions:\", dim(data2), \"\\n\")  # Show performance statistics final_metadata <- backend_get_metadata(ns_backend) cat(   \"Final cache hit rate:\",   round(final_metadata$performance_stats$cache_hit_rate * 100, 1), \"%\\n\" )  # Clean shutdown backend_close(ns_backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"understanding-advanced-backend-patterns","dir":"Articles","previous_headings":"","what":"Understanding Advanced Backend Patterns","title":"Advanced Backend Development: Storage Extensions","text":"NeuroStream backend example showcases several advanced patterns essential production-quality backend development. patterns address real-world challenges like network reliability, memory efficiency, performance optimization.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"intelligent-caching-strategies","dir":"Articles","previous_headings":"Understanding Advanced Backend Patterns","what":"Intelligent Caching Strategies","title":"Advanced Backend Development: Storage Extensions","text":"Modern neuroimaging datasets often large fit entirely memory, exhibit access patterns can exploited intelligent caching. NeuroStream backend implements sophisticated caching system tracks spatial temporal access patterns optimize performance. caching system maintains separate caches metadata, spatial information, data chunks. Metadata cached aggressively since ’s small frequently accessed. Spatial information like masks coordinates cached ’s static throughout analysis session. Data chunks cached based access patterns, recently used chunks kept memory older chunks evicted. multi-tier caching approach ensures common access patterns (like processing runs sequentially repeatedly accessing voxel subsets) achieve high cache hit rates preventing memory exhaustion. system also tracks cache performance, providing insights access patterns can inform future optimizations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"error-recovery-and-resilience","dir":"Articles","previous_headings":"Understanding Advanced Backend Patterns","what":"Error Recovery and Resilience","title":"Advanced Backend Development: Storage Extensions","text":"Network-based backends must handle connection failures, timeouts, data corruption gracefully. NeuroStream backend implements exponential backoff retry logic, connection health monitoring, graceful degradation strategies keep analyses running even network conditions poor. error recovery system distinguishes different types failures applies appropriate recovery strategies. Transient network errors trigger automatic retries exponential backoff, protocol errors authentication failures fail fast informative error messages. system also tracks error rates can switch alternative connection methods primary connections become unreliable. resilience crucial long-running analyses might span hours days. Rather failing completely network issues occur, backend attempts recovery providing progress feedback users. approach significantly improves reliability analyses involving remote cloud-based data sources.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"streaming-and-progressive-loading","dir":"Articles","previous_headings":"Understanding Advanced Backend Patterns","what":"Streaming and Progressive Loading","title":"Advanced Backend Development: Storage Extensions","text":"large datasets, traditional approaches load entire datasets memory become impractical. NeuroStream backend implements streaming protocols enable progressive data loading, currently needed data transferred cached locally. streaming system coordinates caching layer predict future data needs based current access patterns. sequential access detected, system pre-fetches upcoming data chunks. random access patterns detected, focuses caching recently accessed chunks. adaptive behavior ensures optimal performance across different analysis patterns. Streaming also enables real-time analysis scenarios data generated continuously acquisition. backend can connect live data streams provide access data becomes available, enabling real-time quality monitoring adaptive experimental paradigms.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"deep-dive-advanced-backend-features","dir":"Articles","previous_headings":"","what":"Deep Dive: Advanced Backend Features","title":"Advanced Backend Development: Storage Extensions","text":"foundational patterns established, let’s explore specific advanced features distinguish production-quality backends basic implementations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"protocol-abstraction-and-versioning","dir":"Articles","previous_headings":"Deep Dive: Advanced Backend Features","what":"Protocol Abstraction and Versioning","title":"Advanced Backend Development: Storage Extensions","text":"Sophisticated backends often need support multiple protocol versions data format variants. NeuroStream backend demonstrates implement protocol abstraction enables backward compatibility feature negotiation: protocol abstraction enables backends work across different server versions gracefully handle feature unavailability.","code":"# Advanced protocol handling implement_protocol_negotiation <- function(backend) {   negotiate_protocol <- function(backend, requested_version = \"2.1\") {     # Simulate protocol negotiation     server_versions <- c(\"1.0\", \"1.5\", \"2.0\", \"2.1\")     client_versions <- c(\"2.0\", \"2.1\")      # Find highest common version     common_versions <- intersect(server_versions, client_versions)     if (length(common_versions) == 0) {       stop(\"No compatible protocol version found\")     }      negotiated_version <- max(common_versions)     cat(\"Negotiated protocol version:\", negotiated_version, \"\\n\")      # Configure backend based on negotiated version     backend$protocol_version <- negotiated_version     backend$features <- switch(negotiated_version,       \"1.0\" = c(\"basic_access\"),       \"1.5\" = c(\"basic_access\", \"metadata_queries\"),       \"2.0\" = c(\"basic_access\", \"metadata_queries\", \"chunked_transfer\"),       \"2.1\" = c(         \"basic_access\", \"metadata_queries\", \"chunked_transfer\",         \"streaming\", \"compression\", \"quality_metrics\"       )     )      return(backend)   }    # Version-specific method dispatch   get_data_v1 <- function(backend, rows, cols) {     cat(\"Using v1.x data access protocol\\n\")     # Simple data access implementation   }    get_data_v2 <- function(backend, rows, cols) {     cat(\"Using v2.x data access protocol with streaming\\n\")     # Advanced streaming implementation   }    # Dynamic method selection based on protocol version   select_implementation <- function(backend, operation) {     version_major <- substr(backend$protocol_version, 1, 1)      implementations <- list(       \"1\" = list(get_data = get_data_v1),       \"2\" = list(get_data = get_data_v2)     )      return(implementations[[version_major]][[operation]])   }    cat(\"Protocol abstraction framework implemented\\n\")   return(list(negotiate = negotiate_protocol, select = select_implementation)) }  # Example usage protocol_system <- implement_protocol_negotiation() # enhanced_backend <- protocol_system$negotiate(backend, \"2.1\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"advanced-memory-management","dir":"Articles","previous_headings":"Deep Dive: Advanced Backend Features","what":"Advanced Memory Management","title":"Advanced Backend Development: Storage Extensions","text":"Production backends must carefully manage memory usage handle datasets exceed available RAM. NeuroStream backend implements sophisticated memory management including memory-mapped files, lazy loading, intelligent cache eviction: memory management system enables backends handle arbitrarily large datasets maintaining predictable memory usage.","code":"# Advanced memory management system implement_memory_management <- function(backend) {   # Memory usage tracking   track_memory_usage <- function(backend) {     if (requireNamespace(\"pryr\", quietly = TRUE)) {       current_usage <- pryr::mem_used()       backend$memory_stats <- list(         current_usage = current_usage,         peak_usage = max(backend$memory_stats$peak_usage %||% 0, current_usage),         last_check = Sys.time()       )     }     return(backend)   }    # Intelligent cache eviction   implement_cache_eviction <- function(backend, max_cache_size_mb = 256) {     cache_size_mb <- sum(sapply(backend$data_chunks_cache, function(chunk) {       if (is.matrix(chunk)) object.size(chunk) / 1e6 else 0     }))      if (cache_size_mb > max_cache_size_mb) {       cat(         \"Cache size (\", round(cache_size_mb, 1),         \"MB) exceeds limit, evicting least recently used items\\n\"       )        # Sort cache items by access time (simulated)       cache_access_times <- sapply(names(backend$data_chunks_cache), function(key) {         # In practice, track actual access times         runif(1) # Simulate access time       })        # Remove oldest items until under limit       sorted_keys <- names(sort(cache_access_times))       keys_to_remove <- character()        for (key in sorted_keys) {         if (cache_size_mb <= max_cache_size_mb) break          chunk_size_mb <- object.size(backend$data_chunks_cache[[key]]) / 1e6         backend$data_chunks_cache[[key]] <- NULL         cache_size_mb <- cache_size_mb - chunk_size_mb         keys_to_remove <- c(keys_to_remove, key)       }        cat(\"Evicted\", length(keys_to_remove), \"cache items\\n\")     }      return(backend)   }    # Memory-mapped file support   implement_memory_mapping <- function(backend, file_path) {     if (requireNamespace(\"mmap\", quietly = TRUE)) {       cat(\"Using memory-mapped file access for large data\\n\")       # In practice, implement actual memory mapping       backend$memory_mapped <- TRUE       backend$mmap_handle <- list(file = file_path, mapping = \"simulated\")     } else {       cat(\"Memory mapping not available, using standard file access\\n\")       backend$memory_mapped <- FALSE     }     return(backend)   }    # Adaptive loading strategies   implement_adaptive_loading <- function(backend) {     # Analyze access patterns to optimize loading strategy     analyze_access_pattern <- function(access_history) {       if (length(access_history) < 3) {         return(\"random\")       }        # Detect sequential access       diffs <- diff(access_history)       if (all(diffs == diffs[1])) {         return(\"sequential\")       }        # Detect block access       unique_diffs <- unique(diffs)       if (length(unique_diffs) <= 2) {         return(\"block\")       }        return(\"random\")     }      # Adapt loading strategy based on pattern     backend$loading_strategy <- analyze_access_pattern(backend$access_history %||% c())      cat(\"Detected access pattern:\", backend$loading_strategy, \"\\n\")      # Configure prefetching based on pattern     backend$prefetch_size <- switch(backend$loading_strategy,       \"sequential\" = backend$chunk_size_mb * 2, # Aggressive prefetching       \"block\" = backend$chunk_size_mb, # Moderate prefetching       \"random\" = backend$chunk_size_mb * 0.5 # Conservative prefetching     )      return(backend)   }    return(list(     track_memory = track_memory_usage,     evict_cache = implement_cache_eviction,     memory_map = implement_memory_mapping,     adapt_loading = implement_adaptive_loading   )) }  # memory_mgmt <- implement_memory_management()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"quality-assurance-and-validation","dir":"Articles","previous_headings":"Deep Dive: Advanced Backend Features","what":"Quality Assurance and Validation","title":"Advanced Backend Development: Storage Extensions","text":"Production backends include comprehensive quality assurance measures detect data corruption, validate metadata consistency, ensure data integrity: quality assurance system provides comprehensive validation helps ensure data reliability catch potential issues early analysis pipeline.","code":"# Comprehensive quality assurance system implement_quality_assurance <- function(backend) {   # Data integrity checking   validate_data_integrity <- function(data_chunk, expected_checksum = NULL) {     integrity_checks <- list()      # Check for invalid values     if (any(is.na(data_chunk))) {       integrity_checks$na_values <- list(         status = \"WARNING\",         count = sum(is.na(data_chunk)),         proportion = mean(is.na(data_chunk))       )     }      if (any(is.infinite(data_chunk))) {       integrity_checks$infinite_values <- list(         status = \"ERROR\",         count = sum(is.infinite(data_chunk))       )     }      # Check data range     data_range <- range(data_chunk, na.rm = TRUE)     if (diff(data_range) == 0) {       integrity_checks$constant_values <- list(         status = \"WARNING\",         message = \"All values are identical\"       )     }      # Check for unusual values     if (any(abs(data_chunk) > 1000, na.rm = TRUE)) {       integrity_checks$extreme_values <- list(         status = \"WARNING\",         max_abs_value = max(abs(data_chunk), na.rm = TRUE)       )     }      # Checksum validation if provided     if (!is.null(expected_checksum)) {       actual_checksum <- digest::digest(data_chunk, algo = \"md5\")       if (actual_checksum != expected_checksum) {         integrity_checks$checksum_mismatch <- list(           status = \"ERROR\",           expected = expected_checksum,           actual = actual_checksum         )       }     }      return(integrity_checks)   }    # Temporal consistency checking   validate_temporal_consistency <- function(backend) {     consistency_checks <- list()      if (!is.null(backend$temporal_cache)) {       temporal_info <- backend$temporal_cache        # Check for temporal gaps       if (length(temporal_info$acquisition_times) > 1) {         time_diffs <- diff(temporal_info$acquisition_times)         expected_tr <- backend$metadata_cache$acquisition_params$TR          irregular_intervals <- abs(time_diffs - expected_tr) > expected_tr * 0.1         if (any(irregular_intervals)) {           consistency_checks$irregular_timing <- list(             status = \"WARNING\",             irregular_count = sum(irregular_intervals),             max_deviation = max(abs(time_diffs - expected_tr))           )         }       }        # Check run boundary consistency       if (!is.null(temporal_info$run_boundaries)) {         run_lengths <- diff(c(           temporal_info$run_boundaries,           length(temporal_info$timepoints) + 1         ))         if (any(run_lengths <= 0)) {           consistency_checks$invalid_run_boundaries <- list(             status = \"ERROR\",             message = \"Invalid run boundary specification\"           )         }       }     }      return(consistency_checks)   }    # Spatial consistency checking   validate_spatial_consistency <- function(backend) {     consistency_checks <- list()      if (!is.null(backend$spatial_cache)) {       spatial_info <- backend$spatial_cache        # Validate mask properties       mask <- spatial_info$mask       if (all(!mask)) {         consistency_checks$empty_mask <- list(           status = \"ERROR\",           message = \"Mask contains no valid voxels\"         )       }        # Check coordinate consistency       if (!is.null(spatial_info$coordinates)) {         expected_voxels <- nrow(spatial_info$coordinates)         actual_voxels <- length(mask)          if (expected_voxels != actual_voxels) {           consistency_checks$coordinate_mismatch <- list(             status = \"ERROR\",             expected_voxels = expected_voxels,             actual_voxels = actual_voxels           )         }       }     }      return(consistency_checks)   }    # Comprehensive validation report   generate_validation_report <- function(backend, data_sample = NULL) {     report <- list(       timestamp = Sys.time(),       backend_type = class(backend)[1],       validation_status = \"PASS\"     )      # Run all validation checks     if (!is.null(data_sample)) {       report$data_integrity <- validate_data_integrity(data_sample)     }      report$temporal_consistency <- validate_temporal_consistency(backend)     report$spatial_consistency <- validate_spatial_consistency(backend)      # Determine overall status     all_checks <- c(       report$data_integrity, report$temporal_consistency,       report$spatial_consistency     )      error_count <- sum(sapply(all_checks, function(check) {       if (is.list(check) && \"status\" %in% names(check)) {         check$status == \"ERROR\"       } else {         FALSE       }     }))      warning_count <- sum(sapply(all_checks, function(check) {       if (is.list(check) && \"status\" %in% names(check)) {         check$status == \"WARNING\"       } else {         FALSE       }     }))      if (error_count > 0) {       report$validation_status <- \"FAIL\"     } else if (warning_count > 0) {       report$validation_status <- \"WARNING\"     }      report$summary <- list(       errors = error_count,       warnings = warning_count,       status = report$validation_status     )      return(report)   }    return(list(     validate_data = validate_data_integrity,     validate_temporal = validate_temporal_consistency,     validate_spatial = validate_spatial_consistency,     generate_report = generate_validation_report   )) }  # qa_system <- implement_quality_assurance()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"advanced-topics","dir":"Articles","previous_headings":"","what":"Advanced Topics","title":"Advanced Backend Development: Storage Extensions","text":"’ve mastered fundamental advanced patterns, sophisticated techniques enable backends handle demanding neuroimaging scenarios.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"distributed-and-cloud-integration","dir":"Articles","previous_headings":"Advanced Topics","what":"Distributed and Cloud Integration","title":"Advanced Backend Development: Storage Extensions","text":"Modern neuroimaging increasingly involves distributed computing cloud storage. Advanced backends can integrate cloud services, distributed file systems, compute clusters: cloud integration enables backends work seamlessly modern cloud-native neuroimaging workflows.","code":"# Cloud and distributed computing integration implement_cloud_integration <- function(backend) {   # Cloud storage abstraction   setup_cloud_storage <- function(backend, cloud_config) {     supported_providers <- c(\"aws\", \"gcp\", \"azure\", \"custom\")      if (!cloud_config$provider %in% supported_providers) {       stop(\"Unsupported cloud provider: \", cloud_config$provider)     }      # Configure cloud-specific authentication and endpoints     backend$cloud_config <- cloud_config     backend$cloud_authenticated <- TRUE      cat(\"Cloud storage configured for provider:\", cloud_config$provider, \"\\n\")      # Setup cloud-specific optimizations     backend$transfer_optimization <- switch(cloud_config$provider,       \"aws\" = list(multipart_threshold = 100e6, max_concurrency = 10),       \"gcp\" = list(chunk_size = 256e6, compression = TRUE),       \"azure\" = list(block_size = 100e6, parallel_uploads = 8),       \"custom\" = list(use_defaults = TRUE)     )      return(backend)   }    # Distributed caching   implement_distributed_caching <- function(backend) {     # Simulate distributed cache coordination     backend$distributed_cache <- list(       enabled = TRUE,       cache_nodes = c(\"cache-01.cluster\", \"cache-02.cluster\", \"cache-03.cluster\"),       consistency_level = \"eventual\", # or \"strong\"       replication_factor = 2     )      # Cache distribution strategy     distribute_cache_item <- function(cache_key, data) {       # Hash-based consistent distribution       hash_value <- digest::digest(cache_key, algo = \"crc32\")       node_index <- (strtoi(hash_value, 16L) %% length(backend$distributed_cache$cache_nodes)) + 1       primary_node <- backend$distributed_cache$cache_nodes[node_index]        cat(\"Distributing cache item\", cache_key, \"to node\", primary_node, \"\\n\")        # In practice, implement actual distributed cache protocol       return(list(primary_node = primary_node, replicated = TRUE))     }      backend$cache_distribution <- distribute_cache_item     return(backend)   }    # Parallel data access   implement_parallel_access <- function(backend, max_workers = 4) {     if (requireNamespace(\"parallel\", quietly = TRUE)) {       backend$parallel_enabled <- TRUE       backend$max_workers <- max_workers        # Setup worker pool       backend$worker_pool <- parallel::makeCluster(max_workers)        # Parallel data fetching strategy       parallel_fetch_data <- function(data_requests) {         cat(\"Processing\", length(data_requests), \"data requests in parallel\\n\")          results <- parallel::parLapply(           backend$worker_pool, data_requests,           function(request) {             # Simulate parallel data access             Sys.sleep(runif(1, 0.1, 0.5)) # Simulate network/disk latency             return(list(               request = request, status = \"success\",               data_size = request$rows * request$cols             ))           }         )          return(results)       }        backend$parallel_fetch <- parallel_fetch_data     } else {       cat(\"Parallel processing not available\\n\")       backend$parallel_enabled <- FALSE     }      return(backend)   }    return(list(     setup_cloud = setup_cloud_storage,     distributed_cache = implement_distributed_caching,     parallel_access = implement_parallel_access   )) }  # Example cloud configuration cloud_config <- list(   provider = \"aws\",   region = \"us-west-2\",   bucket = \"neuroimaging-data-bucket\",   credentials = list(     access_key_id = \"AKIA...\",     secret_access_key = \"...\",     session_token = \"...\"   ) )  # cloud_integration <- implement_cloud_integration()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"real-time-and-streaming-analytics","dir":"Articles","previous_headings":"Advanced Topics","what":"Real-Time and Streaming Analytics","title":"Advanced Backend Development: Storage Extensions","text":"Advanced backends can support real-time data streams online analysis, adaptive experiments, quality monitoring: Real-time capabilities enable backends support modern adaptive neuroimaging paradigms continuous quality monitoring.","code":"# Real-time streaming and analytics implement_realtime_capabilities <- function(backend) {   # Real-time data stream handling   setup_realtime_stream <- function(backend, stream_config) {     backend$realtime_config <- stream_config     backend$stream_buffer <- list(       size = stream_config$buffer_size %||% 1000,       data = matrix(NA, nrow = stream_config$buffer_size, ncol = 0),       timestamps = rep(NA, stream_config$buffer_size),       write_index = 1,       read_index = 1     )      cat(       \"Real-time stream configured with buffer size:\",       backend$stream_buffer$size, \"\\n\"     )      return(backend)   }    # Online quality monitoring   implement_online_qc <- function(backend) {     backend$online_qc <- list(       enabled = TRUE,       metrics = list(         motion_threshold = 2.0, # mm         signal_dropout_threshold = 0.1, # proportion         temporal_snr_threshold = 10, # ratio         spike_detection_threshold = 3 # standard deviations       ),       alert_callbacks = list()     )      # Real-time quality check function     check_realtime_quality <- function(new_data, timepoint) {       qc_results <- list(timepoint = timepoint, status = \"pass\", alerts = list())        # Motion detection (simulated)       estimated_motion <- runif(1, 0, 3) # mm       if (estimated_motion > backend$online_qc$metrics$motion_threshold) {         qc_results$alerts$motion <- list(           severity = \"warning\",           value = estimated_motion,           threshold = backend$online_qc$metrics$motion_threshold         )       }        # Signal dropout detection       if (any(new_data == 0)) {         dropout_prop <- mean(new_data == 0)         if (dropout_prop > backend$online_qc$metrics$signal_dropout_threshold) {           qc_results$alerts$dropout <- list(             severity = \"error\",             proportion = dropout_prop,             threshold = backend$online_qc$metrics$signal_dropout_threshold           )           qc_results$status <- \"fail\"         }       }        # Temporal SNR check (simplified)       if (timepoint > 10) { # Need some history for SNR calculation         mean_signal <- mean(new_data, na.rm = TRUE)         signal_var <- var(new_data, na.rm = TRUE)         temporal_snr <- ifelse(signal_var > 0, mean_signal / sqrt(signal_var), Inf)          if (temporal_snr < backend$online_qc$metrics$temporal_snr_threshold) {           qc_results$alerts$low_snr <- list(             severity = \"warning\",             snr = temporal_snr,             threshold = backend$online_qc$metrics$temporal_snr_threshold           )         }       }        return(qc_results)     }      backend$check_quality <- check_realtime_quality     return(backend)   }    # Adaptive processing   implement_adaptive_processing <- function(backend) {     backend$adaptive_config <- list(       enabled = TRUE,       adaptation_triggers = c(\"quality_degradation\", \"motion_excess\", \"signal_loss\"),       responses = list(         quality_degradation = \"increase_averaging\",         motion_excess = \"trigger_realignment\",         signal_loss = \"alert_operator\"       )     )      # Adaptive response function     trigger_adaptation <- function(alert_type, alert_data) {       if (alert_type %in% names(backend$adaptive_config$responses)) {         response <- backend$adaptive_config$responses[[alert_type]]         cat(\"Triggering adaptive response:\", response, \"\\n\")          # Implement specific adaptations         switch(response,           \"increase_averaging\" = {             backend$processing_params$smoothing_kernel <-               backend$processing_params$smoothing_kernel * 1.2           },           \"trigger_realignment\" = {             backend$processing_flags$motion_correction <- TRUE           },           \"alert_operator\" = {             cat(\"ALERT: Operator intervention required -\", alert_type, \"\\n\")           }         )       }     }      backend$trigger_adaptation <- trigger_adaptation     return(backend)   }    return(list(     setup_stream = setup_realtime_stream,     online_qc = implement_online_qc,     adaptive_processing = implement_adaptive_processing   )) }  # realtime_system <- implement_realtime_capabilities()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"tips-and-best-practices","dir":"Articles","previous_headings":"","what":"Tips and Best Practices","title":"Advanced Backend Development: Storage Extensions","text":"advanced guidelines learned developing production neuroimaging backends handle enterprise-scale deployments critical research applications.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"performance-monitoring-requirements","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Performance Monitoring Requirements","title":"Advanced Backend Development: Storage Extensions","text":"Required Metrics Production Backends: - Cache hit rates eviction patterns - Data transfer throughput (MB/s) - Error rates category - Resource utilization (memory, file handles, connections) - Latency percentiles (p50, p95, p99) Implement metric collection initial development establish baseline performance characteristics.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"failure-handling-architecture","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Failure Handling Architecture","title":"Advanced Backend Development: Storage Extensions","text":"Required Failure Mitigation Strategies: - Circuit breakers: Prevent cascade failures stopping requests failing services - Graceful degradation: Provide partial functionality components fail - Retry logic: Exponential backoff jitter transient failures - Resource limits: Prevent resource exhaustion quotas timeouts - Error categorization: Distinguish recoverable fatal errors","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"performance-profiling-strategy","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Performance Profiling Strategy","title":"Advanced Backend Development: Storage Extensions","text":"Continuous Profiling Requirements: 1. Profile memory allocation patterns using profmem profvis 2. Benchmark /O operations representative data sizes 3. Measure cache efficiency various access patterns 4. Test performance degradation resource constraints 5. Compare development vs. production performance characteristics Document performance baselines regression thresholds backend specifications.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"production-deployment-strategies","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Production Deployment Strategies","title":"Advanced Backend Development: Storage Extensions","text":"Deploying advanced backends production environments requires careful attention operational concerns:","code":"# Production deployment considerations implement_production_features <- function(backend) {   # Comprehensive logging and monitoring   setup_monitoring <- function(backend) {     backend$monitoring <- list(       enabled = TRUE,       log_level = \"INFO\", # DEBUG, INFO, WARN, ERROR       metrics_endpoint = \"/metrics\",       health_check_endpoint = \"/health\",       performance_tracking = TRUE     )      # Structured logging     log_event <- function(level, message, context = list()) {       log_entry <- list(         timestamp = format(Sys.time(), \"%Y-%m-%d %H:%M:%S\"),         level = level,         backend_id = backend$connection_id,         message = message,         context = context       )        # In production, send to centralized logging system       cat(paste0(         \"[\", log_entry$timestamp, \"] \",         level, \": \", message, \"\\n\"       ))     }      backend$log <- log_event      # Health check implementation     health_check <- function() {       health_status <- list(         status = \"healthy\",         timestamp = Sys.time(),         checks = list()       )        # Connection health       if (backend$is_open) {         health_status$checks$connection <- \"pass\"       } else {         health_status$checks$connection <- \"fail\"         health_status$status <- \"unhealthy\"       }        # Cache health       cache_size <- length(backend$data_chunks_cache)       if (cache_size < 1000) { # Arbitrary threshold         health_status$checks$cache <- \"pass\"       } else {         health_status$checks$cache <- \"warning\"       }        # Error rate health       if (backend$error_recovery_attempts < backend$max_error_recovery_attempts) {         health_status$checks$error_rate <- \"pass\"       } else {         health_status$checks$error_rate <- \"fail\"         health_status$status <- \"unhealthy\"       }        return(health_status)     }      backend$health_check <- health_check     return(backend)   }    # Configuration management   implement_config_management <- function(backend) {     # Support for external configuration     load_config <- function(config_source = NULL) {       default_config <- list(         cache_size_mb = 256,         timeout_seconds = 30,         retry_attempts = 3,         compression_enabled = TRUE,         monitoring_enabled = TRUE       )        if (!is.null(config_source)) {         if (file.exists(config_source)) {           # Load from config file           external_config <- jsonlite::fromJSON(config_source)           config <- modifyList(default_config, external_config)         } else {           # Load from environment variables           config <- default_config           config$cache_size_mb <- as.numeric(Sys.getenv(             \"CACHE_SIZE_MB\",             default_config$cache_size_mb           ))           config$timeout_seconds <- as.numeric(Sys.getenv(             \"TIMEOUT_SECONDS\",             default_config$timeout_seconds           ))           # Add other env var mappings...         }       } else {         config <- default_config       }        return(config)     }      backend$config <- load_config()     cat(\"Configuration loaded with cache size:\", backend$config$cache_size_mb, \"MB\\n\")      return(backend)   }    # Security and authentication   implement_security <- function(backend) {     backend$security <- list(       authentication_required = TRUE,       encryption_in_transit = TRUE,       access_logging = TRUE,       rate_limiting = list(         enabled = TRUE,         requests_per_minute = 1000,         burst_size = 100       )     )      # Token-based authentication     authenticate_request <- function(token) {       # In practice, validate against authentication service       valid_tokens <- c(\"demo_token_123\", \"research_token_456\")       return(token %in% valid_tokens)     }      # Rate limiting     check_rate_limit <- function(client_id) {       # In practice, implement distributed rate limiting       current_requests <- backend$rate_limit_state[[client_id]] %||% 0       if (current_requests >= backend$security$rate_limiting$requests_per_minute) {         return(list(allowed = FALSE, retry_after = 60))       } else {         backend$rate_limit_state[[client_id]] <- current_requests + 1         return(list(allowed = TRUE))       }     }      backend$authenticate <- authenticate_request     backend$check_rate_limit <- check_rate_limit      return(backend)   }    return(list(     setup_monitoring = setup_monitoring,     config_management = implement_config_management,     security = implement_security   )) }  # production_features <- implement_production_features()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"testing-and-validation-strategies","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Testing and Validation Strategies","title":"Advanced Backend Development: Storage Extensions","text":"Advanced backends require comprehensive testing strategies cover functionality, performance, reliability:","code":"# Comprehensive testing framework implement_testing_framework <- function() {   # Unit testing for backend components   create_unit_tests <- function(backend_class) {     test_suite <- list()      # Test basic contract compliance     test_suite$test_contract <- function() {       backend <- do.call(backend_class, list(stream_url = \"test://localhost\"))        # Test lifecycle       testthat::expect_false(backend$is_open)       backend <- backend_open(backend)       testthat::expect_true(backend$is_open)        # Test data access methods exist       testthat::expect_true(exists(\"backend_get_dims\"))       testthat::expect_true(exists(\"backend_get_mask\"))       testthat::expect_true(exists(\"backend_get_data\"))       testthat::expect_true(exists(\"backend_get_metadata\"))        backend_close(backend)       testthat::expect_false(backend$is_open)     }      # Test error handling     test_suite$test_error_handling <- function() {       # Test invalid URLs       testthat::expect_error(         backend_class(stream_url = \"invalid://url\"),         \"Invalid stream URL\"       )        # Test unopened backend access       backend <- backend_class(stream_url = \"test://localhost\")       testthat::expect_error(         backend_get_dims(backend),         \"must be opened\"       )     }      # Test performance characteristics     test_suite$test_performance <- function() {       backend <- backend_class(stream_url = \"test://localhost\")       backend <- backend_open(backend)        # Test cache performance       start_time <- Sys.time()       data1 <- backend_get_data(backend, rows = 1:10, cols = 1:10)       first_access_time <- Sys.time() - start_time        start_time <- Sys.time()       data2 <- backend_get_data(backend, rows = 1:10, cols = 1:10)       second_access_time <- Sys.time() - start_time        # Second access should be faster (cached)       testthat::expect_lt(second_access_time, first_access_time)        backend_close(backend)     }      return(test_suite)   }    # Integration testing   create_integration_tests <- function() {     integration_tests <- list()      # Test with fmridataset integration     integration_tests$test_dataset_integration <- function() {       backend <- neurostream_backend(stream_url = \"test://localhost\")        # Test dataset creation       dataset <- fmri_dataset(         scans = backend,         TR = 2.0,         run_length = c(50, 50)       )        testthat::expect_true(inherits(dataset, \"fmri_dataset\"))        # Test data access through dataset       data_matrix <- get_data_matrix(dataset)       testthat::expect_true(is.matrix(data_matrix))       testthat::expect_equal(nrow(data_matrix), 100) # 50 + 50     }      # Test chunking integration     integration_tests$test_chunking <- function() {       backend <- neurostream_backend(stream_url = \"test://localhost\")       dataset <- fmri_dataset(scans = backend, TR = 2.0, run_length = 100)        # Test chunking       chunks <- data_chunks(dataset, nchunks = 4)       testthat::expect_length(chunks, 4)        # Test chunk data access       for (chunk in chunks) {         testthat::expect_true(is.matrix(chunk$data))         testthat::expect_gt(ncol(chunk$data), 0)         testthat::expect_gt(nrow(chunk$data), 0)       }     }      return(integration_tests)   }    # Performance benchmarking   create_performance_tests <- function() {     perf_tests <- list()      # Benchmark data access patterns     perf_tests$benchmark_access_patterns <- function() {       if (requireNamespace(\"microbenchmark\", quietly = TRUE)) {         backend <- neurostream_backend(stream_url = \"test://localhost\")         backend <- backend_open(backend)          # Benchmark different access patterns         benchmark_results <- microbenchmark::microbenchmark(           sequential_small = backend_get_data(backend, rows = 1:10, cols = 1:100),           sequential_large = backend_get_data(backend, rows = 1:100, cols = 1:100),           random_access = backend_get_data(backend,             rows = sample(1:100, 10),             cols = sample(1:1000, 100)           ),           times = 10         )          print(benchmark_results)         backend_close(backend)         return(benchmark_results)       }     }      # Memory usage profiling     perf_tests$profile_memory_usage <- function() {       if (requireNamespace(\"profmem\", quietly = TRUE)) {         backend <- neurostream_backend(stream_url = \"test://localhost\")          # Profile memory during backend operations         memory_profile <- profmem::profmem({           backend <- backend_open(backend)           data <- backend_get_data(backend, rows = 1:100, cols = 1:1000)           backend_close(backend)         })          return(memory_profile)       }     }      return(perf_tests)   }    return(list(     unit_tests = create_unit_tests,     integration_tests = create_integration_tests,     performance_tests = create_performance_tests   )) }  # testing_framework <- implement_testing_framework()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"troubleshooting-advanced-backend-issues","dir":"Articles","previous_headings":"","what":"Troubleshooting Advanced Backend Issues","title":"Advanced Backend Development: Storage Extensions","text":"Advanced backends introduce complexity can lead sophisticated failure modes. Understanding diagnose resolve issues crucial production deployments.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"network-and-connectivity-issues","dir":"Articles","previous_headings":"Troubleshooting Advanced Backend Issues","what":"Network and Connectivity Issues","title":"Advanced Backend Development: Storage Extensions","text":"Advanced backends often depend network resources, leading complex failure scenarios: Intermittent Connection Failures Implement exponential backoff jitter, circuit breaker patterns, connection pooling. Monitor connection health continuously switch backup endpoints primary connections become unreliable. Data Corruption Transfer Use checksums integrity validation multiple layers. Implement end--end verification automatic retry different transfer methods corruption detected. Performance Degradation Load Monitor network throughput, implement adaptive chunk sizing, use quality--service prioritization. Consider implementing local caching proxies frequently accessed data.","code":"# Network troubleshooting tools implement_network_diagnostics <- function(backend) {   # Connection health monitoring   monitor_connection_health <- function(backend) {     health_metrics <- list(       timestamp = Sys.time(),       connection_latency = NA,       throughput_mbps = NA,       packet_loss = NA,       connection_stable = FALSE     )      # Simulate latency measurement     start_time <- Sys.time()     # In practice: ping or small data request     Sys.sleep(0.01) # Simulate network latency     health_metrics$connection_latency <- difftime(Sys.time(), start_time, units = \"secs\")      # Simulate throughput measurement     # In practice: transfer known data size and measure time     health_metrics$throughput_mbps <- runif(1, 50, 1000) # Mbps      # Determine connection stability     health_metrics$connection_stable <-       health_metrics$connection_latency < 0.1 && health_metrics$throughput_mbps > 100      return(health_metrics)   }    # Adaptive connection management   implement_adaptive_connection <- function(backend) {     backend$connection_adaption <- list(       enabled = TRUE,       performance_history = list(),       adaptation_thresholds = list(         latency_warning = 0.5, # seconds         latency_critical = 2.0, # seconds         throughput_warning = 50, # Mbps         throughput_critical = 10 # Mbps       )     )      adapt_connection_strategy <- function(health_metrics) {       current_performance <- list(         latency = health_metrics$connection_latency,         throughput = health_metrics$throughput_mbps       )        # Store performance history       backend$connection_adaption$performance_history <-         append(backend$connection_adaption$performance_history,           list(current_performance),           after = 0         )        # Keep only recent history       if (length(backend$connection_adaption$performance_history) > 10) {         backend$connection_adaption$performance_history <-           backend$connection_adaption$performance_history[1:10]       }        # Adapt based on current performance       thresholds <- backend$connection_adaption$adaptation_thresholds        if (current_performance$latency > thresholds$latency_critical ||         current_performance$throughput < thresholds$throughput_critical) {         cat(\"Critical performance degradation detected, switching to backup connection\\n\")         backend$connection_strategy <- \"backup\"       } else if (current_performance$latency > thresholds$latency_warning ||         current_performance$throughput < thresholds$throughput_warning) {         cat(\"Performance warning, reducing chunk size\\n\")         backend$chunk_size_mb <- max(backend$chunk_size_mb * 0.8, 8)       } else {         # Performance is good, can increase chunk size         backend$chunk_size_mb <- min(backend$chunk_size_mb * 1.1, 128)       }     }      backend$adapt_connection <- adapt_connection_strategy     return(backend)   }    return(list(     monitor_health = monitor_connection_health,     adaptive_connection = implement_adaptive_connection   )) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"cache-and-memory-management-issues","dir":"Articles","previous_headings":"Troubleshooting Advanced Backend Issues","what":"Cache and Memory Management Issues","title":"Advanced Backend Development: Storage Extensions","text":"Advanced caching systems can exhibit complex behaviors require sophisticated debugging: Cache Thrashing Monitor cache hit rates access patterns. Implement cache warming strategies consider hierarchical caching different eviction policies different access patterns. Memory Leaks Long-Running Sessions Use memory profiling tools implement periodic cache cleanup. Track object lifetimes ensure proper cleanup error conditions. Cache Inconsistency Implement cache invalidation strategies consistency checking. Use versioning timestamps detect stale cache entries.","code":"# Cache debugging and optimization implement_cache_diagnostics <- function(backend) {   # Cache performance analysis   analyze_cache_performance <- function(backend) {     cache_stats <- list(       timestamp = Sys.time(),       total_items = length(backend$data_chunks_cache),       hit_rate = backend$cache_hits / (backend$cache_hits + backend$cache_misses),       memory_usage_mb = sum(sapply(backend$data_chunks_cache, object.size)) / 1e6,       access_patterns = list()     )      # Analyze access patterns     if (length(backend$access_history) > 0) {       access_intervals <- diff(backend$access_history)       cache_stats$access_patterns <- list(         mean_interval = mean(access_intervals),         sequential_accesses = sum(access_intervals == 1) / length(access_intervals),         random_accesses = sum(abs(access_intervals) > 10) / length(access_intervals)       )     }      # Identify cache hotspots     if (length(backend$data_chunks_cache) > 0) {       cache_access_counts <- sapply(names(backend$data_chunks_cache), function(key) {         # In practice, track actual access counts         sample(1:100, 1)       })        cache_stats$hotspots <- list(         most_accessed = names(sort(cache_access_counts, decreasing = TRUE))[1:3],         least_accessed = names(sort(cache_access_counts, decreasing = FALSE))[1:3]       )     }      return(cache_stats)   }    # Cache optimization recommendations   generate_cache_recommendations <- function(cache_stats) {     recommendations <- list()      # Hit rate analysis     if (cache_stats$hit_rate < 0.5) {       recommendations$low_hit_rate <- list(         issue = \"Low cache hit rate\",         suggestion = \"Consider increasing cache size or adjusting eviction policy\",         current_rate = cache_stats$hit_rate       )     }      # Memory usage analysis     if (cache_stats$memory_usage_mb > 512) { # Arbitrary threshold       recommendations$high_memory <- list(         issue = \"High cache memory usage\",         suggestion = \"Consider implementing more aggressive eviction or cache compression\",         current_usage = cache_stats$memory_usage_mb       )     }      # Access pattern analysis     if (!is.null(cache_stats$access_patterns)) {       if (cache_stats$access_patterns$sequential_accesses > 0.7) {         recommendations$sequential_pattern <- list(           issue = \"Highly sequential access pattern detected\",           suggestion = \"Consider implementing prefetching for sequential data\",           sequential_ratio = cache_stats$access_patterns$sequential_accesses         )       }        if (cache_stats$access_patterns$random_accesses > 0.7) {         recommendations$random_pattern <- list(           issue = \"Highly random access pattern detected\",           suggestion = \"Consider larger cache size and LRU eviction policy\",           random_ratio = cache_stats$access_patterns$random_accesses         )       }     }      return(recommendations)   }    return(list(     analyze_performance = analyze_cache_performance,     generate_recommendations = generate_cache_recommendations   )) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"integration-with-other-vignettes","dir":"Articles","previous_headings":"","what":"Integration with Other Vignettes","title":"Advanced Backend Development: Storage Extensions","text":"advanced backend development guide represents culmination fmridataset extension system: Foundation Knowledge: Start Backend Registry understand basic backend contract registration system attempting advanced development. Architecture Context: Architecture Overview provides theoretical foundation understanding advanced backends fit overall system design. Practical Application: - Getting Started - See advanced backends appear end users - Study-Level Analysis - Understand advanced backends scale multi-subject studies - H5 Backend Usage - Example production-quality backend implementation Production Deployment: techniques vignette enable backends can handle enterprise-scale neuroimaging workflows requirements reliability, performance, scalability go far beyond research prototypes. Ecosystem Integration: Advanced backends can integrate cloud platforms, distributed computing systems, real-time data acquisition systems, enabling fmridataset work modern neuroimaging infrastructure environments.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/extending-backends.html","id":"session-information","dir":"Articles","previous_headings":"","what":"Session Information","title":"Advanced Backend Development: Storage Extensions","text":"","code":"sessionInfo()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"motivation-unified-fmri-data-access","dir":"Articles","previous_headings":"","what":"Motivation: Unified fMRI Data Access","title":"Getting Started with fmridataset","text":"fMRI analyses frequently involve heterogeneous data sources: NIfTI files different scanners, BIDS-organized datasets, preprocessed matrices various pipelines, archived HDF5 files. format traditionally requires distinct loading procedures, memory management approaches, temporal organization methods. fmridataset package provides unified interface layer abstracts format-specific differences. abstraction enables: Consistent data access: Identical functions work across supported formats Format-specific optimization: backend implements optimal loading strategies Temporal structure preservation: Unified handling runs, TR, event timing Memory efficiency: Lazy loading chunking strategies adapt data characteristics","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"quick-start-multi-format-data-access","dir":"Articles","previous_headings":"","what":"Quick Start: Multi-Format Data Access","title":"Getting Started with fmridataset","text":"Let’s jump concrete example shows fmridataset simplifies typical workflow. ’ll create dataset simple matrix, add temporal structure experimental events, demonstrate unified interface data access: Now let’s see unified interface action: Technical Note: unified interface operates backend abstraction. data format implements contract, enabling format-independent analysis code.","code":"library(fmridataset)  # Create realistic synthetic fMRI data using helper function activation_periods <- c(20:30, 50:60, 120:130, 150:160) fmri_matrix <- generate_example_fmri_data(   n_timepoints = 200,   n_voxels = 1000,   n_active = 100,   activation_periods = activation_periods,   signal_strength = 0.5 )  # Create dataset with temporal structure dataset <- matrix_dataset(   datamat = fmri_matrix,   TR = 2.0, # 2-second repetition time   run_length = c(100, 100) # Two runs of 100 timepoints each )  # Add experimental design using helper function events <- generate_example_events(   n_runs = 2,   events_per_run = 2,   TR = 2.0,   run_length = 100 )  dataset$event_table <- events  # Display the dataset print_dataset_info(dataset, \"Dataset Summary\") #>  #> Dataset Summary #> --------------- #> Dataset class: matrix_dataset, fmri_dataset, list  #> TR: 2 seconds #> Number of runs: 2  #> Run lengths: 100 100  #> Total timepoints: 200  #> Events: 4 events across 2 runs # Access temporal information (same methods regardless of data source) cat(\"TR:\", get_TR(dataset), \"seconds\\n\") #> TR: 2 seconds cat(\"Number of runs:\", n_runs(dataset), \"\\n\") #> Number of runs: 2 cat(\"Total timepoints:\", n_timepoints(dataset), \"\\n\") #> Total timepoints: 200 cat(\"Run durations:\", get_run_duration(dataset), \"seconds\\n\") #> Run durations: 200 200 seconds  # Get data for analysis (returns standard R matrix) all_data <- get_data_matrix(dataset) cat(\"Full data dimensions:\", dim(all_data), \"\\n\") #> Full data dimensions: 200 1000  # Get data from specific runs run1_data <- get_data_matrix(dataset, run_id = 1) cat(\"Run 1 dimensions:\", dim(run1_data), \"\\n\") #> Run 1 dimensions: 200 1000  # Access experimental events cat(\"\\nEvent Table:\\n\") #>  #> Event Table: print(head(dataset$event_table, 4)) #>       onset duration trial_type run #> 1  66.66667       10     task_A   1 #> 2 133.33333       10     task_B   1 #> 3 266.66667       10     task_A   2 #> 4 333.33333       10     task_B   2"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"core-concepts","dir":"Articles","previous_headings":"","what":"Core Concepts","title":"Getting Started with fmridataset","text":"Now ’ve seen unified interface action, let’s understand key abstractions make fmridataset powerful flexible.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"the-dataset-your-analysis-starting-point","dir":"Articles","previous_headings":"Core Concepts","what":"The Dataset: Your Analysis Starting Point","title":"Getting Started with fmridataset","text":"dataset fmridataset represents everything need fMRI analysis one unified object. Think smart container knows data’s spatial organization (voxels contain brain tissue), temporal structure (timepoints organized runs), experimental design (events occurred). Unlike working raw files matrices, dataset maintains metadata provides consistent access methods regardless underlying data stored. abstraction enables analysis functions operate dataset type method dispatch. dataset implementation handles format-specific /O operations maintaining consistent public methods across backends.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"storage-backends-format-independence","dir":"Articles","previous_headings":"Core Concepts","what":"Storage Backends: Format Independence","title":"Getting Started with fmridataset","text":"Behind every dataset storage backend handles actual data input/output operations. create dataset NIfTI files, fmridataset automatically creates NIfTI backend knows read files efficiently. create dataset matrix, uses matrix backend optimized -memory data. separation interface implementation enables format independence backend abstraction layer. Backend implementations provide lazy loading file-based data sources chunked processing memory-constrained operations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"temporal-structure-more-than-just-time","dir":"Articles","previous_headings":"Core Concepts","what":"Temporal Structure: More Than Just Time","title":"Getting Started with fmridataset","text":"fMRI data rich temporal structure goes beyond simple time series. Runs may different lengths, repetition times vary studies, experimental events need aligned acquisition timing. sampling frame abstraction captures temporal complexity unified model. Sampling frames maintain run boundary information time--sample index mappings. provide methods run-specific data extraction temporal unit conversion seconds TR indices.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"data-chunking-efficiency-without-complexity","dir":"Articles","previous_headings":"Core Concepts","what":"Data Chunking: Efficiency Without Complexity","title":"Getting Started with fmridataset","text":"Modern fMRI datasets can enormous, often exceeding available memory. Traditional approaches require manually split data pieces carefully manage memory usage. fmridataset provides automatic data chunking handles complexity transparently. chunking system partitions datasets memory-manageable segments independent processing. Chunk generation uses configurable parameters (nchunks, runwise) determine voxel groupings, chunk containing data subsets corresponding voxel metadata.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"deep-dive-creating-and-using-datasets","dir":"Articles","previous_headings":"","what":"Deep Dive: Creating and Using Datasets","title":"Getting Started with fmridataset","text":"fundamentals clear, let’s explore create datasets different sources use full capabilities effectively.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"matrix-datasets-when-data-is-already-in-memory","dir":"Articles","previous_headings":"Deep Dive: Creating and Using Datasets > Creating Datasets from Different Sources","what":"Matrix Datasets: When Data is Already in Memory","title":"Getting Started with fmridataset","text":"Matrix datasets perfect already preprocessed data R working simulated data. provide fastest access since file /O required: Matrix datasets store data memory, providing optimal access performance datasets within available RAM limits.","code":"# Create from existing matrix data_matrix <- matrix(rnorm(1000 * 150), nrow = 150, ncol = 1000)  dataset <- matrix_dataset(   datamat = data_matrix,   TR = 2.5,   run_length = c(75, 75) # Two equal runs )  # Immediate access - no loading delay data <- get_data_matrix(dataset) cat(\"Matrix dataset dimensions:\", dim(data), \"\\n\") #> Matrix dataset dimensions: 150 1000"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"file-datasets-lazy-loading-for-large-data","dir":"Articles","previous_headings":"Deep Dive: Creating and Using Datasets > Creating Datasets from Different Sources","what":"File Datasets: Lazy Loading for Large Data","title":"Getting Started with fmridataset","text":"File datasets work NIfTI files implement lazy loading, data remains disk explicitly accessed: File datasets implement lazy loading apply brain mask filtering minimize memory requirements.","code":"# File-based dataset (paths would be real NIfTI files) file_paths <- c(   \"/path/to/subject01_run1.nii.gz\",   \"/path/to/subject01_run2.nii.gz\" ) mask_path <- \"/path/to/brain_mask.nii.gz\"  # Creating the dataset doesn't load any data dataset <- fmri_file_dataset(   scans = file_paths,   mask = mask_path,   TR = 2.0,   run_length = c(180, 180) )  # Data loads when first accessed print(dataset) # Shows metadata only # data <- get_data_matrix(dataset)  # This would trigger loading"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"memory-datasets-working-with-neurovec-objects","dir":"Articles","previous_headings":"Deep Dive: Creating and Using Datasets > Creating Datasets from Different Sources","what":"Memory Datasets: Working with NeuroVec Objects","title":"Getting Started with fmridataset","text":"’re already using neuroim2 package, memory datasets let work existing NeuroVec objects: Memory datasets preserve spatial structure NeuroVec objects providing unified dataset interface.","code":"# Example with NeuroVec objects (requires neuroim2) if (requireNamespace(\"neuroim2\", quietly = TRUE)) {   # Create example NeuroVec   dims <- c(10, 10, 10, 100) # 10x10x10 voxels, 100 timepoints   nvec <- neuroim2::NeuroVec(     array(rnorm(prod(dims)), dims),     space = neuroim2::NeuroSpace(dims)   )    # Create mask   mask_dims <- dims[1:3]   mask <- neuroim2::NeuroVol(     array(1, mask_dims),     space = neuroim2::NeuroSpace(mask_dims)   )    # Create dataset   dataset <- fmri_mem_dataset(     scans = list(nvec),     mask = mask,     TR = 2.0,     run_length = 100   ) }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"direct-data-access","dir":"Articles","previous_headings":"Deep Dive: Creating and Using Datasets > Accessing Data Efficiently","what":"Direct Data Access","title":"Getting Started with fmridataset","text":"primary way get data get_data_matrix(), always returns standard R matrix timepoints × voxels orientation: file-based datasets, operations trigger data loading. package automatically applies brain masks handles necessary format conversions.","code":"# Get complete dataset full_data <- get_data_matrix(dataset) cat(\"Full data shape:\", dim(full_data), \"\\n\") #> Full data shape: 100 1000  # Get specific runs run1 <- get_data_matrix(dataset, run_id = 1) run2 <- get_data_matrix(dataset, run_id = 2) cat(\"Run 1 shape:\", dim(run1), \"\\n\") #> Run 1 shape: 100 1000 cat(\"Run 2 shape:\", dim(run2), \"\\n\") #> Run 2 shape: 100 1000  # Get multiple runs runs_1_2 <- get_data_matrix(dataset, run_id = c(1, 2)) cat(\"Runs 1-2 combined shape:\", dim(runs_1_2), \"\\n\") #> Runs 1-2 combined shape: 100 1000"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"memory-efficient-chunking","dir":"Articles","previous_headings":"Deep Dive: Creating and Using Datasets > Accessing Data Efficiently","what":"Memory-Efficient Chunking","title":"Getting Started with fmridataset","text":"large datasets, chunking enables processing without loading everything memory: chunking system automatically divides voxels optimally provides metadata voxels chunk contains.","code":"# Create chunks for processing chunks <- data_chunks(dataset, nchunks = 4)  # Process each chunk independently results <- list() for (chunk in chunks) {   cat(     \"Processing chunk\", chunk$chunk_num,     \"with\", ncol(chunk$data), \"voxels\\n\"   )    # Your analysis here - each chunk$data is a standard matrix   chunk_result <- colMeans(chunk$data) # Example: compute mean time series   results[[chunk$chunk_num]] <- chunk_result }  # Combine results all_means <- do.call(c, results) cat(\"Computed means for\", length(all_means), \"total voxels\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"run-wise-processing","dir":"Articles","previous_headings":"Deep Dive: Creating and Using Datasets > Accessing Data Efficiently","what":"Run-wise Processing","title":"Getting Started with fmridataset","text":"Many fMRI analyses need process runs separately. chunking system supports directly: Run-wise chunking ensures chunk contains data exactly one run, making perfect analyses require run boundaries.","code":"# Create one chunk per run run_chunks <- data_chunks(dataset, runwise = TRUE)  # Process runs independently run_results <- list() for (chunk in run_chunks) {   cat(     \"Processing run\", chunk$chunk_num,     \"with\", nrow(chunk$data), \"timepoints\\n\"   )    # Run-specific analysis   run_results[[chunk$chunk_num]] <- analyze_run(chunk$data) }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"understanding-sampling-frames","dir":"Articles","previous_headings":"Deep Dive: Creating and Using Datasets > Working with Temporal Structure","what":"Understanding Sampling Frames","title":"Getting Started with fmridataset","text":"Every dataset contains sampling frame captures temporal organization: sampling frame provides temporal metadata needed sophisticated time series analyses.","code":"# Access the sampling frame sf <- dataset$sampling_frame print(sf) #> Sampling Frame #> ============== #>  #> Structure: #>   1 block #>   Total scans: 100 #>  #> Timing: #>   TR: 2 s #>   Precision: 0.1 s #>  #> Duration: #>   Total time: 200.0 s  # Query temporal properties cat(\"TR:\", get_TR(sf), \"seconds\\n\") #> TR: 2 seconds cat(\"Number of runs:\", n_runs(sf), \"\\n\") #> Number of runs: 1 cat(\"Run lengths:\", get_run_lengths(sf), \"timepoints\\n\") #> Run lengths: 100 timepoints cat(\"Total duration:\", get_total_duration(sf), \"seconds\\n\") #> Total duration: 200 seconds  # Get timing information run_durations <- get_run_duration(sf) cat(\"Run durations:\", run_durations, \"seconds\\n\") #> Run durations: 200 seconds  # Access sample indices for each run run_samples <- samples(sf) cat(\"Run 1 samples:\", head(run_samples[[1]]), \"...\\n\") #> Run 1 samples: 1 ... cat(\"Run 2 samples:\", head(run_samples[[2]]), \"...\\n\") #> Run 2 samples: 2 ..."},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"converting-between-time-and-samples","dir":"Articles","previous_headings":"Deep Dive: Creating and Using Datasets > Working with Temporal Structure","what":"Converting Between Time and Samples","title":"Getting Started with fmridataset","text":"Sampling frames enable easy conversion time units sample indices: timing awareness crucial proper event-related analyses temporal alignment.","code":"# Convert event times to sample indices event_onsets <- c(30, 75, 130, 175) # seconds TR <- get_TR(dataset$sampling_frame)  # Manual conversion sample_indices <- round(event_onsets / TR) + 1 cat(\"Event onsets at samples:\", sample_indices, \"\\n\") #> Event onsets at samples: 16 39 66 89  # Use sampling frame for run-aware conversions for (run_idx in 1:n_runs(dataset$sampling_frame)) {   run_start <- samples(dataset$sampling_frame)[[run_idx]][1]   cat(\"Run\", run_idx, \"starts at global sample\", run_start, \"\\n\") } #> Run 1 starts at global sample 1"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"advanced-topics","dir":"Articles","previous_headings":"","what":"Advanced Topics","title":"Getting Started with fmridataset","text":"’re comfortable basic dataset creation access, advanced techniques help handle complex scenarios optimize performance.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"memory-management-strategies","dir":"Articles","previous_headings":"Advanced Topics","what":"Memory Management Strategies","title":"Getting Started with fmridataset","text":"Understanding data loaded helps optimize memory usage large datasets: Use file datasets exploration convert matrix datasets need repeated fast access data.","code":"# File datasets: creation is cheap, access triggers loading file_dataset <- fmri_file_dataset(file_paths, mask_path, TR = 2.0) cat(\"Created file dataset (no data loaded yet)\\n\")  # First access loads data first_access <- get_data_matrix(file_dataset, run_id = 1) cat(\"Loaded run 1:\", object.size(first_access), \"bytes\\n\")  # Subsequent accesses use cached data second_access <- get_data_matrix(file_dataset, run_id = 1) cat(\"Second access (cached)\\n\")  # Convert to matrix dataset to keep everything in memory matrix_version <- as.matrix_dataset(file_dataset) cat(\"Converted to matrix dataset\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"custom-event-integration","dir":"Articles","previous_headings":"Advanced Topics","what":"Custom Event Integration","title":"Getting Started with fmridataset","text":"Experimental design information integrates seamlessly temporal structure: integration experimental design temporal structure enables sophisticated event-related analyses.","code":"# Create detailed event table events <- data.frame(   onset = c(10, 30, 50, 70, 110, 130, 150, 170),   duration = c(2, 2, 2, 2, 2, 2, 2, 2),   trial_type = rep(c(\"faces\", \"houses\"), 4),   amplitude = c(1, 1, 1, 1, 1, 1, 1, 1),   run = c(1, 1, 1, 1, 2, 2, 2, 2) )  # Add to dataset dataset$event_table <- events  # Analyze events in context of temporal structure TR <- get_TR(dataset$sampling_frame) for (i in 1:nrow(events)) {   onset_sample <- round(events$onset[i] / TR) + 1   run_id <- events$run[i]    cat(     \"Event\", i, \":\", events$trial_type[i],     \"at sample\", onset_sample, \"in run\", run_id, \"\\n\"   ) } #> Event 1 : faces at sample 6 in run 1  #> Event 2 : houses at sample 16 in run 1  #> Event 3 : faces at sample 26 in run 1  #> Event 4 : houses at sample 36 in run 1  #> Event 5 : faces at sample 56 in run 2  #> Event 6 : houses at sample 66 in run 2  #> Event 7 : faces at sample 76 in run 2  #> Event 8 : houses at sample 86 in run 2  # Extract data around events extract_event_data <- function(dataset, event_row, window = c(-2, 8)) {   TR <- get_TR(dataset$sampling_frame)   onset_sample <- round(event_row$onset / TR) + 1   run_id <- event_row$run    # Get samples for this window   samples <- (onset_sample + window[1]):(onset_sample + window[2])    # Extract data for this run and time window   run_data <- get_data_matrix(dataset, run_id = run_id)   event_data <- run_data[samples, , drop = FALSE]    return(event_data) }  # Example: extract data around first event event1_data <- extract_event_data(dataset, events[1, ]) cat(\"Event 1 data shape:\", dim(event1_data), \"\\n\") #> Event 1 data shape: 11 1000"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"performance-optimization","dir":"Articles","previous_headings":"Advanced Topics","what":"Performance Optimization","title":"Getting Started with fmridataset","text":"Several strategies can significantly improve performance large datasets: Choose chunk sizes based available memory processing requirements. chunks mean less memory usage overhead.","code":"# Strategy 1: Use appropriate chunk sizes small_chunks <- data_chunks(dataset, nchunks = 20) # Many small chunks large_chunks <- data_chunks(dataset, nchunks = 4) # Fewer large chunks  # Strategy 2: Process runs separately when appropriate run_chunks <- data_chunks(dataset, runwise = TRUE)  # Strategy 3: Use partial loading for file datasets partial_data <- get_data_matrix(dataset, run_id = 1) # Load only one run  # Strategy 4: Monitor memory usage if (requireNamespace(\"pryr\", quietly = TRUE)) {   cat(\"Memory before loading:\", pryr::mem_used(), \"\\n\")   data <- get_data_matrix(dataset)   cat(\"Memory after loading:\", pryr::mem_used(), \"\\n\") }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"tips-and-best-practices","dir":"Articles","previous_headings":"","what":"Tips and Best Practices","title":"Getting Started with fmridataset","text":"practical guidelines learned real-world usage help avoid common pitfalls work effectively.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"memory-management","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Memory Management","title":"Getting Started with fmridataset","text":"Important: file-based datasets exceeding available RAM, use partial loading specifying run_id get_data_matrix(). Loading complete datasets occur runs required simultaneously analysis. working large datasets, memory management becomes crucial: Monitor memory usage adjust chunk sizes based system’s capabilities.","code":"# Good: Process in chunks analyze_large_dataset <- function(dataset) {   chunks <- data_chunks(dataset, nchunks = 10)   results <- list()    for (chunk in chunks) {     # Process chunk     result <- your_analysis(chunk$data)     results[[chunk$chunk_num]] <- result      # Optionally force garbage collection     if (chunk$chunk_num %% 5 == 0) {       gc()     }   }    return(do.call(rbind, results)) }  # Bad: Loading everything at once for large datasets # big_data <- get_data_matrix(very_large_dataset)  # May exhaust memory"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"data-validation","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Data Validation","title":"Getting Started with fmridataset","text":"Required Check: Validate temporal structure using get_run_lengths() ensure alignment experimental design. Run length mismatches indicate data organization errors propagate subsequent analyses.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"chunking-configuration","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Chunking Configuration","title":"Getting Started with fmridataset","text":"Run-Boundary Operations: Configure data_chunks(dataset, runwise = TRUE) analyses requiring run boundary preservation, including: - Temporal detrending - Motion parameter regression - Run-level normalization - Temporal filtering","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"error-prevention","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Error Prevention","title":"Getting Started with fmridataset","text":"Common validation checks prevent downstream errors: Early validation catches problems affect analysis.","code":"validate_dataset <- function(dataset) {   # Check temporal consistency   sf <- dataset$sampling_frame   expected_timepoints <- n_timepoints(sf)    if (inherits(dataset, \"matrix_dataset\")) {     actual_timepoints <- nrow(dataset$datamat)     if (expected_timepoints != actual_timepoints) {       stop(         \"Temporal structure mismatch: expected \", expected_timepoints,         \" timepoints, found \", actual_timepoints       )     }   }    # Check event table consistency   if (nrow(dataset$event_table) > 0) {     max_run <- max(dataset$event_table$run)     n_runs_data <- n_runs(sf)      if (max_run > n_runs_data) {       stop(         \"Events reference run \", max_run,         \" but dataset only has \", n_runs_data, \" runs\"       )     }      # Check event timing     total_duration <- get_total_duration(sf)     max_event_time <- max(dataset$event_table$onset + dataset$event_table$duration)      if (max_event_time > total_duration) {       warning(         \"Events extend beyond scan duration (\",         max_event_time, \" > \", total_duration, \" seconds)\"       )     }   }    cat(\"Dataset validation passed\\n\") }  # validate_dataset(dataset)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"reproducibility","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Reproducibility","title":"Getting Started with fmridataset","text":"Document dataset creation reproducible research: Good metadata practices make analyses reproducible help collaborators understand data structure.","code":"# Create metadata record create_dataset_record <- function(dataset) {   list(     timestamp = Sys.time(),     r_version = R.version.string,     fmridataset_version = packageVersion(\"fmridataset\"),     dataset_class = class(dataset)[1],     temporal_structure = list(       n_runs = n_runs(dataset$sampling_frame),       run_lengths = get_run_lengths(dataset$sampling_frame),       TR = get_TR(dataset$sampling_frame)     ),     spatial_structure = list(       n_voxels = if (inherits(dataset, \"matrix_dataset\")) {         ncol(dataset$datamat)       } else {         \"unknown\"       }     ),     events = list(       n_events = nrow(dataset$event_table),       event_types = if (nrow(dataset$event_table) > 0) {         unique(dataset$event_table$trial_type)       } else {         character(0)       }     )   ) }  # Store metadata # metadata <- create_dataset_record(dataset) # saveRDS(metadata, \"analysis_dataset_metadata.rds\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"troubleshooting","dir":"Articles","previous_headings":"","what":"Troubleshooting","title":"Getting Started with fmridataset","text":"things don’t work expected, solutions address common issues encountered real usage.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"common-error-messages","dir":"Articles","previous_headings":"Troubleshooting","what":"Common Error Messages","title":"Getting Started with fmridataset","text":"“Error: Run lengths sum number timepoints” occurs run_length parameter doesn’t match actual data dimensions. Check sum(run_length) equals number timepoints data matrix files. “Error: read file: [filename]” File path issues common. Use file.exists() verify paths ensure ’re using absolute paths correct relative paths working directory. “Warning: Events extend beyond scan duration” event table contains onsets durations exceed total scan time. Verify event timing units (seconds vs. TRs). “Error: Mask dimensions match data dimensions” spatial dimensions mask don’t match data. matrix datasets, mask one element per column. file datasets, mask spatial dimensions must match data files.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"performance-issues","dir":"Articles","previous_headings":"Troubleshooting","what":"Performance Issues","title":"Getting Started with fmridataset","text":"dataset operations slow: Check file paths: Network drives compressed files can slow read Monitor memory: Use gc() consider smaller chunk sizes Use run-specific access: Load needed runs run_id parameter Consider format conversion: Convert frequently-accessed file datasets matrix datasets","code":"# Benchmark different access patterns if (requireNamespace(\"microbenchmark\", quietly = TRUE)) {   # Compare full vs. partial loading   mb <- microbenchmark::microbenchmark(     full_data = get_data_matrix(dataset),     run1_only = get_data_matrix(dataset, run_id = 1),     times = 5   )   print(mb) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"memory-issues","dir":"Articles","previous_headings":"Troubleshooting","what":"Memory Issues","title":"Getting Started with fmridataset","text":"encounter memory problems: Use chunking datasets exceed available memory, consider processing runs separately.","code":"# Monitor memory usage during operations memory_profile <- function(dataset) {   if (requireNamespace(\"pryr\", quietly = TRUE)) {     start_mem <- pryr::mem_used()     cat(\"Starting memory:\", start_mem, \"\\n\")      # Try loading data     tryCatch(       {         data <- get_data_matrix(dataset, run_id = 1)         loaded_mem <- pryr::mem_used()         cat(\"After loading run 1:\", loaded_mem, \"\\n\")         cat(\"Data size:\", object.size(data), \"\\n\")          rm(data)         gc()         final_mem <- pryr::mem_used()         cat(\"After cleanup:\", final_mem, \"\\n\")       },       error = function(e) {         cat(\"Memory error:\", conditionMessage(e), \"\\n\")       }     )   } }  # memory_profile(dataset)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"integration-with-other-vignettes","dir":"Articles","previous_headings":"","what":"Integration with Other Vignettes","title":"Getting Started with fmridataset","text":"introduction connects several topics fmridataset ecosystem: Next Steps: - Architecture Overview - Understand design principles extensibility model - Study-Level Analysis - Scale single subjects multi-subject studies - H5 Backend Usage - Use HDF5 efficient storage large datasets Advanced Topics: - Backend Registry - Create custom backends new data formats - Extending Backends - Deep dive backend development Related Packages: fmridataset integrates seamlessly broader neuroimaging ecosystem: - neuroim2: Use NeuroVec objects directly memory datasets - fmrireg: Leverage temporal structure regression modeling - DelayedArray: Advanced array operations lazy evaluation","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/fmridataset-intro.html","id":"session-information","dir":"Articles","previous_headings":"","what":"Session Information","title":"Getting Started with fmridataset","text":"","code":"sessionInfo() #> R version 4.5.2 (2025-10-31) #> Platform: x86_64-pc-linux-gnu #> Running under: Ubuntu 24.04.3 LTS #>  #> Matrix products: default #> BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  #> LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 #>  #> locale: #>  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        #>  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    #>  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           #> [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    #>  #> time zone: UTC #> tzcode source: system (glibc) #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] fmridataset_0.8.9 #>  #> loaded via a namespace (and not attached): #>  [1] gtable_0.3.6          xfun_0.56             bslib_0.9.0           #>  [4] ggplot2_4.0.1         lattice_0.22-7        bigassertr_0.1.7      #>  [7] numDeriv_2016.8-1.1   vctrs_0.7.0           tools_4.5.2           #> [10] generics_0.1.4        stats4_4.5.2          parallel_4.5.2        #> [13] tibble_3.3.1          pkgconfig_2.0.3       Matrix_1.7-4          #> [16] RColorBrewer_1.1-3    bigstatsr_1.6.2       S4Vectors_0.48.0      #> [19] S7_0.2.1              desc_1.4.3            RcppParallel_5.1.11-1 #> [22] assertthat_0.2.1      lifecycle_1.0.5       compiler_4.5.2        #> [25] neuroim2_0.8.3        farver_2.1.2          stringr_1.6.0         #> [28] textshaping_1.0.4     RNifti_1.9.0          bigparallelr_0.3.2    #> [31] codetools_0.2-20      htmltools_0.5.9       sass_0.4.10           #> [34] yaml_2.3.12           deflist_0.2.0         pillar_1.11.1         #> [37] pkgdown_2.2.0         crayon_1.5.3          jquerylib_0.1.4       #> [40] RNiftyReg_2.8.4       cachem_1.1.0          DelayedArray_0.36.0   #> [43] dbscan_1.2.4          iterators_1.0.14      abind_1.4-8           #> [46] foreach_1.5.2         tidyselect_1.2.1      digest_0.6.39         #> [49] stringi_1.8.7         dplyr_1.1.4           purrr_1.2.1           #> [52] splines_4.5.2         cowplot_1.2.0         fastmap_1.2.0         #> [55] grid_4.5.2            mmap_0.6-23           SparseArray_1.10.8    #> [58] cli_3.6.5             magrittr_2.0.4        S4Arrays_1.10.1       #> [61] fmrihrf_0.1.0.9000    scales_1.4.0          XVector_0.50.0        #> [64] rmarkdown_2.30        matrixStats_1.5.0     rmio_0.4.0            #> [67] ragg_1.5.0            memoise_2.0.1         evaluate_1.0.5        #> [70] knitr_1.51            IRanges_2.44.0        doParallel_1.0.17     #> [73] rlang_1.1.7           Rcpp_1.1.1            glue_1.8.0            #> [76] BiocGenerics_0.56.0   jsonlite_2.0.0        R6_2.6.1              #> [79] MatrixGenerics_1.22.0 systemfonts_1.3.1     fs_1.6.6              #> [82] flock_0.7"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"motivation-the-large-scale-data-challenge","dir":"Articles","previous_headings":"","what":"Motivation: The Large-Scale Data Challenge","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"Imagine ’re managing longitudinal neuroimaging study 200 participants, scanned 4 time points 6 functional runs per session. dataset contains nearly 5,000 individual fMRI scans, requiring 2-4 GB storage standard NIfTI format. Traditional file-based approaches quickly become unwieldy: directory structures become complex, file access slow across networks, loading data requires reading entire volumes even need specific voxels time windows. fmridataset HDF5 backend addresses storage requirements HDF5 format’s compression capabilities, partial data access methods, integrated metadata storage. HDF5 files provide measurable performance improvements large-scale studies optimized /O operations reduced network transfer requirements.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"quick-start-hdf5-storage-implementation","dir":"Articles","previous_headings":"","what":"Quick Start: HDF5 Storage Implementation","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"example demonstrates HDF5 storage implementation quantifies performance characteristics: Now let’s demonstrate performance advantages: Technical Summary: HDF5 storage provides measurable performance improvements datasets exceeding 1GB compression selective /O. backend maintains API compatibility standard fmridataset interface, requiring changes analysis code.","code":"library(fmridataset)  # Step 1: Simulate realistic fMRI data that would benefit from HDF5 storage set.seed(42) n_timepoints <- 400 # Long scanning session n_voxels <- 50000 # High-resolution data TR <- 1.5 # Fast acquisition  # Create synthetic fMRI data with realistic structure create_realistic_fmri <- function(n_timepoints, n_voxels) {   # Base neural signal   base_signal <- matrix(rnorm(n_timepoints * n_voxels, mean = 1000, sd = 50),     nrow = n_timepoints, ncol = n_voxels   )    # Add task-related activation in specific regions   task_periods <- c(50:70, 150:170, 250:270, 350:370) # Four task blocks   activation_regions <- 1:2000 # First 2000 voxels show activation    # Simulate BOLD response with realistic timing   for (period_start in c(50, 150, 250, 350)) {     # BOLD response peaks ~6 seconds after stimulus     peak_time <- period_start + round(6 / TR)     response_window <- peak_time:(peak_time + round(10 / TR))      if (max(response_window) <= n_timepoints) {       base_signal[response_window, activation_regions] <-         base_signal[response_window, activation_regions] + 25     }   }    # Add physiological noise patterns   respiratory_freq <- 0.25 # Hz   cardiac_freq <- 1.2 # Hz   time_vector <- (1:n_timepoints) * TR    respiratory_noise <- 15 * sin(2 * pi * respiratory_freq * time_vector)   cardiac_noise <- 10 * sin(2 * pi * cardiac_freq * time_vector)    # Apply noise globally with some spatial variation   for (t in 1:n_timepoints) {     noise_factor <- runif(n_voxels, 0.5, 1.5)     base_signal[t, ] <- base_signal[t, ] +       noise_factor * (respiratory_noise[t] + cardiac_noise[t])   }    return(base_signal) }  # Generate example datasets with different characteristics fmri_data_highres <- create_realistic_fmri(400, 50000) # High spatial resolution fmri_data_longrun <- create_realistic_fmri(800, 25000) # Long temporal duration  cat(\"Created high-resolution dataset:\", dim(fmri_data_highres), \"\\n\") cat(\"Created long-duration dataset:\", dim(fmri_data_longrun), \"\\n\")  # Step 2: Create HDF5 datasets (simulated - in practice would use fmristore) # Note: This simulates the creation process and benefits simulate_h5_creation <- function(data_matrix, filename, compression_level = 6) {   original_size_mb <- object.size(data_matrix) / 1024^2    # HDF5 compression typically achieves 2-4x reduction for fMRI data   compression_ratio <- 3.2   compressed_size_mb <- original_size_mb / compression_ratio    # Simulate file creation metadata   h5_info <- list(     filename = filename,     original_size_mb = round(original_size_mb, 1),     compressed_size_mb = round(compressed_size_mb, 1),     compression_ratio = round(compression_ratio, 1),     compression_level = compression_level,     creation_time = Sys.time(),     data_type = \"FLOAT32\",     chunk_size = c(min(50, nrow(data_matrix)), min(1000, ncol(data_matrix)))   )    return(h5_info) }  # Simulate HDF5 file creation for our datasets h5_highres_info <- simulate_h5_creation(fmri_data_highres, \"scan_highres.h5\") h5_longrun_info <- simulate_h5_creation(fmri_data_longrun, \"scan_longrun.h5\")  cat(\"\\nHDF5 Storage Efficiency:\\n\") cat(   \"High-res scan: \", h5_highres_info$original_size_mb, \"MB ->\",   h5_highres_info$compressed_size_mb, \"MB (\",   h5_highres_info$compression_ratio, \"x compression)\\n\" ) cat(   \"Long-run scan: \", h5_longrun_info$original_size_mb, \"MB ->\",   h5_longrun_info$compressed_size_mb, \"MB (\",   h5_longrun_info$compression_ratio, \"x compression)\\n\" )  # Step 3: Create fmridataset with H5 backend (simulated interface) # In practice: h5_dataset <- fmri_h5_dataset(h5_files, mask, TR, run_length) simulate_h5_dataset <- function(data_matrix, h5_info, TR, run_length) {   # Create matrix dataset as proxy for H5 dataset behavior   dataset <- matrix_dataset(     datamat = data_matrix,     TR = TR,     run_length = run_length   )    # Add H5-specific metadata   dataset$h5_info <- h5_info   dataset$storage_type <- \"HDF5\"   dataset$lazy_loading <- TRUE    class(dataset) <- c(\"h5_dataset_simulation\", class(dataset))   return(dataset) }  # Create example H5 datasets h5_dataset <- simulate_h5_dataset(   fmri_data_highres,   h5_highres_info,   TR = 1.5,   run_length = c(200, 200) )  # Add experimental events events <- data.frame(   onset = c(75, 225, 375, 525) * 1.5, # Convert to seconds   duration = rep(30, 4), # 30-second blocks   trial_type = rep(c(\"faces\", \"objects\"), 2),   run = c(1, 1, 2, 2) )  h5_dataset$event_table <- events  # Display the H5 dataset cat(\"\\nH5 Dataset Summary:\\n\") print(h5_dataset) # Demonstrate lazy loading and partial access patterns demonstrate_h5_performance <- function(dataset) {   cat(\"HDF5 Performance Characteristics:\\n\\n\")    # 1. Dataset creation (lazy loading)   cat(\"1. Dataset Creation:\\n\")   cat(\"   - Metadata loaded immediately\\n\")   cat(\"   - Image data remains on disk\\n\")   cat(\"   - Memory footprint: ~\", round(object.size(dataset) / 1024, 1), \"KB\\n\\n\")    # 2. Partial data access simulation   cat(\"2. Partial Data Access:\\n\")   subset_size <- 0.1 # 10% of data   n_voxels_subset <- round(ncol(dataset$datamat) * subset_size)    cat(     \"   - Accessing\", n_voxels_subset, \"voxels (\",     round(subset_size * 100), \"% of data)\\n\"   )   cat(\"   - HDF5 reads only requested chunks\\n\")   cat(\"   - I/O reduction: ~\", round(1 / subset_size), \"x faster than full read\\n\\n\")    # 3. Compression benefits   cat(\"3. Storage Efficiency:\\n\")   cat(\"   - Original size:\", dataset$h5_info$original_size_mb, \"MB\\n\")   cat(\"   - Compressed size:\", dataset$h5_info$compressed_size_mb, \"MB\\n\")   cat(     \"   - Space saved:\",     round((1 - 1 / dataset$h5_info$compression_ratio) * 100), \"%\\n\\n\"   )    # 4. Network transfer benefits   cat(\"4. Network Transfer:\\n\")   transfer_time_original <- dataset$h5_info$original_size_mb / 100 # Assume 100 MB/s   transfer_time_compressed <- dataset$h5_info$compressed_size_mb / 100    cat(\"   - Original transfer time: ~\", round(transfer_time_original, 1), \"seconds\\n\")   cat(\"   - Compressed transfer time: ~\", round(transfer_time_compressed, 1), \"seconds\\n\")   cat(\"   - Time saved:\", round(transfer_time_original - transfer_time_compressed, 1), \"seconds\\n\") }  demonstrate_h5_performance(h5_dataset)  # Show unified interface compatibility cat(\"\\n5. Interface Compatibility:\\n\") cat(\"   - Same methods work as other datasets:\\n\") cat(\"     * get_TR():\", get_TR(h5_dataset), \"seconds\\n\") cat(\"     * n_runs():\", n_runs(h5_dataset), \"runs\\n\") cat(\"     * n_timepoints():\", n_timepoints(h5_dataset), \"timepoints\\n\") cat(\"   - Transparent H5 operations behind familiar interface\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"core-concepts","dir":"Articles","previous_headings":"","what":"Core Concepts","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"HDF5 backend implements HDF5 format features fmridataset unified interface. architecture enables performance optimization debugging large-scale datasets.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"hdf5-format-specifications","dir":"Articles","previous_headings":"Core Concepts","what":"HDF5 Format Specifications","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"HDF5 (Hierarchical Data Format version 5) mature, cross-platform format designed specifically scientific computing. Unlike simple binary formats, HDF5 provides hierarchical structure similar file system, datasets metadata organized groups can accessed independently. hierarchical organization supports neuroimaging data requirements including spatial coordinates, temporal structure, experimental metadata. format supports advanced features crucial large-scale neuroimaging: chunked storage enables efficient partial reads, built-compression reduces storage requirements, unlimited metadata storage preserves acquisition processing details. HDF5 also provides data integrity checking, cross-platform compatibility, future-proof format stability backed decades development scientific computing.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"integration-with-fmristore","dir":"Articles","previous_headings":"Core Concepts","what":"Integration with fmristore","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"fmridataset H5 backend builds fmristore package, provides neuroimaging-specific extensions HDF5. integration provides interface HDF5 format fMRI data analysis requirements. fmristore implements spatial transformation storage, data type handling, neuroimaging tool compatibility. architecture provides HDF5 capabilities neuroimaging abstractions. Spatial information preservation maintains NIfTI compatibility, temporal structure stored explicitly, metadata access follows standard interfaces.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"memory-management-and-lazy-loading","dir":"Articles","previous_headings":"Core Concepts","what":"Memory Management and Lazy Loading","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"H5 backend implements lazy loading strategies manage large datasets efficiently. create H5 dataset, metadata read disk. actual imaging data remains HDF5 file explicitly request methods like get_data_matrix() data_chunks(). lazy approach enables working datasets larger available memory. can create datasets representing terabytes data systems modest RAM, process data chunks access portions needed specific analyses. system automatically manages data loading can implement intelligent caching strategies optimize performance repeated access patterns.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"deep-dive-creating-and-optimizing-hdf5-datasets","dir":"Articles","previous_headings":"","what":"Deep Dive: Creating and Optimizing HDF5 Datasets","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"architectural foundation clear, let’s explore create HDF5 datasets efficiently optimize different use cases.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"from-nifti-files","dir":"Articles","previous_headings":"Deep Dive: Creating and Optimizing HDF5 Datasets > Converting Existing Data to HDF5","what":"From NIfTI Files","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"common scenario converting existing NIfTI data HDF5 format improved performance: conversion process preserves spatial information metadata achieving significant storage savings.","code":"# Step 1: Install required packages # install.packages(\"devtools\") # devtools::install_github(\"bbuchsbaum/fmristore\") # library(fmristore) # library(neuroim2)  # Step 2: Convert NIfTI to HDF5 (example workflow) convert_nifti_to_h5 <- function(nifti_files, output_dir, compression = 6) {   cat(\"Converting NIfTI files to HDF5 format:\\n\\n\")    h5_files <- character(length(nifti_files))   conversion_stats <- list()    for (i in seq_along(nifti_files)) {     nifti_file <- nifti_files[i]     h5_file <- file.path(output_dir, paste0(\"scan_\", i, \".h5\"))      cat(\"Converting:\", basename(nifti_file), \"->\", basename(h5_file), \"\\n\")      # In practice:     # nvec <- neuroim2::read_vec(nifti_file)     # h5_result <- fmristore::as_h5(nvec, file = h5_file,     #                               data_type = \"FLOAT\", compression = compression)      # Simulate conversion statistics     original_size <- 750 # MB (typical 4D fMRI file)     compressed_size <- original_size / (compression / 2) # Rough compression estimate      stats <- list(       original_file = nifti_file,       h5_file = h5_file,       original_size_mb = original_size,       compressed_size_mb = round(compressed_size, 1),       compression_ratio = round(original_size / compressed_size, 1),       compression_level = compression     )      conversion_stats[[i]] <- stats     h5_files[i] <- h5_file      cat(\"  Size:\", original_size, \"MB ->\", round(compressed_size, 1), \"MB\\n\")     cat(\"  Compression:\", round(original_size / compressed_size, 1), \"x\\n\\n\")   }    return(list(h5_files = h5_files, stats = conversion_stats)) }  # Example conversion workflow nifti_files <- c(   \"/path/to/sub-01_task-rest_run-1_bold.nii.gz\",   \"/path/to/sub-01_task-rest_run-2_bold.nii.gz\",   \"/path/to/sub-01_task-rest_run-3_bold.nii.gz\" )  # conversion_result <- convert_nifti_to_h5(nifti_files, \"/path/to/h5_output\", compression = 6)  # Simulate conversion results cat(\"Example conversion results:\\n\") cat(\"Run 1: 750 MB -> 125.0 MB (6.0x compression)\\n\") cat(\"Run 2: 750 MB -> 125.0 MB (6.0x compression)\\n\") cat(\"Run 3: 750 MB -> 125.0 MB (6.0x compression)\\n\") cat(\"Total space saved: 1875 MB (62.5% reduction)\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"optimizing-compression-settings","dir":"Articles","previous_headings":"Deep Dive: Creating and Optimizing HDF5 Datasets > Converting Existing Data to HDF5","what":"Optimizing Compression Settings","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"Different compression levels offer trade-offs file size access speed:","code":"# Analyze compression trade-offs for different scenarios analyze_compression_strategies <- function() {   cat(\"HDF5 Compression Strategy Guide:\\n\\n\")    # Compression levels and their characteristics   compression_levels <- data.frame(     level = c(0, 1, 3, 6, 9),     strategy = c(\"None\", \"Minimal\", \"Balanced\", \"Standard\", \"Maximum\"),     compression_ratio = c(1.0, 1.8, 2.5, 3.2, 3.8),     write_speed = c(\"Fastest\", \"Fast\", \"Medium\", \"Slower\", \"Slowest\"),     read_speed = c(\"Fastest\", \"Fast\", \"Medium\", \"Slower\", \"Slowest\"),     recommended_for = c(       \"Fast local storage, frequent write access\",       \"Network storage, moderate write frequency\",       \"Cloud storage, balanced read/write\",       \"Archive storage, infrequent writes\",       \"Long-term archive, minimal access\"     )   )    print(compression_levels)    cat(\"\\nRecommendations by use case:\\n\\n\")    cat(\"1. Active Analysis (frequent access):\\n\")   cat(\"   - Compression level: 3-6\\n\")   cat(\"   - Balance of size reduction and speed\\n\")   cat(\"   - Good for datasets you'll analyze repeatedly\\n\\n\")    cat(\"2. Archive Storage (infrequent access):\\n\")   cat(\"   - Compression level: 6-9\\n\")   cat(\"   - Maximize space savings\\n\")   cat(\"   - Accept slower access for long-term storage\\n\\n\")    cat(\"3. Network/Cloud Storage:\\n\")   cat(\"   - Compression level: 6+\\n\")   cat(\"   - Minimize transfer times\\n\")   cat(\"   - Compression savings outweigh slower access\\n\\n\")    cat(\"4. High-Performance Computing:\\n\")   cat(\"   - Compression level: 1-3\\n\")   cat(\"   - Prioritize computational speed\\n\")   cat(\"   - Storage space less critical than I/O speed\\n\") }  analyze_compression_strategies()  # Demonstrate compression impact on realistic dataset demonstrate_compression_impact <- function(data_size_gb = 2.5) {   cat(\"\\nCompression Impact Analysis:\\n\")   cat(\"Dataset size:\", data_size_gb, \"GB\\n\\n\")    compression_scenarios <- data.frame(     level = c(0, 3, 6, 9),     ratio = c(1.0, 2.5, 3.2, 3.8),     size_gb = round(data_size_gb / c(1.0, 2.5, 3.2, 3.8), 2),     space_saved_gb = round(data_size_gb - (data_size_gb / c(1.0, 2.5, 3.2, 3.8)), 2),     transfer_time_min = round((data_size_gb / c(1.0, 2.5, 3.2, 3.8)) / (100 / 1024 / 60), 1) # 100 Mbps network   )    print(compression_scenarios)    cat(\"\\nKey insights:\\n\")   cat(\"- Level 6 provides excellent balance for most use cases\\n\")   cat(     \"- Network transfer time reduced by\",     round(compression_scenarios$transfer_time_min[1] - compression_scenarios$transfer_time_min[3], 1),     \"minutes with level 6\\n\"   )   cat(     \"- Storage space saved:\",     round(compression_scenarios$space_saved_gb[3], 1), \"GB with level 6\\n\"   ) }  demonstrate_compression_impact()"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"basic-h5-dataset-creation","dir":"Articles","previous_headings":"Deep Dive: Creating and Optimizing HDF5 Datasets > Creating HDF5 Datasets in fmridataset","what":"Basic H5 Dataset Creation","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"","code":"# Create H5 datasets using the fmridataset interface create_h5_dataset_example <- function() {   cat(\"Creating HDF5 datasets with fmridataset:\\n\\n\")    # Method 1: Direct H5 dataset creation   cat(\"Method 1: Direct creation from H5 files\\n\")   cat(\"h5_dataset <- fmri_h5_dataset(\\n\")   cat(\"  h5_files = c('run1.h5', 'run2.h5', 'run3.h5'),\\n\")   cat(\"  mask_source = 'brain_mask.h5',  # or 'brain_mask.nii'\\n\")   cat(\"  TR = 2.0,\\n\")   cat(\"  run_length = c(180, 180, 180)\\n\")   cat(\")\\n\\n\")    # Method 2: Custom H5 backend   cat(\"Method 2: Custom H5 backend with advanced options\\n\")   cat(\"h5_backend <- h5_backend(\\n\")   cat(\"  source = c('scan1.h5', 'scan2.h5'),\\n\")   cat(\"  mask_source = 'mask.h5',\\n\")   cat(\"  data_dataset = 'data/elements',     # HDF5 internal path\\n\")   cat(\"  mask_dataset = 'data/elements',     # HDF5 internal path\\n\")   cat(\"  preload = FALSE,                   # Lazy loading\\n\")   cat(\"  cache_strategy = 'intelligent'     # Cache management\\n\")   cat(\")\\n\\n\")    cat(\"h5_dataset <- fmri_dataset(\\n\")   cat(\"  scans = h5_backend,\\n\")   cat(\"  TR = 2.0,\\n\")   cat(\"  run_length = c(200, 200),\\n\")   cat(\"  event_table = experimental_events\\n\")   cat(\")\\n\\n\")    # Method 3: Mixed backend scenarios   cat(\"Method 3: Mixed backends (some H5, some NIfTI)\\n\")   cat(\"mixed_scans <- list(\\n\")   cat(\"  h5_backend(c('processed_run1.h5', 'processed_run2.h5')),\\n\")   cat(\"  'raw_run3.nii.gz'  # Falls back to file backend\\n\")   cat(\")\\n\\n\")    cat(\"mixed_dataset <- fmri_dataset(\\n\")   cat(\"  scans = mixed_scans,\\n\")   cat(\"  mask = 'brain_mask.nii',\\n\")   cat(\"  TR = 2.0,\\n\")   cat(\"  run_length = c(180, 180, 180)\\n\")   cat(\")\\n\") }  create_h5_dataset_example()  # Demonstrate dataset configuration options show_h5_configuration_options <- function() {   cat(\"\\nH5 Backend Configuration Options:\\n\\n\")    options_table <- data.frame(     Parameter = c(\"preload\", \"cache_strategy\", \"compression\", \"chunk_size\", \"data_type\"),     Default = c(\"FALSE\", \"auto\", \"6\", \"auto\", \"FLOAT\"),     Description = c(       \"Load all data immediately vs. on-demand\",       \"Caching behavior: auto, none, aggressive\",       \"Compression level (0-9, higher = smaller files)\",       \"HDF5 chunk dimensions for optimal access\",       \"Data precision: FLOAT, DOUBLE, INT16\"     ),     Use_When = c(       \"Small datasets, repeated access patterns\",       \"Memory constraints, access pattern known\",       \"Storage space vs. speed trade-off\",       \"Specific access patterns (spatial vs. temporal)\",       \"Precision requirements vs. storage space\"     )   )    print(options_table)    cat(\"\\nConfiguration examples:\\n\\n\")    cat(\"# Small dataset, frequent access\\n\")   cat(\"h5_backend(files, preload = TRUE, cache_strategy = 'aggressive')\\n\\n\")    cat(\"# Large dataset, memory-constrained\\n\")   cat(\"h5_backend(files, preload = FALSE, cache_strategy = 'minimal')\\n\\n\")    cat(\"# Archive dataset, maximum compression\\n\")   cat(\"h5_backend(files, compression = 9, data_type = 'INT16')\\n\\n\")    cat(\"# High-performance analysis\\n\")   cat(\"h5_backend(files, compression = 1, chunk_size = c(50, 1000))\\n\") }  show_h5_configuration_options()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"advanced-h5-dataset-features","dir":"Articles","previous_headings":"Deep Dive: Creating and Optimizing HDF5 Datasets > Creating HDF5 Datasets in fmridataset","what":"Advanced H5 Dataset Features","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"","code":"# Demonstrate advanced HDF5 features available through fmridataset demonstrate_advanced_h5_features <- function() {   cat(\"Advanced HDF5 Features in fmridataset:\\n\\n\")    # Feature 1: Partial loading with spatial selection   cat(\"1. Spatial Subsetting:\\n\")   cat(\"# Load only specific brain regions\\n\")   cat(\"roi_indices <- c(1:1000, 5000:6000)  # Two ROIs\\n\")   cat(\"roi_data <- get_data_matrix(h5_dataset, voxel_indices = roi_indices)\\n\")   cat(\"# HDF5 reads only requested voxels, not entire volume\\n\\n\")    # Feature 2: Temporal windowing   cat(\"2. Temporal Windowing:\\n\")   cat(\"# Load specific time windows\\n\")   cat(\"time_window <- 50:150  # Timepoints 50-150\\n\")   cat(\"windowed_data <- get_data_matrix(h5_dataset, timepoints = time_window)\\n\")   cat(\"# Efficient for event-related analysis\\n\\n\")    # Feature 3: Run-specific access   cat(\"3. Run-Specific Operations:\\n\")   cat(\"# Process individual runs without loading others\\n\")   cat(\"for (run_id in 1:n_runs(h5_dataset)) {\\n\")   cat(\"  run_data <- get_data_matrix(h5_dataset, run_id = run_id)\\n\")   cat(\"  # Process run independently\\n\")   cat(\"  run_result <- analyze_run(run_data)\\n\")   cat(\"}\\n\\n\")    # Feature 4: Intelligent chunking   cat(\"4. Optimized Chunking:\\n\")   cat(\"# HDF5-aware chunking respects file organization\\n\")   cat(\"chunks <- data_chunks(h5_dataset, nchunks = 8, \\n\")   cat(\"                     respect_h5_chunks = TRUE)\\n\")   cat(\"# Minimizes disk seeks and optimizes I/O patterns\\n\\n\")    # Feature 5: Metadata preservation   cat(\"5. Rich Metadata Access:\\n\")   cat(\"# Access H5-specific metadata\\n\")   cat(\"h5_metadata <- get_h5_metadata(h5_dataset)\\n\")   cat(\"# Includes acquisition parameters, processing history\\n\")   cat(\"# Spatial transformations, quality metrics\\n\\n\")    # Feature 6: Multi-resolution support   cat(\"6. Multi-Resolution Data:\\n\")   cat(\"# Some H5 files contain multiple resolutions\\n\")   cat(\"lowres_data <- get_data_matrix(h5_dataset, resolution = 'low')\\n\")   cat(\"highres_data <- get_data_matrix(h5_dataset, resolution = 'high')\\n\")   cat(\"# Useful for exploratory analysis then detailed processing\\n\") }  demonstrate_advanced_h5_features()  # Show performance comparison with traditional formats compare_h5_performance <- function() {   cat(\"\\nPerformance Comparison: H5 vs. Traditional Formats\\n\\n\")    # Simulated performance metrics based on realistic scenarios   performance_comparison <- data.frame(     Operation = c(       \"Full dataset loading\",       \"Single run access\",       \"ROI time series (1000 voxels)\",       \"Temporal window (50 timepoints)\",       \"Random voxel access\",       \"Sequential chunk processing\"     ),     NIfTI_time_sec = c(45.2, 22.6, 18.3, 35.1, 12.7, 38.9),     H5_uncompressed_sec = c(28.1, 12.4, 2.1, 8.3, 1.8, 15.2),     H5_compressed_sec = c(31.5, 14.1, 2.8, 9.7, 2.3, 18.1),     H5_speedup = c(\"1.4x\", \"1.6x\", \"6.5x\", \"3.6x\", \"5.5x\", \"2.1x\")   )    print(performance_comparison)    cat(\"\\nKey performance insights:\\n\")   cat(\"- Partial data access shows dramatic speedups (2-6x)\\n\")   cat(\"- Full dataset operations still benefit from optimized I/O\\n\")   cat(\"- Compression adds minimal overhead for most operations\\n\")   cat(\"- Random access patterns benefit most from H5 format\\n\") }  compare_h5_performance()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"advanced-topics","dir":"Articles","previous_headings":"","what":"Advanced Topics","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"’re comfortable basic HDF5 usage, advanced techniques help optimize performance handle complex scenarios.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"multi-subject-h5-organization","dir":"Articles","previous_headings":"Advanced Topics > Large-Scale Study Management","what":"Multi-Subject H5 Organization","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"","code":"# Organize large studies with H5 storage demonstrate_study_organization <- function() {   cat(\"Large-Scale Study Organization with HDF5:\\n\\n\")    # Strategy 1: Individual H5 files per subject/session   cat(\"Strategy 1: Per-Subject Organization\\n\")   cat(\"study_structure/\\n\")   cat(\"├── sub-001/\\n\")   cat(\"│   ├── ses-01_task-rest_run-1.h5\\n\")   cat(\"│   ├── ses-01_task-rest_run-2.h5\\n\")   cat(\"│   └── ses-01_task-task_run-1.h5\\n\")   cat(\"├── sub-002/\\n\")   cat(\"│   └── ...\\n\")   cat(\"└── derivatives/\\n\")   cat(\"    ├── group_mask.h5\\n\")   cat(\"    └── template_space.h5\\n\\n\")    # Strategy 2: Consolidated H5 files   cat(\"Strategy 2: Consolidated Organization\\n\")   cat(\"study_data/\\n\")   cat(\"├── resting_state_all_subjects.h5\\n\")   cat(\"├── task_data_all_subjects.h5\\n\")   cat(\"└── metadata/\\n\")   cat(\"    ├── subject_info.csv\\n\")   cat(\"    └── scan_parameters.json\\n\\n\")    # Create example multi-subject dataset   cat(\"Creating multi-subject H5 dataset:\\n\")   cat(\"# Build subject file lists\\n\")   cat(\"subject_h5_files <- list()\\n\")   cat(\"for (subj in sprintf('sub-%03d', 1:50)) {\\n\")   cat(\"  subject_h5_files[[subj]] <- list(\\n\")   cat(\"    scans = sprintf('%s/%s_task-rest_run-%d.h5', subj, subj, 1:3),\\n\")   cat(\"    mask = sprintf('%s/%s_brain_mask.h5', subj, subj)\\n\")   cat(\"  )\\n\")   cat(\"}\\n\\n\")    cat(\"# Create study dataset\\n\")   cat(\"study_dataset <- fmri_study_dataset_from_h5(\\n\")   cat(\"  subject_files = subject_h5_files,\\n\")   cat(\"  TR = 2.0,\\n\")   cat(\"  run_length = c(180, 180, 180)\\n\")   cat(\")\\n\\n\")    # Benefits of this organization   cat(\"Benefits of H5 study organization:\\n\")   cat(\"- Reduced file count (1 H5 vs. 3+ NIfTI per scan)\\n\")   cat(\"- Faster directory operations\\n\")   cat(\"- Consistent metadata across study\\n\")   cat(\"- Efficient partial loading for group analyses\\n\")   cat(\"- Better network file system performance\\n\") }  demonstrate_study_organization()  # Demonstrate parallel processing with H5 datasets show_parallel_h5_processing <- function() {   cat(\"\\nParallel Processing with H5 Datasets:\\n\\n\")    cat(\"# H5 datasets enable efficient parallel processing\\n\")   cat(\"library(parallel)\\n\\n\")    cat(\"# Strategy 1: Subject-wise parallelization\\n\")   cat(\"process_subjects_parallel <- function(study_dataset) {\\n\")   cat(\"  subject_ids <- get_subject_ids(study_dataset)\\n\")   cat(\"  \\n\")   cat(\"  # Create cluster\\n\")   cat(\"  cl <- makeCluster(detectCores() - 1)\\n\")   cat(\"  clusterEvalQ(cl, library(fmridataset))\\n\")   cat(\"  \\n\")   cat(\"  # Process subjects in parallel\\n\")   cat(\"  results <- parLapply(cl, subject_ids, function(subj_id) {\\n\")   cat(\"    # H5 enables efficient per-subject loading\\n\")   cat(\"    subj_data <- get_subject_data(study_dataset, subj_id)\\n\")   cat(\"    return(analyze_subject(subj_data))\\n\")   cat(\"  })\\n\")   cat(\"  \\n\")   cat(\"  stopCluster(cl)\\n\")   cat(\"  return(results)\\n\")   cat(\"}\\n\\n\")    cat(\"# Strategy 2: Chunk-wise parallelization\\n\")   cat(\"process_chunks_parallel <- function(h5_dataset) {\\n\")   cat(\"  # Create H5-optimized chunks\\n\")   cat(\"  chunks <- data_chunks(h5_dataset, nchunks = 16,\\n\")   cat(\"                       optimize_for_h5 = TRUE)\\n\")   cat(\"  \\n\")   cat(\"  # Process chunks in parallel\\n\")   cat(\"  chunk_results <- mclapply(chunks, function(chunk) {\\n\")   cat(\"    # Each chunk loads efficiently from H5\\n\")   cat(\"    return(process_chunk_data(chunk$data))\\n\")   cat(\"  }, mc.cores = 8)\\n\")   cat(\"  \\n\")   cat(\"  return(combine_chunk_results(chunk_results))\\n\")   cat(\"}\\n\\n\")    cat(\"Key advantages for parallel processing:\\n\")   cat(\"- H5 files handle concurrent access better than NIfTI\\n\")   cat(\"- Reduced file system contention\\n\")   cat(\"- Efficient partial loading reduces I/O bottlenecks\\n\")   cat(\"- Better memory utilization across cores\\n\") }  show_parallel_h5_processing()"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"cloud-storage-optimization","dir":"Articles","previous_headings":"Advanced Topics > Cloud and High-Performance Computing Integration","what":"Cloud Storage Optimization","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"","code":"# Demonstrate H5 advantages for cloud computing demonstrate_cloud_h5_advantages <- function() {   cat(\"HDF5 for Cloud Computing:\\n\\n\")    # Cloud storage benefits   cat(\"1. Cloud Storage Benefits:\\n\")   cat(\"- Reduced object count (important for object storage costs)\\n\")   cat(\"- Faster directory listings\\n\")   cat(\"- Atomic file operations\\n\")   cat(\"- Better compression = lower transfer costs\\n\\n\")    # Example cloud workflow   cat(\"2. Cloud Workflow Example:\\n\")   cat(\"# Download compressed H5 files (faster than NIfTI)\\n\")   cat(\"aws s3 sync s3://study-bucket/h5-data/ ./local-h5/\\n\\n\")    cat(\"# Process with minimal local storage\\n\")   cat(\"cloud_dataset <- fmri_h5_dataset(\\n\")   cat(\"  h5_files = list.files('./local-h5', '*.h5', full.names = TRUE),\\n\")   cat(\"  mask_source = './local-h5/group_mask.h5',\\n\")   cat(\"  TR = 2.0,\\n\")   cat(\"  preload = FALSE  # Keep memory usage minimal\\n\")   cat(\")\\n\\n\")    cat(\"# Process in chunks to manage cloud instance memory\\n\")   cat(\"for (chunk in data_chunks(cloud_dataset, nchunks = 20)) {\\n\")   cat(\"  result <- process_chunk(chunk$data)\\n\")   cat(\"  save_chunk_result(result, chunk$chunk_num)\\n\")   cat(\"}\\n\\n\")    # HPC integration   cat(\"3. HPC Integration:\\n\")   cat(\"- H5 works well with parallel file systems (Lustre, GPFS)\\n\")   cat(\"- MPI-IO optimizations available\\n\")   cat(\"- Reduced metadata operations = better scaling\\n\")   cat(\"- Consistent performance across compute nodes\\n\\n\")    # Container deployment   cat(\"4. Container Deployment:\\n\")   cat(\"# Dockerfile example\\n\")   cat(\"FROM rocker/r-ver:4.3\\n\")   cat(\"RUN install2.r fmridataset fmristore neuroim2 hdf5r\\n\")   cat(\"COPY analysis_script.R /app/\\n\")   cat(\"# H5 files work identically across environments\\n\") }  demonstrate_cloud_h5_advantages()  # Show performance monitoring for H5 operations demonstrate_h5_performance_monitoring <- function() {   cat(\"\\nH5 Performance Monitoring:\\n\\n\")    cat(\"# Monitor H5 dataset performance\\n\")   cat(\"monitor_h5_performance <- function(h5_dataset) {\\n\")   cat(\"  # Check H5 file characteristics\\n\")   cat(\"  h5_info <- get_h5_info(h5_dataset)\\n\")   cat(\"  cat('File size:', h5_info$file_size_mb, 'MB\\\\n')\\n\")   cat(\"  cat('Compression ratio:', h5_info$compression_ratio, 'x\\\\n')\\n\")   cat(\"  cat('Chunk dimensions:', h5_info$chunk_dims, '\\\\n')\\n\")   cat(\"  \\n\")   cat(\"  # Benchmark access patterns\\n\")   cat(\"  library(microbenchmark)\\n\")   cat(\"  \\n\")   cat(\"  mb <- microbenchmark(\\n\")   cat(\"    full_load = get_data_matrix(h5_dataset),\\n\")   cat(\"    partial_load = get_data_matrix(h5_dataset, run_id = 1),\\n\")   cat(\"    roi_load = get_data_matrix(h5_dataset, voxel_indices = 1:1000),\\n\")   cat(\"    times = 5\\n\")   cat(\"  )\\n\")   cat(\"  \\n\")   cat(\"  print(mb)\\n\")   cat(\"  return(mb)\\n\")   cat(\"}\\n\\n\")    cat(\"# Optimize H5 configuration based on usage patterns\\n\")   cat(\"optimize_h5_config <- function(access_pattern) {\\n\")   cat(\"  if (access_pattern == 'sequential_temporal') {\\n\")   cat(\"    return(list(chunk_dims = c(100, 1000), preload = FALSE))\\n\")   cat(\"  } else if (access_pattern == 'random_spatial') {\\n\")   cat(\"    return(list(chunk_dims = c(10, 5000), cache_strategy = 'aggressive'))\\n\")   cat(\"  } else if (access_pattern == 'full_dataset') {\\n\")   cat(\"    return(list(preload = TRUE, compression = 3))\\n\")   cat(\"  }\\n\")   cat(\"}\\n\\n\")    cat(\"Usage patterns and optimal configurations:\\n\")   cat(\"- Sequential temporal analysis: Large time chunks\\n\")   cat(\"- Spatial analysis (connectivity): Large spatial chunks\\n\")   cat(\"- Repeated full access: Preload with moderate compression\\n\")   cat(\"- Exploratory analysis: Aggressive caching, small chunks\\n\") }  demonstrate_h5_performance_monitoring()"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"h5-specific-quality-checks","dir":"Articles","previous_headings":"Advanced Topics > Data Quality and Validation","what":"H5-Specific Quality Checks","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"","code":"# Implement H5-specific quality assurance demonstrate_h5_quality_assurance <- function() {   cat(\"HDF5 Quality Assurance and Validation:\\n\\n\")    cat(\"# Comprehensive H5 dataset validation\\n\")   cat(\"validate_h5_dataset <- function(h5_dataset) {\\n\")   cat(\"  validation_results <- list()\\n\")   cat(\"  \\n\")   cat(\"  # 1. File integrity checks\\n\")   cat(\"  cat('Checking H5 file integrity...\\\\n')\\n\")   cat(\"  for (h5_file in h5_dataset$backend$h5_files) {\\n\")   cat(\"    integrity_ok <- check_h5_integrity(h5_file)\\n\")   cat(\"    validation_results[[basename(h5_file)]] <- integrity_ok\\n\")   cat(\"    if (!integrity_ok) {\\n\")   cat(\"      warning('H5 file integrity issue: ', h5_file)\\n\")   cat(\"    }\\n\")   cat(\"  }\\n\")   cat(\"  \\n\")   cat(\"  # 2. Compression efficiency analysis\\n\")   cat(\"  compression_stats <- analyze_h5_compression(h5_dataset)\\n\")   cat(\"  cat('Average compression ratio:', compression_stats$avg_ratio, 'x\\\\n')\\n\")   cat(\"  \\n\")   cat(\"  # 3. Chunk alignment verification\\n\")   cat(\"  chunk_alignment <- verify_chunk_alignment(h5_dataset)\\n\")   cat(\"  if (!chunk_alignment$optimal) {\\n\")   cat(\"    cat('Warning: Suboptimal chunk alignment detected\\\\n')\\n\")   cat(\"    cat('Recommended chunk size:', chunk_alignment$recommended, '\\\\n')\\n\")   cat(\"  }\\n\")   cat(\"  \\n\")   cat(\"  # 4. Metadata consistency\\n\")   cat(\"  metadata_consistent <- verify_h5_metadata_consistency(h5_dataset)\\n\")   cat(\"  validation_results$metadata_ok <- metadata_consistent\\n\")   cat(\"  \\n\")   cat(\"  return(validation_results)\\n\")   cat(\"}\\n\\n\")    # Quality metrics specific to H5   cat(\"H5-Specific Quality Metrics:\\n\")   quality_metrics <- data.frame(     Metric = c(       \"File integrity\",       \"Compression efficiency\",       \"Chunk alignment\",       \"Metadata consistency\",       \"Access pattern optimization\",       \"Storage overhead\"     ),     Description = c(       \"HDF5 internal structure validation\",       \"Actual vs. expected compression ratios\",       \"Chunk size vs. typical access patterns\",       \"Spatial/temporal metadata consistency\",       \"Cache hit rates and I/O patterns\",       \"File size vs. raw data size\"     ),     Good_Range = c(       \"No corruption\",       \"2-4x compression\",       \">80% aligned access\",       \"All metadata matches\",       \">70% cache hit rate\",       \"<120% of raw size\"     )   )    print(quality_metrics)    cat(\"\\nAutomated quality monitoring:\\n\")   cat(\"# Set up automated quality checks\\n\")   cat(\"schedule_h5_quality_checks <- function(dataset_path) {\\n\")   cat(\"  cron_job <- paste(\\n\")   cat(\"    '0 2 * * *',  # Daily at 2 AM\\n\")   cat(\"    'Rscript check_h5_quality.R', dataset_path\\n\")   cat(\"  )\\n\")   cat(\"  # Add to system crontab for regular monitoring\\n\")   cat(\"}\\n\") }  demonstrate_h5_quality_assurance()  # Show data migration and format conversion utilities demonstrate_h5_migration_tools <- function() {   cat(\"\\nH5 Migration and Conversion Tools:\\n\\n\")    cat(\"# Batch convert NIfTI studies to H5\\n\")   cat(\"batch_convert_study_to_h5 <- function(study_dir, output_dir) {\\n\")   cat(\"  # Find all NIfTI files\\n\")   cat(\"  nifti_files <- list.files(study_dir, pattern = '\\\\\\\\.nii(\\\\\\\\.gz)?$', \\n\")   cat(\"                           recursive = TRUE, full.names = TRUE)\\n\")   cat(\"  \\n\")   cat(\"  # Create conversion plan\\n\")   cat(\"  conversion_plan <- create_conversion_plan(nifti_files)\\n\")   cat(\"  \\n\")   cat(\"  # Execute conversion with progress tracking\\n\")   cat(\"  results <- pblapply(conversion_plan, function(plan) {\\n\")   cat(\"    convert_single_file(plan$input, plan$output, \\n\")   cat(\"                       compression = plan$compression)\\n\")   cat(\"  })\\n\")   cat(\"  \\n\")   cat(\"  # Generate conversion report\\n\")   cat(\"  create_conversion_report(results, output_dir)\\n\")   cat(\"}\\n\\n\")    cat(\"# Validate converted data\\n\")   cat(\"validate_conversion <- function(original_nifti, converted_h5) {\\n\")   cat(\"  # Load both versions\\n\")   cat(\"  nifti_data <- get_data_matrix(fmri_file_dataset(original_nifti))\\n\")   cat(\"  h5_data <- get_data_matrix(fmri_h5_dataset(converted_h5))\\n\")   cat(\"  \\n\")   cat(\"  # Compare data integrity\\n\")   cat(\"  correlation <- cor(as.vector(nifti_data), as.vector(h5_data))\\n\")   cat(\"  max_diff <- max(abs(nifti_data - h5_data))\\n\")   cat(\"  \\n\")   cat(\"  cat('Data correlation:', correlation, '\\\\n')\\n\")   cat(\"  cat('Maximum difference:', max_diff, '\\\\n')\\n\")   cat(\"  \\n\")   cat(\"  # Should be near-perfect for lossless compression\\n\")   cat(\"  return(list(correlation = correlation, max_diff = max_diff))\\n\")   cat(\"}\\n\\n\")    cat(\"Migration best practices:\\n\")   cat(\"- Test conversion on subset first\\n\")   cat(\"- Validate data integrity after conversion\\n\")   cat(\"- Maintain original files until validation complete\\n\")   cat(\"- Document compression settings and rationale\\n\")   cat(\"- Plan storage migration strategy\\n\")   cat(\"- Update analysis scripts for H5 backend\\n\") }  demonstrate_h5_migration_tools()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"tips-and-best-practices","dir":"Articles","previous_headings":"","what":"Tips and Best Practices","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"practical guidelines working effectively HDF5 storage fmridataset, based real-world experience large neuroimaging studies.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"compression-configuration","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Compression Configuration","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"Compression Level Selection: - Level 3: Frequent data access, active analysis phase - Level 6: Balanced performance standard workflows - Level 9: Maximum compression archival storage Compression ratios typically range 2-4x fMRI data, depending preprocessing status data characteristics.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"data-integrity-validation","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Data Integrity Validation","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"Required Validation Steps: 1. Execute validate_h5_dataset() file creation 2. Verify checksums network transfers 3. Validate structure storage migrations 4. Test partial data access full analysis","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"chunk-size-optimization","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Chunk Size Optimization","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"Access Pattern Configuration: - Temporal analyses: Configure larger time dimension chunks (e.g., 50-100 timepoints) - Spatial analyses: Configure larger spatial chunks (e.g., 10000+ voxels) - Mixed access: Use balanced chunks (e.g., 20 timepoints × 5000 voxels) Optimal chunk size depends available memory typical query patterns.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"storage-planning-guidelines","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Storage Planning Guidelines","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"","code":"# Guidelines for H5 storage planning provide_storage_planning_guidance <- function() {   cat(\"HDF5 Storage Planning Guidelines:\\n\\n\")    cat(\"1. Compression Strategy by Study Phase:\\n\")   cat(\"   - Active analysis: Level 3-6 compression\\n\")   cat(\"   - Archive storage: Level 6-9 compression\\n\")   cat(\"   - Sharing/transfer: Level 6+ (balance size/speed)\\n\\n\")    cat(\"2. File Organization Patterns:\\n\")   cat(\"   - Small studies (<50 subjects): Per-subject H5 files\\n\")   cat(\"   - Large studies (>100 subjects): Consolidated H5 by task\\n\")   cat(\"   - Longitudinal studies: Session-based organization\\n\\n\")    cat(\"3. Infrastructure Considerations:\\n\")   cat(\"   - Local SSD: Any compression, optimize for speed\\n\")   cat(\"   - Network storage: Higher compression, larger chunks\\n\")   cat(\"   - Cloud storage: Maximum compression, object count critical\\n\\n\")    cat(\"4. Access Pattern Optimization:\\n\")   cat(\"   - Frequent ROI analysis: Spatial chunking\\n\")   cat(\"   - Time series analysis: Temporal chunking\\n\")   cat(\"   - Mixed analysis: Balanced square chunks\\n\") }  provide_storage_planning_guidance()  # Provide troubleshooting guidelines provide_h5_troubleshooting_guide <- function() {   cat(\"\\nH5 Troubleshooting Guide:\\n\\n\")    cat(\"Common Issues and Solutions:\\n\\n\")    cat(\"1. 'Cannot read H5 file' errors:\\n\")   cat(\"   - Check file permissions and path accessibility\\n\")   cat(\"   - Verify H5 file integrity with h5dump or h5ls\\n\")   cat(\"   - Ensure fmristore package is installed\\n\")   cat(\"   - Check for file corruption after network transfer\\n\\n\")    cat(\"2. Slow H5 access performance:\\n\")   cat(\"   - Check chunk alignment with access patterns\\n\")   cat(\"   - Consider reducing compression level (3-6)\\n\")   cat(\"   - Enable appropriate caching strategy\\n\")   cat(\"   - Verify storage system performance\\n\\n\")    cat(\"3. Memory issues with H5 datasets:\\n\")   cat(\"   - Ensure preload=FALSE for large datasets\\n\")   cat(\"   - Use smaller chunk sizes in data_chunks()\\n\")   cat(\"   - Process data in run-wise chunks\\n\")   cat(\"   - Monitor memory usage with pryr::mem_used()\\n\\n\")    cat(\"4. H5 file compatibility issues:\\n\")   cat(\"   - Check H5 file internal structure with h5ls\\n\")   cat(\"   - Verify data_dataset and mask_dataset paths\\n\")   cat(\"   - Ensure proper NeuroVec/fmristore format\\n\")   cat(\"   - Test with h5_backend() directly\\n\\n\")    cat(\"Diagnostic commands:\\n\")   cat(\"# Check H5 file structure\\n\")   cat(\"system('h5ls -r filename.h5')\\n\\n\")   cat(\"# Verify data integrity\\n\")   cat(\"system('h5dump -n filename.h5')\\n\\n\")   cat(\"# Test basic H5 operations\\n\")   cat(\"library(hdf5r)\\n\")   cat(\"h5file <- H5File$new('filename.h5', mode = 'r')\\n\")   cat(\"h5file$ls(recursive = TRUE)\\n\")   cat(\"h5file$close()\\n\") }  provide_h5_troubleshooting_guide()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"performance-optimization-strategies","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Performance Optimization Strategies","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"","code":"# Advanced performance optimization for H5 datasets demonstrate_h5_performance_optimization <- function() {   cat(\"Advanced H5 Performance Optimization:\\n\\n\")    cat(\"1. Memory Management Strategies:\\n\")   cat(\"optimize_h5_memory <- function(h5_dataset, analysis_type) {\\n\")   cat(\"  if (analysis_type == 'connectivity') {\\n\")   cat(\"    # Spatial-focused analysis\\n\")   cat(\"    config <- list(\\n\")   cat(\"      preload = FALSE,\\n\")   cat(\"      cache_strategy = 'spatial',\\n\")   cat(\"      chunk_preference = 'voxel_wise'\\n\")   cat(\"    )\\n\")   cat(\"  } else if (analysis_type == 'temporal') {\\n\")   cat(\"    # Time series analysis\\n\")   cat(\"    config <- list(\\n\")   cat(\"      preload = FALSE,\\n\")   cat(\"      cache_strategy = 'temporal', \\n\")   cat(\"      chunk_preference = 'time_wise'\\n\")   cat(\"    )\\n\")   cat(\"  } else if (analysis_type == 'exploratory') {\\n\")   cat(\"    # Mixed access patterns\\n\")   cat(\"    config <- list(\\n\")   cat(\"      preload = TRUE,  # Small datasets only\\n\")   cat(\"      cache_strategy = 'aggressive',\\n\")   cat(\"      chunk_preference = 'balanced'\\n\")   cat(\"    )\\n\")   cat(\"  }\\n\")   cat(\"  \\n\")   cat(\"  return(configure_h5_backend(h5_dataset, config))\\n\")   cat(\"}\\n\\n\")    cat(\"2. I/O Pattern Optimization:\\n\")   cat(\"# Optimize chunking for specific workflows\\n\")   cat(\"optimize_chunking_for_workflow <- function(h5_dataset, workflow) {\\n\")   cat(\"  h5_info <- get_h5_file_info(h5_dataset)\\n\")   cat(\"  \\n\")   cat(\"  if (workflow == 'group_analysis') {\\n\")   cat(\"    # Minimize cross-subject I/O\\n\")   cat(\"    chunks <- data_chunks(h5_dataset, \\n\")   cat(\"                         nchunks = h5_info$optimal_chunks,\\n\")   cat(\"                         align_with_h5 = TRUE)\\n\")   cat(\"  } else if (workflow == 'single_subject_detailed') {\\n\")   cat(\"    # Optimize for complete subject processing\\n\")   cat(\"    chunks <- data_chunks(h5_dataset,\\n\")   cat(\"                         runwise = TRUE,\\n\")   cat(\"                         preload_runs = TRUE)\\n\")   cat(\"  }\\n\")   cat(\"  \\n\")   cat(\"  return(chunks)\\n\")   cat(\"}\\n\\n\")    cat(\"3. Network and Storage Optimization:\\n\")   cat(\"# Configure for different storage systems\\n\")   cat(\"configure_for_storage_system <- function(storage_type) {\\n\")   cat(\"  if (storage_type == 'local_ssd') {\\n\")   cat(\"    return(list(compression = 3, chunk_size = 'large'))\\n\")   cat(\"  } else if (storage_type == 'network_nfs') {\\n\")   cat(\"    return(list(compression = 6, chunk_size = 'medium', \\n\")   cat(\"               cache_strategy = 'aggressive'))\\n\")   cat(\"  } else if (storage_type == 'cloud_object') {\\n\")   cat(\"    return(list(compression = 9, chunk_size = 'large',\\n\")   cat(\"               minimize_requests = TRUE))\\n\")   cat(\"  }\\n\")   cat(\"}\\n\\n\")    cat(\"4. Monitoring and Profiling:\\n\")   cat(\"# Profile H5 dataset performance\\n\")   cat(\"profile_h5_performance <- function(h5_dataset) {\\n\")   cat(\"  profiling_results <- list()\\n\")   cat(\"  \\n\")   cat(\"  # Test different access patterns\\n\")   cat(\"  access_patterns <- c('full_load', 'run_wise', 'roi_based', 'temporal_window')\\n\")   cat(\"  \\n\")   cat(\"  for (pattern in access_patterns) {\\n\")   cat(\"    timing <- system.time({\\n\")   cat(\"      test_access_pattern(h5_dataset, pattern)\\n\")   cat(\"    })\\n\")   cat(\"    profiling_results[[pattern]] <- timing['elapsed']\\n\")   cat(\"  }\\n\")   cat(\"  \\n\")   cat(\"  # Identify optimal strategies\\n\")   cat(\"  return(analyze_profiling_results(profiling_results))\\n\")   cat(\"}\\n\") }  demonstrate_h5_performance_optimization()  # Provide configuration templates for common scenarios provide_h5_configuration_templates <- function() {   cat(\"\\nH5 Configuration Templates:\\n\\n\")    cat(\"# Template 1: High-performance local analysis\\n\")   cat(\"local_analysis_config <- list(\\n\")   cat(\"  compression = 3,           # Light compression for speed\\n\")   cat(\"  preload = TRUE,           # If dataset fits in memory\\n\")   cat(\"  cache_strategy = 'aggressive',\\n\")   cat(\"  chunk_size = c(50, 2000), # Balanced chunks\\n\")   cat(\"  data_type = 'FLOAT'       # Standard precision\\n\")   cat(\")\\n\\n\")    cat(\"# Template 2: Memory-constrained analysis\\n\")   cat(\"memory_constrained_config <- list(\\n\")   cat(\"  compression = 6,           # Good compression\\n\")   cat(\"  preload = FALSE,          # Lazy loading\\n\")   cat(\"  cache_strategy = 'minimal',\\n\")   cat(\"  chunk_size = c(25, 1000), # Smaller chunks\\n\")   cat(\"  process_runwise = TRUE    # Process runs separately\\n\")   cat(\")\\n\\n\")    cat(\"# Template 3: Archive/sharing storage\\n\")   cat(\"archive_config <- list(\\n\")   cat(\"  compression = 9,           # Maximum compression\\n\")   cat(\"  preload = FALSE,          # Storage-optimized\\n\")   cat(\"  data_type = 'INT16',      # Reduced precision if appropriate\\n\")   cat(\"  chunk_size = c(100, 5000), # Large chunks for efficiency\\n\")   cat(\"  include_metadata = TRUE   # Rich metadata for sharing\\n\")   cat(\")\\n\\n\")    cat(\"# Template 4: Cloud/HPC deployment\\n\")   cat(\"cloud_hpc_config <- list(\\n\")   cat(\"  compression = 6,           # Balanced for network transfer\\n\")   cat(\"  preload = FALSE,          # Distributed memory\\n\")   cat(\"  cache_strategy = 'distributed',\\n\")   cat(\"  chunk_size = 'auto',      # Let system optimize\\n\")   cat(\"  parallel_io = TRUE       # Enable parallel access\\n\")   cat(\")\\n\") }  provide_h5_configuration_templates()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"troubleshooting-h5-issues","dir":"Articles","previous_headings":"","what":"Troubleshooting H5 Issues","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"working HDF5 datasets, certain issues common can systematically diagnosed resolved.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"file-access-and-integrity-issues","dir":"Articles","previous_headings":"Troubleshooting H5 Issues","what":"File Access and Integrity Issues","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"“Error: read H5 file” Check file permissions, verify file exists, ensure fmristore properly installed. Use h5ls filename.h5 test basic file access. “H5 file appears corrupted” often occurs network transfers. Use h5dump -n filename.h5 check file structure integrity, consider re-transferring checksums.","code":"# Diagnostic functions for H5 issues diagnose_h5_issues <- function(h5_file) {   cat(\"Diagnosing H5 file issues for:\", h5_file, \"\\n\\n\")    # Check basic file properties   if (!file.exists(h5_file)) {     cat(\"ERROR: File does not exist\\n\")     return(FALSE)   }    cat(\"File size:\", round(file.size(h5_file) / 1024^2, 1), \"MB\\n\")   cat(\"File permissions:\", file.access(h5_file, mode = 4), \"(0 = readable)\\n\")    # Test H5 library access   tryCatch(     {       if (requireNamespace(\"hdf5r\", quietly = TRUE)) {         h5file <- hdf5r::H5File$new(h5_file, mode = \"r\")         structure <- h5file$ls(recursive = TRUE)         h5file$close()         cat(\"H5 structure accessible: YES\\n\")         cat(\"Internal groups/datasets:\", length(structure), \"\\n\")       }     },     error = function(e) {       cat(\"H5 structure accessible: NO\\n\")       cat(\"Error:\", conditionMessage(e), \"\\n\")     }   )    # Test with fmridataset   tryCatch(     {       test_dataset <- fmri_h5_dataset(h5_file, TR = 2.0, run_length = 100)       cat(\"fmridataset compatibility: YES\\n\")     },     error = function(e) {       cat(\"fmridataset compatibility: NO\\n\")       cat(\"Error:\", conditionMessage(e), \"\\n\")     }   ) }  # diagnose_h5_issues(\"problematic_scan.h5\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"performance-troubleshooting","dir":"Articles","previous_headings":"Troubleshooting H5 Issues","what":"Performance Troubleshooting","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"Slow H5 data access Check chunk sizes align access patterns, consider reducing compression level, verify storage system performance. High memory usage H5 datasets Ensure preload=FALSE large datasets, use smaller chunk sizes, process data run-wise rather loading entire datasets.","code":"# Performance troubleshooting for H5 datasets troubleshoot_h5_performance <- function(h5_dataset) {   cat(\"H5 Performance Troubleshooting:\\n\\n\")    # Check dataset configuration   cat(\"1. Dataset Configuration:\\n\")   cat(\"   Preload setting:\", h5_dataset$backend$preload, \"\\n\")   cat(\"   Cache strategy:\", h5_dataset$backend$cache_strategy %||% \"default\", \"\\n\")    # Estimate memory usage   dims <- dim(get_data_matrix(h5_dataset, run_id = 1))   estimated_size <- prod(dims) * 4 / 1024^2 # Assume 4 bytes per float   cat(\"   Estimated memory per run:\", round(estimated_size, 1), \"MB\\n\\n\")    # Test access patterns   cat(\"2. Access Pattern Performance:\\n\")   if (requireNamespace(\"microbenchmark\", quietly = TRUE)) {     mb <- microbenchmark::microbenchmark(       single_run = get_data_matrix(h5_dataset, run_id = 1),       small_chunk = {         chunks <- data_chunks(h5_dataset, nchunks = 10)         chunks[[1]]$data       },       times = 3     )     print(summary(mb))   }    # Recommendations   cat(\"\\n3. Optimization Recommendations:\\n\")   if (estimated_size > 500) {     cat(\"   - Large dataset detected: Use chunked processing\\n\")     cat(\"   - Consider preload=FALSE and smaller chunk sizes\\n\")   }    if (h5_dataset$backend$preload) {     cat(\"   - Preloading enabled: Good for repeated access\\n\")   } else {     cat(\"   - Lazy loading: Good for memory efficiency\\n\")   } }  # troubleshoot_h5_performance(h5_dataset)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"integration-with-other-vignettes","dir":"Articles","previous_headings":"","what":"Integration with Other Vignettes","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"HDF5 storage guide connects several aspects fmridataset ecosystem: Prerequisites: Start Getting Started understand basic dataset interface exploring H5-specific features. Architecture Understanding: Architecture Overview explains H5 backends fit overall storage abstraction system. Scaling Applications: - Study-Level Analysis - H5 storage provides significant advantages large multi-subject studies - Backend Registry - See H5 backends integrate pluggable storage system Advanced Development: Extending Backends shows create custom H5-based backends specialized storage needs. Ecosystem Integration: H5 datasets work seamlessly broader neuroimaging ecosystem: - fmristore: Provides underlying H5 neuroimaging format - neuroim2: NeuroVec objects can stored loaded H5 format - DelayedArray: Advanced lazy evaluation memory-efficient operations - BiocParallel: Efficient parallel processing H5-stored data","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/h5-backend-usage.html","id":"session-information","dir":"Articles","previous_headings":"","what":"Session Information","title":"HDF5 Storage: High-Performance Data Management for Large-Scale fMRI Studies","text":"","code":"sessionInfo()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"motivation-the-multi-subject-challenge","dir":"Articles","previous_headings":"","what":"Motivation: The Multi-Subject Challenge","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Imagine ’ve developed sophisticated single-subject fMRI analysis pipeline works beautifully individual participants. can load data, extract time series regions interest, perform connectivity analyses, generate comprehensive reports. Now PI asks scale 50-subject study data three different sites, using slightly different acquisition protocols. subjects missing runs, others different numbers sessions, file organization varies sites. Traditional approaches require complex loops, inconsistent data structure management, missing data handling, careful memory orchestration avoid loading subjects simultaneously. fmridataset study-level analysis provides unified interface scaling single subjects multi-site studies maintaining memory efficiency chunking capabilities.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"quick-start-building-a-study-dataset","dir":"Articles","previous_headings":"","what":"Quick Start: Building a Study Dataset","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"example demonstrates single-subject study-level analysis transition unified interface scaling: Now let’s see unified interface action: Technical Design: Study datasets implement interface single-subject datasets hierarchical abstraction. design enables single-subject analysis code scale multi-subject studies without modification.","code":"library(fmridataset)  # Step 1: Create individual subject datasets (simulating realistic scenarios) create_subject_data <- function(subject_id, n_runs = 2, n_timepoints_per_run = 100) {   set.seed(as.numeric(gsub(\"sub-\", \"\", subject_id))) # Reproducible per subject    # Simulate subject-specific characteristics   n_voxels <- sample(800:1200, 1) # Different brain sizes    # Create realistic fMRI data with some subject-specific signal   data_matrices <- list()   for (run in 1:n_runs) {     run_data <- matrix(       rnorm(n_timepoints_per_run * n_voxels,         mean = 0, sd = 1       ),       nrow = n_timepoints_per_run, ncol = n_voxels     )      # Add subject-specific activation pattern     if (subject_id %in% c(\"sub-001\", \"sub-003\", \"sub-005\")) {       # \"High responders\" - stronger activation in first 100 voxels       activation_timepoints <- sample(1:n_timepoints_per_run, 20)       run_data[activation_timepoints, 1:100] <-         run_data[activation_timepoints, 1:100] + 1.5     }      data_matrices[[run]] <- run_data   }    # Combine runs into single matrix   combined_data <- do.call(rbind, data_matrices)    # Create subject dataset   dataset <- matrix_dataset(     datamat = combined_data,     TR = 2.0,     run_length = rep(n_timepoints_per_run, n_runs)   )    return(dataset) }  # Create multiple subject datasets subject_ids <- c(\"sub-001\", \"sub-002\", \"sub-003\", \"sub-004\", \"sub-005\") subject_datasets <- list()  for (subject_id in subject_ids) {   subject_datasets[[subject_id]] <- create_subject_data(subject_id)   cat(\"Created dataset for\", subject_id, \"\\n\") }  # Step 2: Combine into study-level dataset study_dataset <- fmri_study_dataset(   datasets = subject_datasets,   subject_ids = subject_ids )  cat(\"Study dataset created with\", length(subject_ids), \"subjects\\n\") print(study_dataset) # Same methods work at study level cat(\"Study TR:\", get_TR(study_dataset), \"seconds\\n\") cat(\"Study runs per subject:\", n_runs(study_dataset), \"\\n\") cat(\"Study timepoints per subject:\", n_timepoints(study_dataset), \"\\n\")  # Access subject-specific information subject_list <- subject_ids(study_dataset) cat(\"Subject IDs:\", paste(subject_list, collapse = \", \"), \"\\n\")  # Demonstrate memory-efficient subject iteration for (i in 1:min(3, length(subject_list))) {   subject_id <- subject_list[i]   cat(\"Subject\", subject_id, \"available for analysis\\n\") }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"understanding-study-level-architecture","dir":"Articles","previous_headings":"","what":"Understanding Study-Level Architecture","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"study-level analysis capabilities fmridataset built sophisticated architectural principles enable efficient scaling preserving capabilities rely single-subject analyses.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"lazy-evaluation-memory-efficiency-at-scale","dir":"Articles","previous_headings":"Understanding Study-Level Architecture","what":"Lazy Evaluation: Memory Efficiency at Scale","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"important concept study-level analysis lazy evaluation, ensures subject data loaded actually needed. architectural choice enables analysis arbitrarily large studies without exhausting system memory. create study dataset, ’re creating smart container knows subjects doesn’t immediately load data. lazy approach means can work studies containing hundreds subjects modest hardware. system maintains metadata subject’s temporal structure, spatial organization, experimental design keeping actual neuroimaging data disk specific analyses require . design pattern essential modern neuroimaging research studies routinely exceed memory capacity individual workstations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"unified-interface-scaling","dir":"Articles","previous_headings":"Understanding Study-Level Architecture","what":"Unified Interface Scaling","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"study dataset implements interface individual datasets, enabling seamless scaling analysis code. call get_TR() study dataset, returns common TR across subjects. request data chunks, system automatically coordinates chunking across subjects maintaining programming interface use single subjects. unified scaling means can develop test analysis pipelines single subjects, deploy unchanged full studies. system handles complexity coordinating operations across multiple subjects preserving simple, intuitive interface ’re already familiar . design significantly reduces cognitive load scaling analyses eliminates many sources bugs arise translating single-subject group-level code.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"subject-aware-chunking","dir":"Articles","previous_headings":"Understanding Study-Level Architecture","what":"Subject-Aware Chunking","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"chunking system extends naturally study-level datasets becoming subject-aware. Rather just dividing voxels across chunks, system can create chunks respect subject boundaries, enabling parallel processing strategies work efficiently multi-subject data. subject-aware chunking crucial analyses need maintain subject identity throughout processing. chunking system also enables sophisticated memory management strategies large studies. can choose process subjects individually minimize memory usage, group multiple subjects per chunk analysis benefits cross-subject operations. flexibility enables optimal performance different analysis patterns maintaining simple chunking interface.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"deep-dive-study-level-operations","dir":"Articles","previous_headings":"","what":"Deep Dive: Study-Level Operations","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"architectural foundations clear, let’s explore full range capabilities available study-level analysis see use effectively.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"from-individual-dataset-objects","dir":"Articles","previous_headings":"Deep Dive: Study-Level Operations > Creating Study Datasets from Different Sources","what":"From Individual Dataset Objects","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"straightforward approach combining existing dataset objects: approach provides maximum flexibility handling heterogeneous study designs.","code":"# Create individual datasets with realistic variation subject_data <- list()  # Subject 1: Standard 2-run acquisition subject_data[[\"sub-001\"]] <- matrix_dataset(   datamat = matrix(rnorm(20000), nrow = 200, ncol = 100),   TR = 2.0,   run_length = c(100, 100) )  # Subject 2: Different number of runs (3 runs) subject_data[[\"sub-002\"]] <- matrix_dataset(   datamat = matrix(rnorm(21000), nrow = 210, ncol = 100),   TR = 2.0,   run_length = c(70, 70, 70) )  # Subject 3: Different voxel count (different preprocessing) subject_data[[\"sub-003\"]] <- matrix_dataset(   datamat = matrix(rnorm(24000), nrow = 200, ncol = 120),   TR = 2.0,   run_length = c(100, 100) )  # Combine into study dataset study_ds <- fmri_study_dataset(   datasets = subject_data,   subject_ids = names(subject_data) )  cat(\"Created study with\", length(subject_data), \"subjects\\n\") cat(\"Run structure varies across subjects\\n\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"from-file-paths-with-bids-like-organization","dir":"Articles","previous_headings":"Deep Dive: Study-Level Operations > Creating Study Datasets from Different Sources","what":"From File Paths with BIDS-like Organization","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"studies organized consistent file structures: approach works well studies consistent organization schemes.","code":"# Simulate BIDS-like file organization study_root <- \"/path/to/study\" subject_files <- list()  # Generate file paths for each subject for (subj in c(\"001\", \"002\", \"003\")) {   subject_files[[paste0(\"sub-\", subj)]] <- list(     scans = c(       file.path(study_root, paste0(\"sub-\", subj), \"func\", paste0(\"sub-\", subj, \"_task-rest_run-1_bold.nii.gz\")),       file.path(study_root, paste0(\"sub-\", subj), \"func\", paste0(\"sub-\", subj, \"_task-rest_run-2_bold.nii.gz\"))     ),     mask = file.path(study_root, paste0(\"sub-\", subj), \"func\", paste0(\"sub-\", subj, \"_space-MNI152_desc-brain_mask.nii.gz\"))   ) }  # Create study dataset from file structure (would work with real files) # study_from_files <- fmri_study_dataset_from_files( #   subject_files = subject_files, #   TR = 2.0, #   run_length = c(180, 180)  # Common across subjects # )  cat(\"File-based study dataset structure prepared\\n\") cat(\"Supports heterogeneous file organization patterns\\n\")"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"subject-by-subject-processing","dir":"Articles","previous_headings":"Deep Dive: Study-Level Operations > Memory-Efficient Data Access Patterns","what":"Subject-by-Subject Processing","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"memory-efficient approach processes one subject time: pattern minimizes memory usage enables processing arbitrarily large studies.","code":"# Process subjects individually to minimize memory usage analyze_subject_individually <- function(study_dataset) {   subject_list <- subject_ids(study_dataset)   subject_results <- list()    for (subj_id in subject_list) {     cat(\"Processing subject\", subj_id, \"\\n\")      # Extract data for this subject only     subj_data <- get_data_matrix(study_dataset, subject_id = subj_id)      # Perform subject-specific analysis     subj_result <- list(       subject_id = subj_id,       n_timepoints = nrow(subj_data),       n_voxels = ncol(subj_data),       mean_signal = mean(subj_data),       signal_variance = var(as.vector(subj_data))     )      subject_results[[subj_id]] <- subj_result      # Explicit memory cleanup     rm(subj_data)     gc()   }    return(subject_results) }  # Apply individual processing individual_results <- analyze_subject_individually(study_dataset)  # Summarize results for (result in individual_results) {   cat(     \"Subject\", result$subject_id, \":\",     result$n_timepoints, \"timepoints,\",     result$n_voxels, \"voxels,\",     \"mean signal =\", round(result$mean_signal, 3), \"\\n\"   ) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"chunked-cross-subject-processing","dir":"Articles","previous_headings":"Deep Dive: Study-Level Operations > Memory-Efficient Data Access Patterns","what":"Chunked Cross-Subject Processing","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"analyses benefit cross-subject operations: approach enables efficient cross-subject analyses maintaining memory control.","code":"# Process data in chunks that span subjects analyze_cross_subject_chunks <- function(study_dataset, nchunks = 3) {   # Create study-level chunks   study_chunks <- data_chunks(study_dataset, nchunks = nchunks)    chunk_results <- list()   for (chunk in study_chunks) {     cat(       \"Processing chunk\", chunk$chunk_num,       \"with\", ncol(chunk$data), \"voxels across subjects\\n\"     )      # Chunk data includes information about which subjects contributed     chunk_analysis <- list(       chunk_id = chunk$chunk_num,       total_voxels = ncol(chunk$data),       total_timepoints = nrow(chunk$data),       cross_subject_correlation = mean(cor(chunk$data)),       voxel_indices = chunk$voxel_indices # Which voxels this chunk contains     )      chunk_results[[chunk$chunk_num]] <- chunk_analysis   }    return(chunk_results) }  # Apply cross-subject chunked processing chunk_results <- analyze_cross_subject_chunks(study_dataset)  # Summarize chunk-based results for (result in chunk_results) {   cat(     \"Chunk\", result$chunk_id, \":\",     result$total_voxels, \"voxels,\",     \"mean correlation =\", round(result$cross_subject_correlation, 3), \"\\n\"   ) }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"fmri_series-flexible-subject-and-voxel-selection","dir":"Articles","previous_headings":"Deep Dive: Study-Level Operations > Advanced Study-Level Features","what":"fmri_series: Flexible Subject and Voxel Selection","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"fmri_series function provides sophisticated access study data flexible selection criteria: fmri_series approach provides maximum flexibility exploratory analyses.","code":"# Extract time series from specific voxels and subjects series_subset <- fmri_series(   study_dataset,   selector = 1:10, # First 10 voxels   subject_ids = c(\"sub-001\", \"sub-003\"), # Specific subjects   timepoints = 1:50 # First 50 timepoints )  cat(\"Extracted series from\", ncol(series_subset), \"voxels\\n\") cat(\"Time series length:\", nrow(series_subset), \"timepoints\\n\")  # Convert to tibble for advanced analysis if (requireNamespace(\"dplyr\", quietly = TRUE)) {   series_tibble <- as_tibble(series_subset)    # Group by subject for subject-specific analysis   subject_summary <- series_tibble %>%     dplyr::group_by(subject_id) %>%     dplyr::summarise(       mean_signal = mean(signal, na.rm = TRUE),       signal_sd = sd(signal, na.rm = TRUE),       n_observations = dplyr::n(),       .groups = \"drop\"     )    print(subject_summary) }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"handling-heterogeneous-study-designs","dir":"Articles","previous_headings":"Deep Dive: Study-Level Operations > Advanced Study-Level Features","what":"Handling Heterogeneous Study Designs","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Study datasets can accommodate subjects different run structures temporal organizations: flexibility enables analysis real-world studies complex designs.","code":"# Create subjects with different temporal structures heterogeneous_subjects <- list()  # Subject A: 2 runs, 100 timepoints each heterogeneous_subjects[[\"sub-A\"]] <- matrix_dataset(   datamat = matrix(rnorm(20000), nrow = 200, ncol = 100),   TR = 2.0,   run_length = c(100, 100) )  # Subject B: 3 runs, 80 timepoints each heterogeneous_subjects[[\"sub-B\"]] <- matrix_dataset(   datamat = matrix(rnorm(24000), nrow = 240, ncol = 100),   TR = 2.0,   run_length = c(80, 80, 80) )  # Subject C: 1 long run, 200 timepoints heterogeneous_subjects[[\"sub-C\"]] <- matrix_dataset(   datamat = matrix(rnorm(20000), nrow = 200, ncol = 100),   TR = 2.0,   run_length = 200 )  # Combine into heterogeneous study hetero_study <- fmri_study_dataset(   datasets = heterogeneous_subjects,   subject_ids = names(heterogeneous_subjects) )  # Study-level operations work despite heterogeneity cat(\"Heterogeneous study TR:\", get_TR(hetero_study), \"seconds\\n\")  # Subject-specific operations preserve individual structure for (subj_id in subject_ids(hetero_study)) {   subj_runs <- n_runs(hetero_study, subject_id = subj_id)   cat(\"Subject\", subj_id, \"has\", subj_runs, \"runs\\n\") }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"advanced-topics","dir":"Articles","previous_headings":"","what":"Advanced Topics","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"’re comfortable basic study-level operations, advanced techniques help handle complex scenarios optimize performance large studies.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"memory-monitoring-and-management","dir":"Articles","previous_headings":"Advanced Topics > Performance Optimization for Large Studies","what":"Memory Monitoring and Management","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Understanding memory usage patterns helps optimize large study analyses: monitoring helps choose appropriate chunking strategies.","code":"# Monitor memory usage during study operations monitor_study_memory <- function(study_dataset) {   if (requireNamespace(\"pryr\", quietly = TRUE)) {     start_memory <- pryr::mem_used()     cat(\"Starting memory usage:\", format(start_memory, units = \"Mb\"), \"\\n\")      # Simulate loading one subject     subj_data <- get_data_matrix(study_dataset, subject_id = subject_ids(study_dataset)[1])     single_subj_memory <- pryr::mem_used()     cat(\"After loading one subject:\", format(single_subj_memory, units = \"Mb\"), \"\\n\")      # Calculate memory per subject     memory_per_subject <- single_subj_memory - start_memory     cat(\"Memory per subject:\", format(memory_per_subject, units = \"Mb\"), \"\\n\")      # Estimate memory for full study     n_subjects <- length(subject_ids(study_dataset))     estimated_full_memory <- start_memory + (memory_per_subject * n_subjects)     cat(       \"Estimated memory for\", n_subjects, \"subjects:\",       format(estimated_full_memory, units = \"Mb\"), \"\\n\"     )      # Cleanup     rm(subj_data)     gc()      return(list(       memory_per_subject = memory_per_subject,       estimated_full_study = estimated_full_memory     ))   } }  # memory_stats <- monitor_study_memory(study_dataset)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"parallel-processing-strategies","dir":"Articles","previous_headings":"Advanced Topics > Performance Optimization for Large Studies","what":"Parallel Processing Strategies","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Study datasets enable sophisticated parallel processing approaches: Parallel processing can significantly accelerate large study analyses.","code":"# Parallel subject processing process_subjects_parallel <- function(study_dataset, analysis_func) {   if (requireNamespace(\"parallel\", quietly = TRUE)) {     # Detect cores     n_cores <- min(parallel::detectCores() - 1, length(subject_ids(study_dataset)))     cat(\"Using\", n_cores, \"cores for parallel processing\\n\")      # Create cluster     cl <- parallel::makeCluster(n_cores)      # Export necessary objects     parallel::clusterEvalQ(cl, library(fmridataset))     parallel::clusterExport(cl, c(\"study_dataset\", \"analysis_func\"))      # Process subjects in parallel     subject_list <- subject_ids(study_dataset)     results <- parallel::parLapply(cl, subject_list, function(subj_id) {       # Extract data for this subject       subj_data <- get_data_matrix(study_dataset, subject_id = subj_id)        # Apply analysis function       result <- analysis_func(subj_data)       result$subject_id <- subj_id        return(result)     })      # Cleanup     parallel::stopCluster(cl)      return(results)   } else {     cat(\"parallel package not available, using sequential processing\\n\")     # Fallback to sequential processing     return(process_subjects_sequential(study_dataset, analysis_func))   } }  # Example analysis function simple_analysis <- function(data_matrix) {   list(     mean_signal = mean(data_matrix),     n_timepoints = nrow(data_matrix),     n_voxels = ncol(data_matrix)   ) }  # Apply parallel processing (commented out for vignette) # parallel_results <- process_subjects_parallel(study_dataset, simple_analysis)"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"study-level-quality-metrics","dir":"Articles","previous_headings":"Advanced Topics > Quality Control and Validation","what":"Study-Level Quality Metrics","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Implement comprehensive quality control across entire study: Systematic quality control helps ensure reliable study results.","code":"# Comprehensive study quality assessment assess_study_quality <- function(study_dataset) {   subject_list <- subject_ids(study_dataset)   quality_metrics <- list()    for (subj_id in subject_list) {     cat(\"Assessing quality for\", subj_id, \"\\n\")      # Get subject data     subj_data <- get_data_matrix(study_dataset, subject_id = subj_id)      # Calculate quality metrics     metrics <- list(       subject_id = subj_id,       n_timepoints = nrow(subj_data),       n_voxels = ncol(subj_data),       mean_signal = mean(subj_data),       signal_range = diff(range(subj_data)),       zero_voxels = sum(colSums(subj_data == 0) == nrow(subj_data)),       outlier_timepoints = sum(abs(scale(rowMeans(subj_data))) > 3),       temporal_snr = mean(subj_data) / sd(rowMeans(subj_data))     )      quality_metrics[[subj_id]] <- metrics      # Memory cleanup     rm(subj_data)   }    return(quality_metrics) }  # Run quality assessment quality_results <- assess_study_quality(study_dataset)  # Summarize quality across study quality_summary <- data.frame(   subject_id = sapply(quality_results, function(x) x$subject_id),   n_timepoints = sapply(quality_results, function(x) x$n_timepoints),   n_voxels = sapply(quality_results, function(x) x$n_voxels),   temporal_snr = sapply(quality_results, function(x) x$temporal_snr),   outlier_timepoints = sapply(quality_results, function(x) x$outlier_timepoints) )  cat(\"Study Quality Summary:\\n\") print(quality_summary)  # Identify potential issues problematic_subjects <- quality_summary$subject_id[quality_summary$outlier_timepoints > 5] if (length(problematic_subjects) > 0) {   cat(\"Subjects with potential quality issues:\", paste(problematic_subjects, collapse = \", \"), \"\\n\") }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"data-consistency-validation","dir":"Articles","previous_headings":"Advanced Topics > Quality Control and Validation","what":"Data Consistency Validation","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Verify consistency across subjects study: Consistency validation helps identify data preprocessing issues early.","code":"# Validate study consistency validate_study_consistency <- function(study_dataset) {   subject_list <- subject_ids(study_dataset)   consistency_report <- list()    # Check temporal structure consistency   tr_values <- sapply(subject_list, function(subj_id) {     get_TR(study_dataset, subject_id = subj_id)   })    consistency_report$tr_consistent <- length(unique(tr_values)) == 1   consistency_report$tr_range <- range(tr_values)    # Check run structure patterns   run_counts <- sapply(subject_list, function(subj_id) {     n_runs(study_dataset, subject_id = subj_id)   })    consistency_report$run_count_consistent <- length(unique(run_counts)) <= 2 # Allow some variation   consistency_report$run_count_distribution <- table(run_counts)    # Check data dimensions   voxel_counts <- sapply(subject_list, function(subj_id) {     subj_data <- get_data_matrix(study_dataset, subject_id = subj_id)     ncol(subj_data)   })    consistency_report$voxel_count_consistent <- length(unique(voxel_counts)) <= 3 # Allow preprocessing variation   consistency_report$voxel_count_range <- range(voxel_counts)    return(consistency_report) }  # Run consistency validation consistency_results <- validate_study_consistency(study_dataset)  cat(\"Study Consistency Report:\\n\") cat(\"TR consistent:\", consistency_results$tr_consistent, \"\\n\") cat(\"Run counts:\", paste(names(consistency_results$run_count_distribution),   consistency_results$run_count_distribution,   sep = \":\", collapse = \", \" ), \"\\n\") cat(\"Voxel count range:\", consistency_results$voxel_count_range, \"\\n\")  # Report potential issues if (!consistency_results$tr_consistent) {   warning(\"TR values vary across subjects: \", paste(consistency_results$tr_range, collapse = \"-\")) }  if (!consistency_results$voxel_count_consistent) {   warning(\"Voxel counts vary significantly across subjects\") }"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"tips-and-best-practices","dir":"Articles","previous_headings":"","what":"Tips and Best Practices","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"practical guidelines learned analyzing large neuroimaging studies help work effectively study-level datasets.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"memory-management-for-large-studies","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Memory Management for Large Studies","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Subject-Level Processing: Process subjects individually using get_data_matrix(study_dataset, subject_id = \"sub-001\") rather loading complete studies. Full study loading appropriate : - Total data size < 50% available RAM - Cross-subject operations require simultaneous access - Computational efficiency outweighs memory constraints","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"quality-control-requirements","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Quality Control Requirements","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Pre-Analysis Validation: 1. Verify temporal alignment across subjects 2. Check signal--noise ratios per subject 3. Identify motion outliers using framewise displacement 4. Validate mask coverage across subjects 5. Document exclusion criteria thresholds Implement quality control group analyses prevent outlier contamination group statistics.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"analysis-strategy-optimization","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Analysis Strategy Optimization","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Two-Phase Approach: 1. Exploration Phase: Use fmri_series() subset selection hypothesis generation parameter tuning 2. Production Phase: Implement chunked processing data_chunks() full study analysis approach minimizes computational resources development ensuring scalability final analyses.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"memory-management-strategies","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Memory Management Strategies","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Effective memory management crucial large study analyses:","code":"# Memory-efficient study analysis patterns memory_efficient_patterns <- function() {   cat(\"Memory-efficient study analysis patterns:\\n\\n\")    cat(\"1. Subject-by-subject processing:\\n\")   cat(\"   - Load one subject at a time\\n\")   cat(\"   - Explicit cleanup with rm() and gc()\\n\")   cat(\"   - Monitor memory usage throughout\\n\\n\")    cat(\"2. Chunked processing:\\n\")   cat(\"   - Use study-level chunking for cross-subject operations\\n\")   cat(\"   - Choose chunk sizes based on available RAM\\n\")   cat(\"   - Consider subject boundaries in chunking strategy\\n\\n\")    cat(\"3. Lazy evaluation:\\n\")   cat(\"   - Keep study datasets lazy until analysis needed\\n\")   cat(\"   - Use metadata queries before loading data\\n\")   cat(\"   - Avoid unnecessary data conversions\\n\\n\")    cat(\"4. Parallel processing:\\n\")   cat(\"   - Process subjects in parallel when possible\\n\")   cat(\"   - Monitor total memory usage across cores\\n\")   cat(\"   - Use appropriate chunk sizes for parallel work\\n\") }  memory_efficient_patterns()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"study-design-considerations","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Study Design Considerations","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Design study analysis workflow handle real-world complexities:","code":"# Study design best practices study_design_guidelines <- function() {   cat(\"Study design considerations:\\n\\n\")    cat(\"1. Heterogeneity planning:\\n\")   cat(\"   - Expect different run structures across subjects\\n\")   cat(\"   - Plan for missing data scenarios\\n\")   cat(\"   - Design analyses robust to preprocessing differences\\n\\n\")    cat(\"2. Quality control integration:\\n\")   cat(\"   - Implement QC at multiple stages\\n\")   cat(\"   - Use automated outlier detection\\n\")   cat(\"   - Document exclusion criteria clearly\\n\\n\")    cat(\"3. Reproducibility measures:\\n\")   cat(\"   - Document study dataset creation parameters\\n\")   cat(\"   - Version control analysis scripts\\n\")   cat(\"   - Save intermediate results for validation\\n\\n\")    cat(\"4. Scalability planning:\\n\")   cat(\"   - Test analysis pipelines on subsets first\\n\")   cat(\"   - Plan for computational resource requirements\\n\")   cat(\"   - Design modular analysis components\\n\") }  study_design_guidelines()"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"error-handling-and-robustness","dir":"Articles","previous_headings":"Tips and Best Practices","what":"Error Handling and Robustness","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Build robust analysis pipelines handle real-world data issues: Robust error handling ensures individual subject issues don’t derail entire study analyses.","code":"# Robust study analysis with error handling robust_subject_analysis <- function(study_dataset, analysis_func) {   subject_list <- subject_ids(study_dataset)   successful_results <- list()   failed_subjects <- character()    for (subj_id in subject_list) {     cat(\"Processing\", subj_id, \"... \")      tryCatch(       {         # Attempt to load and analyze subject data         subj_data <- get_data_matrix(study_dataset, subject_id = subj_id)          # Basic data validation         if (nrow(subj_data) == 0 || ncol(subj_data) == 0) {           stop(\"Empty data matrix\")         }          if (any(is.na(subj_data)) && mean(is.na(subj_data)) > 0.1) {           warning(             \"High proportion of missing values (\",             round(mean(is.na(subj_data)) * 100, 1), \"%)\"           )         }          # Apply analysis         result <- analysis_func(subj_data)         result$subject_id <- subj_id         result$analysis_successful <- TRUE          successful_results[[subj_id]] <- result         cat(\"SUCCESS\\n\")          # Cleanup         rm(subj_data)       },       error = function(e) {         cat(\"FAILED -\", conditionMessage(e), \"\\n\")         failed_subjects <- c(failed_subjects, subj_id)          # Store failure information         successful_results[[subj_id]] <- list(           subject_id = subj_id,           analysis_successful = FALSE,           error_message = conditionMessage(e)         )       }     )   }    # Summary report   cat(\"\\nAnalysis Summary:\\n\")   cat(\"Successful subjects:\", length(successful_results) - length(failed_subjects), \"\\n\")   cat(\"Failed subjects:\", length(failed_subjects), \"\\n\")    if (length(failed_subjects) > 0) {     cat(\"Failed subject IDs:\", paste(failed_subjects, collapse = \", \"), \"\\n\")   }    return(successful_results) }  # Example usage with error handling safe_analysis <- function(data_matrix) {   # Simple analysis that might fail on problematic data   list(     mean_signal = mean(data_matrix, na.rm = TRUE),     signal_stability = sd(rowMeans(data_matrix, na.rm = TRUE)),     data_quality_score = 1 - mean(is.na(data_matrix))   ) }  # robust_results <- robust_subject_analysis(study_dataset, safe_analysis)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"troubleshooting-study-level-issues","dir":"Articles","previous_headings":"","what":"Troubleshooting Study-Level Issues","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"working study-level datasets, certain issues common can systematically diagnosed resolved.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"memory-related-problems","dir":"Articles","previous_headings":"Troubleshooting Study-Level Issues","what":"Memory-Related Problems","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"“Error: allocate vector size X Gb” indicates ’re trying load much data simultaneously. Switch subject--subject processing reduce chunk sizes. Slow performance study operations Check ’re accidentally triggering full study data loading. Use subject-specific access patterns monitor memory usage.","code":"# Diagnose memory issues diagnose_memory_issues <- function(study_dataset) {   cat(\"Memory diagnostic for study dataset:\\n\\n\")    # Check study size   n_subjects <- length(subject_ids(study_dataset))   cat(\"Number of subjects:\", n_subjects, \"\\n\")    # Estimate memory requirements   first_subject <- subject_ids(study_dataset)[1]   sample_data <- get_data_matrix(study_dataset, subject_id = first_subject)   sample_size <- object.size(sample_data)    cat(\"Sample subject data size:\", format(sample_size, units = \"Mb\"), \"\\n\")   cat(\"Estimated full study size:\", format(sample_size * n_subjects, units = \"Gb\"), \"\\n\")    # Memory recommendations   available_ram <- 8 # Assume 8GB system   recommended_chunks <- ceiling((sample_size * n_subjects) / (available_ram * 0.5 * 1e9))    cat(\"Recommended processing approach:\\n\")   if (recommended_chunks <= 1) {     cat(\"  - Full study loading feasible\\n\")   } else if (recommended_chunks <= n_subjects) {     cat(\"  - Subject-by-subject processing recommended\\n\")   } else {     cat(\"  - Chunked processing within subjects required\\n\")   }    rm(sample_data)   gc() }  # diagnose_memory_issues(study_dataset)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"subject-inconsistency-issues","dir":"Articles","previous_headings":"Troubleshooting Study-Level Issues","what":"Subject Inconsistency Issues","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"Different temporal structures across subjects often normal real studies. Use subject-specific access patterns explicit subjects ’re analyzing. Varying voxel counts across subjects Common due different preprocessing pipelines. Plan analyses handle variation standardize preprocessing.","code":"# Handle subject inconsistencies handle_subject_inconsistencies <- function(study_dataset) {   subject_list <- subject_ids(study_dataset)    # Catalog inconsistencies   inconsistencies <- list()    # Check temporal structure   temporal_info <- sapply(subject_list, function(subj_id) {     list(       TR = get_TR(study_dataset, subject_id = subj_id),       n_runs = n_runs(study_dataset, subject_id = subj_id),       n_timepoints = n_timepoints(study_dataset, subject_id = subj_id)     )   }, simplify = FALSE)    # Identify outliers   tr_values <- sapply(temporal_info, function(x) x$TR)   run_counts <- sapply(temporal_info, function(x) x$n_runs)   timepoint_counts <- sapply(temporal_info, function(x) x$n_timepoints)    # Report inconsistencies   if (length(unique(tr_values)) > 1) {     cat(\"TR inconsistency detected:\\n\")     for (i in seq_along(subject_list)) {       cat(\"  \", subject_list[i], \": TR =\", tr_values[i], \"\\n\")     }   }    if (length(unique(run_counts)) > 1) {     cat(\"Run count inconsistency detected:\\n\")     for (i in seq_along(subject_list)) {       cat(\"  \", subject_list[i], \": runs =\", run_counts[i], \"\\n\")     }   }    # Suggest handling strategies   cat(\"\\nSuggested handling strategies:\\n\")   cat(\"- Use subject-specific processing for inconsistent parameters\\n\")   cat(\"- Consider subsetting to consistent subjects for group analyses\\n\")   cat(\"- Implement analysis methods robust to temporal differences\\n\") }  # handle_subject_inconsistencies(study_dataset)"},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"integration-with-other-vignettes","dir":"Articles","previous_headings":"","what":"Integration with Other Vignettes","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"study-level analysis guide connects several aspects fmridataset ecosystem: Prerequisites: Start Getting Started understand single-subject dataset operations scaling studies. Architecture Understanding: Architecture Overview explains study datasets leverage abstractions single-subject datasets enabling efficient scaling. Advanced Backend Usage: - H5 Backend Usage - Efficient storage large multi-subject studies - Backend Registry - Custom backends specialized study organizations Extension Development: Extending Backends shows create study-aware custom backends. Ecosystem Integration: Study datasets work seamlessly neuroimaging packages: - fmrireg: Study-level regression modeling temporal structure preservation - DelayedArray: Advanced array operations memory-efficient cross-subject analyses - BiocParallel: Sophisticated parallel processing strategies large studies","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/articles/study-level-analysis.html","id":"session-information","dir":"Articles","previous_headings":"","what":"Session Information","title":"Scaling to Study-Level Analysis: From Single Subjects to Group Studies","text":"","code":"sessionInfo()"},{"path":"https://bbuchsbaum.github.io/fmridataset/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Bradley Buchsbaum. Author, maintainer.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Buchsbaum B (2026). fmridataset: Unified Container fMRI Datasets. R package version 0.8.9, https://github.com/bbuchsbaum/fmridataset.","code":"@Manual{,   title = {fmridataset: Unified Container for fMRI Datasets},   author = {Bradley Buchsbaum},   year = {2026},   note = {R package version 0.8.9},   url = {https://github.com/bbuchsbaum/fmridataset}, }"},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":null,"dir":"","previous_headings":"","what":"Motivation: Why Create Custom Backends?","title":"Motivation: Why Create Custom Backends?","text":"research group using custom MATLAB pipeline exports preprocessed fMRI data JSON files separate metadata. data format optimized specific analyses, includes custom quality metrics, integrates lab’s database system. Rather converting data standard formats writing custom loading code analysis, can create backend makes format work seamlessly fmridataset. Creating custom backend means specialized data format immediately gains access fmridataset features: unified interfaces, efficient chunking, study-level operations, compatibility entire ecosystem. vignette teaches essentials backend development practical examples, showing implement required interface optimize specific use case.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"a-complete-example-json-backend","dir":"","previous_headings":"","what":"A Complete Example: JSON Backend","title":"Motivation: Why Create Custom Backends?","text":"Let’s create working backend JSON-formatted fMRI data demonstrates essential concepts: 💡 Key Insight: backend needs implement five methods work entire fmridataset ecosystem. simple interface provides tremendous power flexibility.","code":"# Create a complete JSON backend implementation json_backend <- function(json_file, metadata_file = NULL) {   # Input validation   if (!file.exists(json_file)) {     stop(\"JSON file not found: \", json_file)   }      # Initialize backend structure   backend <- list(     json_file = json_file,     metadata_file = metadata_file,     data_cache = NULL,     dims_cache = NULL,     is_open = FALSE   )      class(backend) <- c(\"json_backend\", \"storage_backend\")   backend }  # Implement the open method backend_open.json_backend <- function(backend) {   if (backend$is_open) {     return(backend)  # Already open   }      # Simulate reading JSON data (in practice, use jsonlite::fromJSON)   # For demonstration, create synthetic data   set.seed(123)   n_time <- 100   n_voxels <- 500      backend$data_cache <- matrix(     rnorm(n_time * n_voxels),     nrow = n_time,     ncol = n_voxels   )      backend$dims_cache <- list(     spatial = c(n_voxels, 1, 1),  # Flat spatial structure     time = n_time   )      backend$is_open <- TRUE   backend }  # Implement the close method backend_close.json_backend <- function(backend) {   backend$data_cache <- NULL   backend$dims_cache <- NULL   backend$is_open <- FALSE   invisible(backend) }  # Implement dimension query backend_get_dims.json_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Backend must be opened first\")   }   backend$dims_cache }  # Implement data access backend_get_data.json_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) {     stop(\"Backend must be opened first\")   }      data <- backend$data_cache      # Handle subsetting   if (!is.null(rows)) {     data <- data[rows, , drop = FALSE]   }   if (!is.null(cols)) {     data <- data[, cols, drop = FALSE]   }      data }  # Implement mask generation backend_get_mask.json_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Backend must be opened first\")   }      # All voxels are valid in our JSON format   rep(TRUE, backend$dims_cache$spatial[1]) }  # Test the backend json_file <- tempfile(fileext = \".json\") writeLines(\"{}\", json_file)  # Create dummy file  backend <- json_backend(json_file) backend <- backend_open(backend)  dims <- backend_get_dims(backend) cat(\"Backend dimensions - Time:\", dims$time, \"Spatial:\", dims$spatial[1], \"\\n\") #> Backend dimensions - Time: 100 Spatial: 500  # Get some data sample_data <- backend_get_data(backend, rows = 1:10, cols = 1:50) cat(\"Retrieved data shape:\", dim(sample_data), \"\\n\") #> Retrieved data shape: 10 50  backend_close(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"understanding-the-backend-contract","dir":"","previous_headings":"","what":"Understanding the Backend Contract","title":"Motivation: Why Create Custom Backends?","text":"backend contract defines minimal interface backends must implement. Understanding contract essential creating compatible backends.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"required-methods","dir":"","previous_headings":"","what":"Required Methods","title":"Motivation: Why Create Custom Backends?","text":"Every backend must implement five S3 methods:","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"id_1-backend_open","dir":"","previous_headings":"Required Methods","what":"1. backend_open()","title":"Motivation: Why Create Custom Backends?","text":"Opens backend acquires resources (file handles, connections, memory). method idempotent - calling multiple times open backend safe.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"id_2-backend_close","dir":"","previous_headings":"Required Methods","what":"2. backend_close()","title":"Motivation: Why Create Custom Backends?","text":"Releases resources cleans . closing, backend hold external resources.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"id_3-backend_get_dims","dir":"","previous_headings":"Required Methods","what":"3. backend_get_dims()","title":"Motivation: Why Create Custom Backends?","text":"Returns dimensions list spatial (3-element vector) time (single integer) elements. must work without loading data.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"id_4-backend_get_data","dir":"","previous_headings":"Required Methods","what":"4. backend_get_data()","title":"Motivation: Why Create Custom Backends?","text":"Returns data timepoints × voxels orientation. Must support optional row column subsetting efficient partial loading.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"id_5-backend_get_mask","dir":"","previous_headings":"Required Methods","what":"5. backend_get_mask()","title":"Motivation: Why Create Custom Backends?","text":"Returns logical vector indicating valid voxels. Length must equal product spatial dimensions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"optional-methods","dir":"","previous_headings":"","what":"Optional Methods","title":"Motivation: Why Create Custom Backends?","text":"methods enhance functionality aren’t required:","code":"# Optional: Metadata extraction backend_get_metadata.json_backend <- function(backend) {   if (!backend$is_open) {     stop(\"Backend must be opened first\")   }      list(     format = \"json\",     compression = \"none\",     creation_date = Sys.Date(),     custom_metrics = list(       quality_score = 0.95,       motion_level = \"low\"     )   ) }  # Optional: Validation backend_validate.json_backend <- function(backend) {   # Check data integrity   if (!backend$is_open) {     return(FALSE)   }      # Validate dimensions   dims <- backend$dims_cache   expected_size <- dims$time * dims$spatial[1]   actual_size <- length(backend$data_cache)      if (expected_size != actual_size) {     warning(\"Data size mismatch\")     return(FALSE)   }      TRUE }  # Test optional methods backend <- backend_open(json_backend(json_file)) metadata <- backend_get_metadata(backend) cat(\"Format:\", metadata$format, \"\\n\") #> Format: json cat(\"Quality score:\", metadata$custom_metrics$quality_score, \"\\n\") #> Quality score: 0.95  is_valid <- backend_validate(backend) #> Error in backend_validate(backend): could not find function \"backend_validate\" cat(\"Backend valid:\", is_valid, \"\\n\") #> Error: object 'is_valid' not found backend_close(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"deep-dive-implementation-patterns","dir":"","previous_headings":"","what":"Deep Dive: Implementation Patterns","title":"Motivation: Why Create Custom Backends?","text":"Let’s explore common patterns make backends robust efficient.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"state-management-pattern","dir":"","previous_headings":"","what":"State Management Pattern","title":"Motivation: Why Create Custom Backends?","text":"Backends need track whether ’re open manage resources appropriately:","code":"# Robust state management example stateful_backend <- function(source) {   backend <- list(     source = source,     # State flags     is_open = FALSE,     is_validated = FALSE,     has_error = FALSE,     # Resource tracking     resources = list(),     # Error information     last_error = NULL   )      class(backend) <- c(\"stateful_backend\", \"storage_backend\")   backend }  # Safe resource acquisition backend_open.stateful_backend <- function(backend) {   if (backend$is_open) {     return(backend)  # Idempotent   }      tryCatch({     # Acquire resources     backend$resources$data <- matrix(rnorm(1000), 100, 10)     backend$is_open <- TRUE     backend$has_error <- FALSE   }, error = function(e) {     backend$has_error <- TRUE     backend$last_error <- conditionMessage(e)     stop(\"Failed to open backend: \", conditionMessage(e))   })      backend }  # Safe resource cleanup backend_close.stateful_backend <- function(backend) {   if (!backend$is_open) {     return(invisible(backend))  # Already closed   }      # Release all resources   backend$resources <- list()   backend$is_open <- FALSE      invisible(backend) }  # Implement other required methods... backend_get_dims.stateful_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")   list(spatial = c(10, 1, 1), time = 100) }  backend_get_data.stateful_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) stop(\"Backend not open\")   data <- backend$resources$data   if (!is.null(rows)) data <- data[rows, , drop = FALSE]   if (!is.null(cols)) data <- data[, cols, drop = FALSE]   data }  backend_get_mask.stateful_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")   rep(TRUE, 10) }  # Test state management backend <- stateful_backend(\"dummy_source\") cat(\"Initial state - is_open:\", backend$is_open, \"\\n\") #> Initial state - is_open: FALSE  backend <- backend_open(backend) cat(\"After open - is_open:\", backend$is_open, \"\\n\") #> After open - is_open: TRUE  backend <- backend_close(backend) cat(\"After close - is_open:\", backend$is_open, \"\\n\") #> After close - is_open: FALSE"},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"lazy-loading-pattern","dir":"","previous_headings":"","what":"Lazy Loading Pattern","title":"Motivation: Why Create Custom Backends?","text":"Implement lazy loading defer expensive operations:","code":"# Lazy loading backend lazy_backend <- function(data_source) {   backend <- list(     data_source = data_source,     is_open = FALSE,     # Lazy caches     dims_cache = NULL,     data_cache = NULL,     mask_cache = NULL   )      class(backend) <- c(\"lazy_backend\", \"storage_backend\")   backend }  backend_open.lazy_backend <- function(backend) {   backend$is_open <- TRUE   # Don't load data yet!   backend }  backend_get_dims.lazy_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")      # Load dimensions only when first requested   if (is.null(backend$dims_cache)) {     # In practice, read just headers/metadata     backend$dims_cache <- list(       spatial = c(100, 1, 1),       time = 50     )   }      backend$dims_cache }  backend_get_data.lazy_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) stop(\"Backend not open\")      # Load data only when first accessed   if (is.null(backend$data_cache)) {     cat(\"Loading data (lazy)...\\n\")     backend$data_cache <- matrix(rnorm(5000), 50, 100)   }      data <- backend$data_cache   if (!is.null(rows)) data <- data[rows, , drop = FALSE]   if (!is.null(cols)) data <- data[, cols, drop = FALSE]   data }  backend_get_mask.lazy_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")      if (is.null(backend$mask_cache)) {     dims <- backend_get_dims(backend)     backend$mask_cache <- rep(TRUE, dims$spatial[1])   }      backend$mask_cache }  backend_close.lazy_backend <- function(backend) {   backend$dims_cache <- NULL   backend$data_cache <- NULL   backend$mask_cache <- NULL   backend$is_open <- FALSE   invisible(backend) }  # Demonstrate lazy loading backend <- lazy_backend(\"source\") backend <- backend_open(backend)  cat(\"Getting dimensions...\\n\") #> Getting dimensions... dims <- backend_get_dims(backend)  # No data loading  cat(\"\\nGetting mask...\\n\") #>  #> Getting mask... mask <- backend_get_mask(backend)  # Still no data loading  cat(\"\\nGetting data...\\n\") #>  #> Getting data... data <- backend_get_data(backend, rows = 1:10)  # NOW data loads #> Loading data (lazy)...  backend_close(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"validation-pattern","dir":"","previous_headings":"","what":"Validation Pattern","title":"Motivation: Why Create Custom Backends?","text":"Implement validation ensure data integrity:","code":"# Create validation utilities validate_backend_contract <- function(backend_class) {   required_methods <- c(     \"backend_open\",     \"backend_close\",      \"backend_get_dims\",     \"backend_get_data\",     \"backend_get_mask\"   )      missing_methods <- character()      for (method in required_methods) {     full_method <- paste0(method, \".\", backend_class)     if (!exists(full_method)) {       missing_methods <- c(missing_methods, method)     }   }      if (length(missing_methods) > 0) {     stop(\"Backend class '\", backend_class, \"' missing required methods: \",          paste(missing_methods, collapse = \", \"))   }      cat(\"✓ Backend class '\", backend_class, \"' implements all required methods\\n\")   TRUE }  # Test our backends validate_backend_contract(\"json_backend\") #> ✓ Backend class ' json_backend ' implements all required methods #> [1] TRUE validate_backend_contract(\"lazy_backend\") #> ✓ Backend class ' lazy_backend ' implements all required methods #> [1] TRUE"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"caching-strategies","dir":"","previous_headings":"","what":"Caching Strategies","title":"Motivation: Why Create Custom Backends?","text":"Implement intelligent caching better performance:","code":"# Advanced caching backend cached_backend <- function(source, cache_size_mb = 100) {   backend <- list(     source = source,     cache_size_mb = cache_size_mb,     is_open = FALSE,     # Multi-level cache     cache = list(       dims = NULL,       mask = NULL,       data_blocks = list(),       access_times = list()     ),     # Cache statistics     stats = list(       hits = 0,       misses = 0,       evictions = 0     )   )      class(backend) <- c(\"cached_backend\", \"storage_backend\")   backend }  # Implement cache management cache_get_or_load <- function(backend, key, loader_fn) {   if (!is.null(backend$cache[[key]])) {     backend$stats$hits <- backend$stats$hits + 1     cat(\"Cache hit for\", key, \"\\n\")     return(backend$cache[[key]])   }      backend$stats$misses <- backend$stats$misses + 1   cat(\"Cache miss for\", key, \"- loading...\\n\")      value <- loader_fn()   backend$cache[[key]] <- value   backend$cache$access_times[[key]] <- Sys.time()      value }  backend_open.cached_backend <- function(backend) {   backend$is_open <- TRUE   backend }  backend_get_dims.cached_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")      cache_get_or_load(backend, \"dims\", function() {     list(spatial = c(100, 1, 1), time = 50)   }) }  backend_get_data.cached_backend <- function(backend, rows = NULL, cols = NULL) {   if (!backend$is_open) stop(\"Backend not open\")      # Create cache key based on request   cache_key <- paste0(\"data_\",                       paste(range(rows %||% 1:50), collapse = \"_\"),                      \"_\",                      paste(range(cols %||% 1:100), collapse = \"_\"))      data <- cache_get_or_load(backend, cache_key, function() {     matrix(rnorm(5000), 50, 100)   })      if (!is.null(rows)) data <- data[rows, , drop = FALSE]   if (!is.null(cols)) data <- data[, cols, drop = FALSE]   data }  backend_get_mask.cached_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")      cache_get_or_load(backend, \"mask\", function() {     rep(TRUE, 100)   }) }  backend_close.cached_backend <- function(backend) {   # Report cache statistics   cat(\"\\nCache statistics:\\n\")   cat(\"  Hits:\", backend$stats$hits, \"\\n\")   cat(\"  Misses:\", backend$stats$misses, \"\\n\")   cat(\"  Hit rate:\",        round(100 * backend$stats$hits /              (backend$stats$hits + backend$stats$misses), 1), \"%\\n\")      backend$cache <- list()   backend$is_open <- FALSE   invisible(backend) }  # Demonstrate caching `%||%` <- function(x, y) if (is.null(x)) y else x  backend <- cached_backend(\"source\") backend <- backend_open(backend)  # First access - cache miss data1 <- backend_get_data(backend, rows = 1:10) #> Cache miss for data_1_10_1_100 - loading...  # Second access - cache hit data2 <- backend_get_data(backend, rows = 1:10) #> Cache miss for data_1_10_1_100 - loading...  # Different subset - cache miss data3 <- backend_get_data(backend, rows = 11:20) #> Cache miss for data_11_20_1_100 - loading...  backend_close(backend) #>  #> Cache statistics: #>   Hits: 0  #>   Misses: 0  #>   Hit rate: NaN %"},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"error-handling","dir":"","previous_headings":"","what":"Error Handling","title":"Motivation: Why Create Custom Backends?","text":"Robust error handling makes backends production-ready:","code":"# Create a backend with comprehensive error handling robust_backend <- function(source) {   backend <- list(     source = source,     is_open = FALSE,     error_log = list()   )      class(backend) <- c(\"robust_backend\", \"storage_backend\")   backend }  # Helper to log errors log_error <- function(backend, operation, error) {   backend$error_log[[length(backend$error_log) + 1]] <- list(     timestamp = Sys.time(),     operation = operation,     message = conditionMessage(error)   )   backend }  backend_open.robust_backend <- function(backend) {   tryCatch({     if (backend$is_open) {       warning(\"Backend already open\")       return(backend)     }          # Simulate potential failures     if (runif(1) > 0.8) {       stop(\"Simulated connection failure\")     }          backend$is_open <- TRUE     cat(\"Successfully opened backend\\n\")     backend        }, error = function(e) {     backend <- log_error(backend, \"open\", e)     stop(\"Failed to open backend: \", conditionMessage(e))   }) }  backend_get_data.robust_backend <- function(backend, rows = NULL, cols = NULL) {   tryCatch({     if (!backend$is_open) {       stop(\"Backend not open\")     }          # Validate indices     if (!is.null(rows) && any(rows < 1)) {       stop(\"Invalid row indices\")     }          if (!is.null(cols) && any(cols < 1)) {       stop(\"Invalid column indices\")     }          # Return data     matrix(rnorm(5000), 50, 100)[rows %||% 1:50, cols %||% 1:100, drop = FALSE]        }, error = function(e) {     backend <- log_error(backend, \"get_data\", e)     stop(\"Data access failed: \", conditionMessage(e))   }) }  # Implement other methods... backend_get_dims.robust_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")   list(spatial = c(100, 1, 1), time = 50) }  backend_get_mask.robust_backend <- function(backend) {   if (!backend$is_open) stop(\"Backend not open\")   rep(TRUE, 100) }  backend_close.robust_backend <- function(backend) {   if (length(backend$error_log) > 0) {     cat(\"\\nError log:\\n\")     for (error in backend$error_log) {       cat(\"  -\", error$operation, \"at\", format(error$timestamp),            \":\", error$message, \"\\n\")     }   }   backend$is_open <- FALSE   invisible(backend) }  # Test error handling set.seed(123) backend <- robust_backend(\"source\")  # May fail randomly result <- tryCatch({   backend <- backend_open(backend)   data <- backend_get_data(backend, rows = 1:10)   cat(\"Data retrieved successfully\\n\")   backend_close(backend) }, error = function(e) {   cat(\"Caught error:\", conditionMessage(e), \"\\n\") }) #> Successfully opened backend #> Data retrieved successfully"},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"tips-and-best-practices","dir":"","previous_headings":"","what":"Tips and Best Practices","title":"Motivation: Why Create Custom Backends?","text":"essential guidelines creating robust, efficient backends. ⚠️ Performance Tip: Always implement lazy loading large datasets. Load metadata dimensions quickly, defer data loading actually needed. 🛡️ Best Practice: Make backend methods idempotent. Opening already-open backend closing already-closed backend safe operations. ⚡ Pro Tip: Cache computed values like masks dimensions. often queried multiple times rarely change.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"backend-development-checklist","dir":"","previous_headings":"","what":"Backend Development Checklist","title":"Motivation: Why Create Custom Backends?","text":"considering backend complete:","code":"backend_checklist <- function() {   cat(\"Backend Development Checklist:\\n\\n\")      cat(\"Required Functionality:\\n\")   cat(\"  ☐ Implements all 5 required methods\\n\")   cat(\"  ☐ Returns correct data orientations\\n\")   cat(\"  ☐ Handles NULL rows/cols in get_data\\n\")   cat(\"  ☐ Returns valid dimension structure\\n\")   cat(\"  ☐ Mask length matches spatial dimensions\\n\\n\")      cat(\"Robustness:\\n\")   cat(\"  ☐ Validates inputs in constructor\\n\")   cat(\"  ☐ Checks is_open state in all methods\\n\")   cat(\"  ☐ Handles errors gracefully\\n\")   cat(\"  ☐ Cleans up resources in close\\n\")   cat(\"  ☐ Methods are idempotent\\n\\n\")      cat(\"Performance:\\n\")   cat(\"  ☐ Implements lazy loading\\n\")   cat(\"  ☐ Caches frequently accessed values\\n\")   cat(\"  ☐ Minimizes memory footprint\\n\")   cat(\"  ☐ Supports partial data loading\\n\\n\")      cat(\"Documentation:\\n\")   cat(\"  ☐ Constructor documented\\n\")   cat(\"  ☐ Error messages are informative\\n\")   cat(\"  ☐ Usage examples provided\\n\")   cat(\"  ☐ Performance characteristics noted\\n\") }  backend_checklist() #> Backend Development Checklist: #>  #> Required Functionality: #>   ☐ Implements all 5 required methods #>   ☐ Returns correct data orientations #>   ☐ Handles NULL rows/cols in get_data #>   ☐ Returns valid dimension structure #>   ☐ Mask length matches spatial dimensions #>  #> Robustness: #>   ☐ Validates inputs in constructor #>   ☐ Checks is_open state in all methods #>   ☐ Handles errors gracefully #>   ☐ Cleans up resources in close #>   ☐ Methods are idempotent #>  #> Performance: #>   ☐ Implements lazy loading #>   ☐ Caches frequently accessed values #>   ☐ Minimizes memory footprint #>   ☐ Supports partial data loading #>  #> Documentation: #>   ☐ Constructor documented #>   ☐ Error messages are informative #>   ☐ Usage examples provided #>   ☐ Performance characteristics noted"},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"testing-your-backend","dir":"","previous_headings":"","what":"Testing Your Backend","title":"Motivation: Why Create Custom Backends?","text":"Comprehensive testing ensures reliability:","code":"# Test suite for backends test_backend <- function(backend_constructor, test_source) {   cat(\"Testing backend implementation...\\n\\n\")      # Test construction   cat(\"Testing construction...\")   backend <- backend_constructor(test_source)   cat(\" ✓\\n\")      # Test opening   cat(\"Testing open...\")   backend <- backend_open(backend)   cat(\" ✓\\n\")      # Test dimensions   cat(\"Testing dimensions...\")   dims <- backend_get_dims(backend)   stopifnot(is.list(dims))   stopifnot(all(c(\"spatial\", \"time\") %in% names(dims)))   stopifnot(length(dims$spatial) == 3)   cat(\" ✓\\n\")      # Test mask   cat(\"Testing mask...\")   mask <- backend_get_mask(backend)   stopifnot(is.logical(mask))   stopifnot(length(mask) == prod(dims$spatial))   cat(\" ✓\\n\")      # Test data access   cat(\"Testing data access...\")   data <- backend_get_data(backend)   stopifnot(is.matrix(data))   stopifnot(nrow(data) == dims$time)   cat(\" ✓\\n\")      # Test subsetting   cat(\"Testing subsetting...\")   subset_data <- backend_get_data(backend, rows = 1:10, cols = 1:20)   stopifnot(dim(subset_data)[1] == 10)   stopifnot(dim(subset_data)[2] == 20)   cat(\" ✓\\n\")      # Test closing   cat(\"Testing close...\")   backend_close(backend)   cat(\" ✓\\n\")      cat(\"\\n✅ All tests passed!\\n\") }  # Test our JSON backend test_backend(json_backend, json_file) #> Testing backend implementation... #>  #> Testing construction... ✓ #> Testing open... ✓ #> Testing dimensions... ✓ #> Testing mask... ✓ #> Testing data access... ✓ #> Testing subsetting... ✓ #> Testing close... ✓ #>  #> ✅ All tests passed!"},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"troubleshooting","dir":"","previous_headings":"","what":"Troubleshooting","title":"Motivation: Why Create Custom Backends?","text":"Common issues developing backends solutions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"dimension-mismatches","dir":"","previous_headings":"","what":"Dimension Mismatches","title":"Motivation: Why Create Custom Backends?","text":"Problem: “Error: Mask length match spatial dimensions” Solution: Ensure length(backend_get_mask(backend)) equals prod(backend_get_dims(backend)$spatial)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"memory-issues","dir":"","previous_headings":"","what":"Memory Issues","title":"Motivation: Why Create Custom Backends?","text":"Problem: Large datasets cause memory errors Solution: Implement lazy loading support partial data access row/column subsetting","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"state-management","dir":"","previous_headings":"","what":"State Management","title":"Motivation: Why Create Custom Backends?","text":"Problem: “Error: Backend open” methods Solution: Always check is_open flag provide informative error messages","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"integration-with-other-vignettes","dir":"","previous_headings":"","what":"Integration with Other Vignettes","title":"Motivation: Why Create Custom Backends?","text":"backend development guide connects : Prerequisites: - Getting Started - Understand backends fit ecosystem - Architecture Overview - Learn design principles Next Steps: - Backend Registry - Register backend automatic selection - Advanced Backend Patterns - Sophisticated techniques production backends Applications: - H5 Backend Usage - See production backend action","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/backend-development-basics.html","id":"session-information","dir":"","previous_headings":"","what":"Session Information","title":"Motivation: Why Create Custom Backends?","text":"","code":"sessionInfo() #> R version 4.3.2 (2023-10-31) #> Platform: aarch64-apple-darwin20 (64-bit) #> Running under: macOS Sonoma 14.3 #>  #> Matrix products: default #> BLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib  #> LAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0 #>  #> locale: #> [1] en_CA.UTF-8/en_CA.UTF-8/en_CA.UTF-8/C/en_CA.UTF-8/en_CA.UTF-8 #>  #> time zone: America/Toronto #> tzcode source: internal #>  #> attached base packages: #> [1] stats     graphics  grDevices utils     datasets  methods   base      #>  #> other attached packages: #> [1] fmridataset_0.8.9 #>  #> loaded via a namespace (and not attached): #>   [1] checklist_0.4.2       remotes_2.5.0         rlang_1.1.6           #>   [4] magrittr_2.0.3        hunspell_3.0.6        matrixStats_1.5.0     #>   [7] compiler_4.3.2        callr_3.7.6           vctrs_0.6.5           #>  [10] stringr_1.5.1         profvis_0.4.0         pkgconfig_2.0.3       #>  [13] crayon_1.5.3          fastmap_1.2.0         backports_1.5.0       #>  [16] XVector_0.42.0        ellipsis_0.3.2        promises_1.3.3        #>  [19] rmarkdown_2.29        sessioninfo_1.2.3     ps_1.9.1              #>  [22] fmrihrf_0.1.0         purrr_1.0.4           xfun_0.52             #>  [25] gert_2.1.5            zlibbioc_1.48.2       cachem_1.1.0          #>  [28] rmio_0.4.0            neuroim2_0.8.1        jsonlite_2.0.0        #>  [31] later_1.4.2           DelayedArray_0.28.0   parallel_4.3.2        #>  [34] prettyunits_1.2.0     R6_2.6.1              stringi_1.8.7         #>  [37] RColorBrewer_1.1-3    pkgload_1.4.0         numDeriv_2016.8-1.1   #>  [40] Rcpp_1.1.0            assertthat_0.2.1      iterators_1.0.14      #>  [43] knitr_1.50            usethis_3.1.0         IRanges_2.36.0        #>  [46] tidyselect_1.2.1      httpuv_1.6.16         Matrix_1.6-5          #>  [49] splines_4.3.2         abind_1.4-8           yaml_2.3.10           #>  [52] doParallel_1.0.17     codetools_0.2-19      miniUI_0.1.2          #>  [55] curl_6.4.0            processx_3.8.6        pkgbuild_1.4.8        #>  [58] lattice_0.21-9        tibble_3.3.0          shiny_1.10.0          #>  [61] withr_3.0.2           askpass_1.2.1         evaluate_1.0.4        #>  [64] desc_1.4.3            RcppParallel_5.1.10   urlchecker_1.0.1      #>  [67] xml2_1.3.8            pillar_1.11.0         MatrixGenerics_1.14.0 #>  [70] RNifti_1.8.0          foreach_1.5.2         stats4_4.3.2          #>  [73] rex_1.2.1             bigassertr_0.1.7      mmap_0.6-22           #>  [76] dbscan_1.2.2          generics_0.1.4        pingr_2.0.5           #>  [79] rprojroot_2.0.4       credentials_2.0.2     xopen_1.0.1           #>  [82] S4Vectors_0.40.2      ggplot2_3.5.2         codemetar_0.3.5       #>  [85] scales_1.4.0          xtable_1.8-4          glue_1.8.0            #>  [88] lazyeval_0.2.2        tools_4.3.2           deflist_0.2.0         #>  [91] sys_3.4.3             fs_1.6.6              cowplot_1.2.0         #>  [94] grid_4.3.2            gh_1.5.0              colorspace_2.1-1      #>  [97] lintr_3.2.0           devtools_2.4.5        flock_0.7             #> [100] RNiftyReg_2.8.4       cli_3.6.5             bigparallelr_0.3.2    #> [103] rcmdcheck_1.4.0       S4Arrays_1.2.1        dplyr_1.1.4           #> [106] gtable_0.3.6          digest_0.6.37         BiocGenerics_0.48.1   #> [109] SparseArray_1.2.4     htmlwidgets_1.6.4     farver_2.1.2          #> [112] memoise_2.0.1         htmltools_0.5.8.1     pkgdown_2.1.3         #> [115] lifecycle_1.0.4       httr_1.4.7            bigstatsr_1.6.1       #> [118] mime_0.13             openssl_2.3.3"},{"path":"https://bbuchsbaum.github.io/fmridataset/fmridataset_cheatsheet_v3.html","id":"micro-dsl-v34-source-format-grammar","dir":"","previous_headings":"","what":"Micro-DSL v3.4 Source Format Grammar","title":"NA","text":"HUMAN-READABLE source format v3.4. extends v3.3 semantic annotations constrained types maintaining clarity completeness. separate compilation step produces compressed format. Budget: Target ≤ 250 lines ≤ 2,500 tokens optimal LLM processing. 1. Document Structure: 2. Sigils (v3.3): 3. Type System (v3.4 Enhanced Constraints): 4. Entry Format (v3.4 Enhanced): Standard aliases (use default): - DF data frame arguments - Fml formula arguments (fml) - V<T> vectors brevity helps 6. Constraint Definitions (v3.4 New): 7. Class Documentation (v3.4 Enhanced): 8. Metadata Tags (v3.3 + v3.4 additions): 9. Semantic Annotations (v3.4 New): 10. Structured Return Types (v3.4 New): 11. Example Entry (v3.4): 12. Conditional Constraints (v3.4): 13. Best Practices (v3.4): - Use specific sigils (@s3g @g) - Always specify vector vs scalar types - Use standard type aliases (DF, Fml, V) - Add constraints match.arg/stopifnot/checks - Keep !cov lists short (3-6 classes max) - Document semantic relationships concisely - Use structured types complex returns - Define reusable constraints @constraint - Include conditional logic @/@implies - Group related functions +family tags - Mark side effects detailed sub-facets - Stay within budget (≤250 lines) 14. Meta-Footer: 15. Export Detection Priority: 1. NAMESPACE file: export(), S3method(), exportClasses(), exportMethods(), exportPattern() 2. Roxygen tags: @export documentation 3. neither present: skip symbol (guess include) 16. Inference Heuristics (apply silently): - Type defaults: TRUE/FALSE → lgl, “text” → chr, 1L → int, 1.0 → dbl - Common patterns: data/df/tbl → DF, formula → Fml, weights → vec - Enums: match.arg(x, c(“”,“b”)) → chr[“”|“b”] - Ranges: stopifnot(x >= 0 && x <= 1) → dbl[0..1] - Side effects: file.* → fs, plot/ggplot → plot, message/cat → console - Determinism: runif/sample/rnorm → +deterministic:false","code":"@pkg package_name | description [Type Aliases section] [Constraint Definitions section] [Legend section if needed] # Package Name [Sections with entries] [Dependencies section] [Meta-Footer] @pkg - Package declaration @f   - Function @d   - Data object @x   - Re-export from another package  S3 System: @s3g - S3 generic (UseMethod) @s3m - S3 method (generic.class) @s3c - S3 class definition  S4 System: @s4g - S4 generic (setGeneric) @s4m - S4 method (setMethod) @s4c - S4 class (setClass)  S7 System: @s7g - S7 generic (new_generic) @s7m - S7 method @s7c - S7 class (new_class)  R6 System: @r6c - R6 class (R6Class) Scalars (default): int, dbl, chr, lgl, raw, cpl Vectors: vec<type> or type[] Matrices: mat<type> or mat<type,rows,cols> Arrays: arr<type,dims> Lists: lst<type> or lst{field:type, ...} (structured) Data frames: df, tbl, data.table Factors: fct, ord Dates: Date, POSIXct, POSIXlt  Union types: type1|type2|type3 Nullable: type? (shorthand for type|NULL) Any type: any Ellipsis: ... or ...:type (e.g., ...:expr for NSE)  Class types: s3:classname, s4:classname, r6:classname, s7:classname  CONSTRAINED TYPES (v3.4): Enums: chr[\"opt1\"|\"opt2\"|\"opt3\"] Ranges: int[min..max], dbl[min..max] Patterns: chr[/regex/] Exclusions: int[1..100]&!=[13,17] References: @ref:constraint_name @sigil name (param1:type1[constraint]?=default, param2:type2, ...) | Description -> return_type | return_schema   +tag:value +tag:value   !cov [Class1, Class2] (for generics)   - param1 : Additional description     @annotation:value @annotation:value   - param2 : (constants: \"a\", \"b\", CONST) Valid values     @requires:condition @affects:target   - param3 : (key_funcs: func1, func2) Related functions     @lifecycle:init @units:measurement   ```verbatim   # Optional verbatim R code block **5. Type Aliases Section (v3.4 Enhanced):** ```markdown ## Type Aliases: DF = df|tbl|data.table              # Standard data frame types V<T> = vec<T>                       # Vector shorthand Fml = s3:formula                    # Formula objects Gg = s3:ggplot                      # ggplot2 objects Config = lst{method:chr, opts:lst}  # Structured config ValidPort = int[1024..65535]       # Constrained port range ## Constraint Definitions: @constraint positive_weights | Positive numeric weights   type: vec<dbl>   validates: all(. > 0)   length: @env:nrow(data)    @constraint valid_identifier | Valid R identifier   type: chr   pattern: /^[a-zA-Z_][a-zA-Z0-9_.]*$/   not_reserved: TRUE @s4c ClassName | One-line description   - slots: name1 (type1[constraint]) @annotation:value            name2 (type2) @lifecycle:init @immutable   - extends: ParentClass   - validity: Description of validity rules  @r6c ClassName | One-line description   - fields: field1 (type1[constraint]) @purpose:role             field2 (type2) @lazy @cached   - methods: method1 (args) -> ret_type              method2 (args) -> ret_type   - inherits: ParentClass +family:group_name           # Function family +pipe:in|out                # Pipe compatibility (in, out, or both) +nse:param1,param2          # Parameters using NSE +side:effect[details]       # Side effects with sub-facets   - fs[read|write|delete]   # File system operations   - plot[device|file]       # Graphics output   - console[print|message|warning]  # Console output   - network[http|socket|download]   # Network operations   - options[get|set|env]    # Global options/environment   - db[read|write|query]    # Database operations +perf:O(complexity)         # Performance complexity +mem:usage                  # Memory usage pattern +compute:intensity          # Computational intensity +deprecated:replacement     # Deprecation with suggested alternative +wraps:function            # This function wraps another +calls:func1,func2         # Functions called internally +see:related1,related2     # Related functions to consider +parallel:capable          # Can use parallel processing (v3.4) +deterministic:false       # Non-deterministic results (v3.4) +pure:false               # Has side effects (v3.4) BEHAVIORAL: @controls:aspect          # Parameter controls specific behavior @affects:target          # Changes affect another component @modifies:target         # Directly modifies target  DEPENDENCY: @requires:condition      # Prerequisite condition @conflicts:parameter     # Mutually exclusive with @extends:base           # Extends functionality  VALIDATION: @validates:constraint    # Validation rule @range:[min,max]        # Numeric range @length:constraint      # Length requirement @pattern:regex          # Pattern matching  SEMANTIC ROLE: @purpose:role           # Semantic purpose @units:measurement      # Physical/logical units @example:value          # Example values @default-reason:why     # Why this default  LIFECYCLE: @lifecycle:stage        # When relevant (init|config|runtime|cleanup) @immutable             # Cannot be modified @cached                # Result is cached @lazy                  # Evaluated on demand  CONDITIONAL: @when:condition        # Conditional applicability @implies:consequence   # Logical implication @if:cond @then:result  # If-then constraints -> lst{   field1: type1 @annotation,   field2: type2[constraint] @annotation,   nested: lst{     subfield: type   } } @f analyze_model (   model:s3:lm,   type:chr[\"summary\"|\"anova\"|\"diagnostics\"]?=\"summary\",   conf.level:dbl[0.5..0.99]?=0.95 ) | Analyze fitted model -> lst{   statistics: df @purpose:results,   plots: lst<s3:ggplot>? @when:type=\"diagnostics\",   interpretation: chr @purpose:summary }   +family:analysis +compute:light   - model : @requires:fitted @validates:has-residuals   - type : @controls:output-format @affects:return-structure   - conf.level : @purpose:confidence @affects:statistics.ci @f process_data (   data:df,   method:chr[\"scale\"|\"center\"|\"none\"]?=\"none\",   scale.center:lgl?=TRUE,   scale.scale:lgl?=TRUE ) | Process data with scaling options -> df   - method : @controls:processing   - scale.center : @when:method=\"scale\" @requires:TRUE                    @when:method=\"center\" @implies:scale.scale=FALSE   - scale.scale : @when:method=\"scale\" @default:TRUE                   @conflicts:method=\"center\" --- ## Meta-Footer - Micro-DSL Version: v3.4-source - Package: {pkg} (Version: X.Y.Z) - Generated: [ISO-8601 timestamp] - Features: types[constrained] sigils[specific] metadata[rich] semantics[annotated] - Coverage: {n_documented_exports} / {n_total_exports} exports - Provenance: exports[NAMESPACE], enums[match.arg/switch], constraints[assertions/checks] @pkg fmridataset | Unified Container for fMRI Datasets  ## Type Aliases: DF = df|tbl|data.table V<T> = vec<T> Fml = s3:formula Gg = s3:ggplot  ## Constraint Definitions: @constraint positive_weights | Positive numeric weights   type: vec<dbl>   validates: all(. > 0)  # fmridataset  ## 1. Core  @f fmri_dataset (scans:chr[], mask:chr, TR:dbl, run_length:int[], event_table:DF?=data.frame(), base_path:chr?=\".\", censor:vec<int>?=NULL, preload:lgl?=FALSE, mode:chr[\"normal\"|\"bigvec\"|\"mmap\"|\"filebacked\"]?=\"normal\", dummy_mode:lgl?=FALSE) | Create fMRI dataset -> s3:fmri_dataset   +family:dataset +pipe:in +side:fs[read] +see:fmri_mem_dataset, fmri_h5_dataset   - scans : @requires:files-exist @purpose:data-files   - mask : @requires:file-exist @purpose:mask-file   - TR : @units:seconds @purpose:temporal-resolution   - run_length : @purpose:run-lengths   - event_table : @purpose:event-metadata   - base_path : @purpose:relative-paths   - censor : @purpose:exclude-scans   - preload : @purpose:load-strategy   - mode : @purpose:storage-mode   - dummy_mode : @purpose:test-mode  @f fmri_mem_dataset (scans:lst<s3:NeuroVec>, mask:s3:NeuroVol, TR:dbl, run_length:int[], event_table:DF?=data.frame(), base_path:chr?=\".\", censor:vec<int>?=NULL) | Create in-memory fMRI dataset -> s3:fmri_mem_dataset   +family:dataset +pipe:in +side:fs[read] +see:fmri_dataset, fmri_h5_dataset   - scans : @requires:NeuroVec-objects @purpose:data-objects   - mask : @requires:NeuroVol-object @purpose:mask-object   - TR : @units:seconds @purpose:temporal-resolution   - run_length : @purpose:run-lengths   - event_table : @purpose:event-metadata   - base_path : @purpose:relative-paths   - censor : @purpose:exclude-scans  @f fmri_h5_dataset (h5_files:chr[], mask_source:chr|s3:NeuroVol, TR:dbl, run_length:int[], event_table:DF?=data.frame(), base_path:chr?=\".\", censor:vec<int>?=NULL, preload:lgl?=FALSE, mask_dataset:chr?=\"data/elements\", data_dataset:chr?=\"data\") | Create fMRI dataset from H5 files -> s3:fmri_file_dataset   +family:dataset +pipe:in +side:fs[read] +see:fmri_dataset, fmri_mem_dataset   - h5_files : @requires:files-exist @purpose:data-files   - mask_source : @requires:file-exist @purpose:mask-file   - TR : @units:seconds @purpose:temporal-resolution   - run_length : @purpose:run-lengths   - event_table : @purpose:event-metadata   - base_path : @purpose:relative-paths   - censor : @purpose:exclude-scans   - preload : @purpose:load-strategy   - mask_dataset : @purpose:mask-path   - data_dataset : @purpose:data-path  @f latent_dataset (source:chr[]|lst<s3:LatentNeuroVec>, TR:dbl, run_length:int[], event_table:DF?=data.frame(), base_path:chr?=\".\", censor:vec<int>?=NULL, preload:lgl?=FALSE) | Create latent space fMRI dataset -> s3:latent_dataset   +family:dataset +pipe:in +side:fs[read] +see:fmri_dataset, fmri_mem_dataset   - source : @requires:files-exist @purpose:data-files   - TR : @units:seconds @purpose:temporal-resolution   - run_length : @purpose:run-lengths   - event_table : @purpose:event-metadata   - base_path : @purpose:relative-paths   - censor : @purpose:exclude-scans   - preload : @purpose:load-strategy  @f get_latent_scores (x:s3:latent_dataset, rows:int[]?=NULL, cols:int[]?=NULL, ...:expr) | Extract latent scores -> mat<dbl>   +family:data-access +pipe:in +see:get_spatial_loadings, reconstruct_voxels   - x : @purpose:dataset   - rows : @purpose:subset-rows   - cols : @purpose:subset-cols  @f get_spatial_loadings (x:s3:latent_dataset, components:int[]?=NULL, ...:expr) | Extract spatial loadings -> mat<dbl>   +family:data-access +pipe:in +see:get_latent_scores, reconstruct_voxels   - x : @purpose:dataset   - components : @purpose:subset-components  @f get_component_info (x:s3:latent_dataset, ...:expr) | Get component metadata -> lst   +family:data-access +pipe:in +see:get_latent_scores, get_spatial_loadings   - x : @purpose:dataset  @f reconstruct_voxels (x:s3:latent_dataset, rows:int[]?=NULL, voxels:int[]?=NULL, ...:expr) | Reconstruct voxel data -> mat<dbl>   +family:data-access +pipe:in +see:get_latent_scores, get_spatial_loadings   - x : @purpose:dataset   - rows : @purpose:subset-rows   - voxels : @purpose:subset-voxels  @f fmri_series (dataset:s3:fmri_dataset, selector:any?=NULL, timepoints:int[]?=NULL, output:chr[\"fmri_series\"|\"DelayedMatrix\"]?=\"fmri_series\", event_window:any?=NULL, ...:expr) | Query fMRI time series -> s3:fmri_series|DelayedMatrix   +family:data-access +pipe:in +see:fmri_dataset, fmri_mem_dataset   - dataset : @purpose:dataset   - selector : @purpose:spatial-selector   - timepoints : @purpose:temporal-selector   - output : @purpose:return-format  @f fmri_group (subjects:DF, id:chr, dataset_col:chr?=\"dataset\", space:chr?=NULL, mask_strategy:chr[\"subject_specific\"|\"intersect\"|\"union\"]?=\"subject_specific\") | Create fMRI group -> s3:fmri_group   +family:group +pipe:in +see:fmri_dataset, fmri_mem_dataset   - subjects : @purpose:subject-data   - id : @purpose:identifier   - dataset_col : @purpose:dataset-column   - space : @purpose:common-space   - mask_strategy : @purpose:mask-strategy  @f iter_subjects (gd:s3:fmri_group, order_by:chr?=NULL) | Iterate subjects -> lst   +family:group +pipe:in +see:fmri_group, fmri_dataset   - gd : @purpose:group-dataset   - order_by : @purpose:order-iteration  @f group_map (gd:s3:fmri_group, .f:fml, ..., out:chr[\"list\"|\"bind_rows\"]?=\"list\", order_by:chr?=NULL, on_error:chr[\"stop\"|\"warn\"|\"skip\"]?=\"stop\") | Map function over subjects -> lst|DF   +family:group +pipe:in +see:fmri_group, fmri_dataset   - gd : @purpose:group-dataset   - .f : @purpose:map-function   - ... : @purpose:additional-args   - out : @purpose:output-format   - order_by : @purpose:order-iteration   - on_error : @purpose:error-handling  @f group_reduce (gd:s3:fmri_group, .map:fml, .reduce:fml, .init:any, order_by:chr?=NULL, on_error:chr[\"stop\"|\"warn\"|\"skip\"]?=\"stop\", ...) | Reduce over subjects -> any   +family:group +pipe:in +see:fmri_group, fmri_dataset   - gd : @purpose:group-dataset   - .map : @purpose:map-function   - .reduce : @purpose:reduce-function   - .init : @purpose:initial-value   - order_by : @purpose:order-iteration   - on_error : @purpose:error-handling  @f filter_subjects (gd:s3:fmri_group, ...:expr) | Filter subjects -> s3:fmri_group   +family:group +pipe:in +see:fmri_group, fmri_dataset   - gd : @purpose:group-dataset   - ... : @purpose:filter-expressions  @f mutate_subjects (gd:s3:fmri_group, ...:expr) | Mutate subject attributes -> s3:fmri_group   +family:group +pipe:in +see:fmri_group, fmri_dataset   - gd : @purpose:group-dataset   - ... : @purpose:mutate-expressions  @f left_join_subjects (gd:s3:fmri_group, y:DF, by:chr?=NULL, ...) | Left join subject metadata -> s3:fmri_group   +family:group +pipe:in +see:fmri_group, fmri_dataset   - gd : @purpose:group-dataset   - y : @purpose:join-data   - by : @purpose:join-keys  @f sample_subjects (gd:s3:fmri_group, n:int, replace:lgl?=FALSE, strata:chr?=NULL) | Sample subjects -> s3:fmri_group   +family:group +pipe:in +see:fmri_group, fmri_dataset   - gd : @purpose:group-dataset   - n : @purpose:sample-size   - replace : @purpose:sample-replacement   - strata : @purpose:sample-strata  @f stream_subjects (gd:s3:fmri_group, prefetch:int?=1, order_by:chr?=NULL) | Stream subjects -> lst   +family:group +pipe:in +see:fmri_group, fmri_dataset   - gd : @purpose:group-dataset   - prefetch : @purpose:prefetch-size   - order_by : @purpose:order-iteration  ## Dependencies - Imports: assertthat, cachem, deflist, fmrihrf, fs, lifecycle, memoise, Matrix, methods, neuroim2, purrr, tibble, DelayedArray, S4Vectors, utils - Suggests: bench, bidser, crayon, arrow, dplyr, fmristore, foreach, mockery, mockr, Rarr, testthat (>= 3.0.0), knitr, rmarkdown  --- ## Meta-Footer - Micro-DSL Version: v3.4-source - Package: fmridataset (Version: 0.8.9) - Generated: 2023-10-05T12:00:00Z - Features: types[constrained] sigils[specific] metadata[rich] semantics[annotated] - Coverage: 50 / 50 exports - Provenance: exports[NAMESPACE], enums[match.arg/switch], constraints[assertions/checks]"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Unified Container for fMRI Datasets","text":"fmridataset provides unified framework representing functional magnetic resonance imaging (fMRI) data various sources. package supports multiple data backends offers consistent interface working fMRI datasets regardless underlying storage format.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"features","dir":"","previous_headings":"","what":"Features","title":"Unified Container for fMRI Datasets","text":"Unified Interface: Work fMRI data NIfTI files, BIDS projects, pre-loaded NeuroVec objects, -memory matrices single API Lazy Loading: Efficient memory management -demand data loading Flexible Backends: Pluggable storage backends different data formats Data Chunking: Built-support processing large datasets chunks Temporal Structure: Rich sampling frame representation run lengths, TR, temporal organization Integration Ready: Seamlessly integrates neuroimaging analysis workflows","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Unified Container for fMRI Datasets","text":"can install development version fmridataset GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"bbuchsbaum/fmridataset\")"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"creating-datasets","dir":"","previous_headings":"Quick Start","what":"Creating Datasets","title":"Unified Container for fMRI Datasets","text":"","code":"library(fmridataset)  # From NIfTI files dataset <- fmri_dataset(   scans = c(\"run1.nii\", \"run2.nii\"),   mask = \"mask.nii\",    TR = 2.0,   run_length = c(240, 240) )  # From in-memory matrix mat_data <- matrix(rnorm(1000), nrow = 100, ncol = 10) dataset <- matrix_dataset(   datamat = mat_data,   TR = 1.5,    run_length = 100 )  # From pre-loaded NeuroVec objects   dataset <- fmri_mem_dataset(   scans = list(neurovec1, neurovec2),   mask = mask_vol,   TR = 2.0 )"},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"data-access","dir":"","previous_headings":"Quick Start","what":"Data Access","title":"Unified Container for fMRI Datasets","text":"","code":"# Get full data matrix data_matrix <- get_data_matrix(dataset)  # Get spatial mask mask <- get_mask(dataset)  # Access temporal properties n_timepoints(dataset$sampling_frame) n_runs(dataset$sampling_frame) get_TR(dataset$sampling_frame)"},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"data-chunking","dir":"","previous_headings":"Quick Start","what":"Data Chunking","title":"Unified Container for fMRI Datasets","text":"","code":"# Process data in chunks chunks <- data_chunks(dataset, nchunks = 5) for (i in 1:5) {   chunk <- chunks$nextElem()   # Process chunk$data, chunk$voxel_ind, etc. }  # Run-wise processing run_chunks <- data_chunks(dataset, runwise = TRUE) run1_data <- run_chunks$nextElem()"},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"type-conversions","dir":"","previous_headings":"Quick Start","what":"Type Conversions","title":"Unified Container for fMRI Datasets","text":"","code":"# Convert to matrix format mat_dataset <- as.matrix_dataset(dataset)  # All dataset types support the same interface print(dataset) summary(dataset$sampling_frame)"},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"architecture","dir":"","previous_headings":"","what":"Architecture","title":"Unified Container for fMRI Datasets","text":"package uses modular architecture following key components: Storage Backends: Pluggable data access layer (matrix_backend, nifti_backend) Dataset Constructors: High-level dataset creation functions Sampling Frames: Temporal structure representation Data Access Methods: Consistent interface data retrieval Chunking System: Efficient processing large datasets","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"related-packages","dir":"","previous_headings":"","what":"Related Packages","title":"Unified Container for fMRI Datasets","text":"neuroim2: Neuroimaging data structures fmristore: Advanced fMRI data storage bidser: BIDS dataset utilities","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"getting-help","dir":"","previous_headings":"","what":"Getting Help","title":"Unified Container for fMRI Datasets","text":"Check package documentation detailed guides Report bugs request features GitHub Issues See vignettes detailed examples","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Unified Container for fMRI Datasets","text":"Contributions welcome! Please see Contributing Guide details.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Unified Container for fMRI Datasets","text":"GPL (>= 3)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/all_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"All Voxels Series Selector — all_selector","title":"All Voxels Series Selector — all_selector","text":"Select voxels dataset mask.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/all_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"All Voxels Series Selector — all_selector","text":"","code":"all_selector()"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/all_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"All Voxels Series Selector — all_selector","text":"object class all_selector","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/all_selector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"All Voxels Series Selector — all_selector","text":"","code":"# Select all voxels sel <- all_selector()"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/all_timepoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper returning all timepoints for a dataset — all_timepoints","title":"Helper returning all timepoints for a dataset — all_timepoints","text":"Helper returning timepoints dataset","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/all_timepoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper returning all timepoints for a dataset — all_timepoints","text":"","code":"all_timepoints(dataset)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/all_timepoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper returning all timepoints for a dataset — all_timepoints","text":"dataset fmri_dataset object.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/all_timepoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper returning all timepoints for a dataset — all_timepoints","text":"Integer vector valid timepoint indices.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/analyze_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate Analysis Function for Examples — analyze_run","title":"Simulate Analysis Function for Examples — analyze_run","text":"Simulate Analysis Function Examples","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/analyze_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate Analysis Function for Examples — analyze_run","text":"","code":"analyze_run(data, method = \"mean\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/analyze_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate Analysis Function for Examples — analyze_run","text":"data Data matrix method Analysis method","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/analyze_run.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate Analysis Function for Examples — analyze_run","text":"Numeric vector results","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix.fmri_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert fmri_series to Matrix — as.matrix.fmri_series","title":"Convert fmri_series to Matrix — as.matrix.fmri_series","text":"method realizes underlying lazy matrix returns ordinary matrix timepoints rows voxels columns.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix.fmri_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert fmri_series to Matrix — as.matrix.fmri_series","text":"","code":"# S3 method for class 'fmri_series' as.matrix(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix.fmri_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert fmri_series to Matrix — as.matrix.fmri_series","text":"x fmri_series object ... Additional arguments (ignored)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix.fmri_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert fmri_series to Matrix — as.matrix.fmri_series","text":"matrix timepoints rows voxels columns","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix.fmri_series.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert fmri_series to Matrix — as.matrix.fmri_series","text":"","code":"# \\donttest{ # Create small example mat <- matrix(rnorm(20), nrow = 4, ncol = 5)  backend <- matrix_backend(mat, mask = rep(TRUE, ncol(mat))) dataset <- fmri_dataset(backend, TR = 1, run_length = nrow(mat)) fs <- fmri_series(dataset)  # Convert to matrix mat_result <- as.matrix(fs) dim(mat_result) # 4 x 5 #> [1] 4 5 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert to Matrix Dataset — as.matrix_dataset","title":"Convert to Matrix Dataset — as.matrix_dataset","text":"Generic function convert various fMRI dataset types matrix_dataset objects. Provides unified interface getting matrix-based representations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert to Matrix Dataset — as.matrix_dataset","text":"","code":"as.matrix_dataset(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert to Matrix Dataset — as.matrix_dataset","text":"x fMRI dataset object ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert to Matrix Dataset — as.matrix_dataset","text":"matrix_dataset object data input","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix_dataset.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert to Matrix Dataset — as.matrix_dataset","text":"function converts different dataset representations standard matrix_dataset format, stores data matrix timepoints rows voxels columns. useful algorithms require matrix operations consistent data format needed.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as.matrix_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert to Matrix Dataset — as.matrix_dataset","text":"","code":"# \\donttest{ # Convert various dataset types to matrix_dataset # (example requires actual dataset object) # mat_ds <- as.matrix_dataset(some_dataset) # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delarr.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert backend to a delarr lazy matrix — as_delarr","title":"Convert backend to a delarr lazy matrix — as_delarr","text":"Provides lightweight S3 interface defers materialization backend data. returned object compatible delarr::collect() well base .matrix() realization.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delarr.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert backend to a delarr lazy matrix — as_delarr","text":"","code":"as_delarr(backend, ...)  # S3 method for class 'matrix_backend' as_delarr(backend, ...)  # S3 method for class 'nifti_backend' as_delarr(backend, ...)  # S3 method for class 'study_backend' as_delarr(backend, ...)  # Default S3 method as_delarr(backend, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delarr.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert backend to a delarr lazy matrix — as_delarr","text":"backend storage backend object ... Passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delarr.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert backend to a delarr lazy matrix — as_delarr","text":"delarr lazy matrix","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delayed_array-dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Dataset Objects to DelayedArray — as_delayed_array-dataset","title":"Convert Dataset Objects to DelayedArray — as_delayed_array-dataset","text":"Provides DelayedArray interface dataset objects. methods convert fmri_dataset matrix_dataset objects DelayedArrays memory-efficient operations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delayed_array-dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Dataset Objects to DelayedArray — as_delayed_array-dataset","text":"","code":"# S4 method for class 'matrix_dataset' as_delayed_array(backend, sparse_ok = FALSE)  # S4 method for class 'fmri_file_dataset' as_delayed_array(backend, sparse_ok = FALSE)  # S4 method for class 'fmri_mem_dataset' as_delayed_array(backend, sparse_ok = FALSE)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delayed_array-dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Dataset Objects to DelayedArray — as_delayed_array-dataset","text":"backend storage backend object sparse_ok Logical, allow sparse representation possible","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delayed_array.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Backend to DelayedArray — as_delayed_array","title":"Convert Backend to DelayedArray — as_delayed_array","text":"Provides DelayedArray interface storage backends. returned object lazily retrieves data via backend subsets array accessed.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delayed_array.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Backend to DelayedArray — as_delayed_array","text":"","code":"as_delayed_array(backend, sparse_ok = FALSE, ...)  # S3 method for class 'nifti_backend' as_delayed_array(backend, sparse_ok = FALSE, ...)  # S3 method for class 'matrix_backend' as_delayed_array(backend, sparse_ok = FALSE, ...)  # S3 method for class 'study_backend' as_delayed_array(backend, sparse_ok = FALSE, ...)  # Default S3 method as_delayed_array(backend, sparse_ok = FALSE, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delayed_array.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Backend to DelayedArray — as_delayed_array","text":"backend storage backend object sparse_ok Logical, allow sparse representation possible ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delayed_array.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Backend to DelayedArray — as_delayed_array","text":"DelayedArray object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_delayed_array.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert Backend to DelayedArray — as_delayed_array","text":"","code":"if (FALSE) { # \\dontrun{ b <- matrix_backend(matrix(rnorm(20), nrow = 5)) da <- as_delayed_array(b) dim(da) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_fmri_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Coerce a data frame into an fmri_group — as_fmri_group","title":"Coerce a data frame into an fmri_group — as_fmri_group","text":"Coerce data frame fmri_group","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_fmri_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Coerce a data frame into an fmri_group — as_fmri_group","text":"","code":"as_fmri_group(   subjects,   id,   dataset_col = \"dataset\",   space = NULL,   mask_strategy = c(\"subject_specific\", \"intersect\", \"union\") )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_fmri_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Coerce a data frame into an fmri_group — as_fmri_group","text":"subjects data.frame (tibble) one row per subject one column contains per-subject fmri_dataset objects stored list column. id Character scalar giving name subject identifier column. dataset_col Character scalar naming list column stores per-subject dataset handles. space Optional character string describing nominal common space subjects (e.g., \"MNI152NLin2009cAsym\"). mask_strategy One \"subject_specific\", \"intersect\", \"union\" describing masks handled combining subjects. declarative flag ; resampling performed constructor.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_fmri_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Coerce a data frame into an fmri_group — as_fmri_group","text":"fmri_group object.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_tibble.fmri_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert fmri_series to Tibble — as_tibble.fmri_series","title":"Convert fmri_series to Tibble — as_tibble.fmri_series","text":"returned tibble contains one row per voxel/timepoint combination metadata columns temporal_info voxel_info signal column data values.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_tibble.fmri_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert fmri_series to Tibble — as_tibble.fmri_series","text":"","code":"# S3 method for class 'fmri_series' as_tibble(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_tibble.fmri_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert fmri_series to Tibble — as_tibble.fmri_series","text":"x fmri_series object ... Additional arguments (ignored)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_tibble.fmri_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert fmri_series to Tibble — as_tibble.fmri_series","text":"tibble columns temporal_info, voxel_info, signal column containing fMRI signal values","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_tibble.fmri_series.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert fmri_series to Tibble — as_tibble.fmri_series","text":"","code":"# \\donttest{ # Create small example mat <- matrix(rnorm(12), nrow = 3, ncol = 4)  backend <- matrix_backend(mat, mask = rep(TRUE, ncol(mat))) dataset <- fmri_dataset(backend, TR = 1, run_length = nrow(mat)) fs <- fmri_series(dataset)  # Convert to tibble tbl_result <- tibble::as_tibble(fs) # Result has 12 rows (3 timepoints x 4 voxels) # with columns: time, voxel, signal # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_tibble.fmri_study_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert fmri_study_dataset to a tibble or lazy matrix — as_tibble.fmri_study_dataset","title":"Convert fmri_study_dataset to a tibble or lazy matrix — as_tibble.fmri_study_dataset","text":"Primary data access method study-level datasets. default returns lazy matrix (typically delarr object) row-wise metadata attached. materialise = TRUE, data matrix materialised returned tibble metadata columns prepended.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_tibble.fmri_study_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert fmri_study_dataset to a tibble or lazy matrix — as_tibble.fmri_study_dataset","text":"","code":"# S3 method for class 'fmri_study_dataset' as_tibble(x, materialise = FALSE, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_tibble.fmri_study_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert fmri_study_dataset to a tibble or lazy matrix — as_tibble.fmri_study_dataset","text":"x fmri_study_dataset object materialise Logical; return materialised tibble? Default FALSE. ... Additional arguments (unused)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/as_tibble.fmri_study_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert fmri_study_dataset to a tibble or lazy matrix — as_tibble.fmri_study_dataset","text":"Either lazy matrix metadata attributes tibble materialise = TRUE.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend-registry.html","id":null,"dir":"Reference","previous_headings":"","what":"Backend Registry System — backend-registry","title":"Backend Registry System — backend-registry","text":"pluggable registry system storage backends allows external packages register new backend types without modifying fmridataset package. enables extensibility maintaining backward compatibility.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend-registry.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Backend Registry System — backend-registry","text":"registry system manages backend factories create backend instances. backend must implement storage backend contract defined storage-backend.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_close.html","id":null,"dir":"Reference","previous_headings":"","what":"Close a Storage Backend — backend_close.h5_backend","title":"Close a Storage Backend — backend_close.h5_backend","text":"Closes storage backend releases resources. Stateless backends can implement -op.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_close.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Close a Storage Backend — backend_close.h5_backend","text":"","code":"# S3 method for class 'h5_backend' backend_close(backend)  # S3 method for class 'latent_backend' backend_close(backend)  # S3 method for class 'matrix_backend' backend_close(backend)  # S3 method for class 'nifti_backend' backend_close(backend)  backend_close(backend)  # S3 method for class 'study_backend' backend_close(backend)  # S3 method for class 'zarr_backend' backend_close(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_close.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Close a Storage Backend — backend_close.h5_backend","text":"backend storage backend object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_close.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Close a Storage Backend — backend_close.h5_backend","text":"NULL (invisibly)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Data from Backend — backend_get_data.h5_backend","title":"Get Data from Backend — backend_get_data.h5_backend","text":"Reads data backend canonical timepoints × voxels orientation.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Data from Backend — backend_get_data.h5_backend","text":"","code":"# S3 method for class 'h5_backend' backend_get_data(backend, rows = NULL, cols = NULL)  # S3 method for class 'latent_backend' backend_get_data(backend, rows = NULL, cols = NULL)  # S3 method for class 'matrix_backend' backend_get_data(backend, rows = NULL, cols = NULL)  # S3 method for class 'nifti_backend' backend_get_data(backend, rows = NULL, cols = NULL)  backend_get_data(backend, rows = NULL, cols = NULL)  # S3 method for class 'study_backend' backend_get_data(backend, rows = NULL, cols = NULL)  # S3 method for class 'zarr_backend' backend_get_data(backend, rows = NULL, cols = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Data from Backend — backend_get_data.h5_backend","text":"backend storage backend object rows Integer vector row indices (timepoints) read, NULL cols Integer vector column indices (voxels) read, NULL ","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Data from Backend — backend_get_data.h5_backend","text":"matrix timepoints × voxels orientation","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_dims.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Dimensions from Backend — backend_get_dims.h5_backend","title":"Get Dimensions from Backend — backend_get_dims.h5_backend","text":"Returns dimensions data stored backend.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_dims.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Dimensions from Backend — backend_get_dims.h5_backend","text":"","code":"# S3 method for class 'h5_backend' backend_get_dims(backend)  # S3 method for class 'latent_backend' backend_get_dims(backend)  # S3 method for class 'matrix_backend' backend_get_dims(backend)  # S3 method for class 'nifti_backend' backend_get_dims(backend)  backend_get_dims(backend)  # S3 method for class 'study_backend' backend_get_dims(backend)  # S3 method for class 'zarr_backend' backend_get_dims(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_dims.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Dimensions from Backend — backend_get_dims.h5_backend","text":"backend storage backend object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_dims.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Dimensions from Backend — backend_get_dims.h5_backend","text":"named list elements: spatial: numeric vector length 3 (x, y, z dimensions) time: integer, number timepoints","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_loadings.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Spatial Loadings from Latent Backend — backend_get_loadings","title":"Get Spatial Loadings from Latent Backend — backend_get_loadings","text":"Get Spatial Loadings Latent Backend","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_loadings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Spatial Loadings from Latent Backend — backend_get_loadings","text":"","code":"backend_get_loadings(backend, components = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_loadings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Spatial Loadings from Latent Backend — backend_get_loadings","text":"backend latent_backend object components Optional component indices extract","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_loadings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Spatial Loadings from Latent Backend — backend_get_loadings","text":"Matrix sparse matrix spatial loadings","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_mask.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Mask from Backend — backend_get_mask.h5_backend","title":"Get Mask from Backend — backend_get_mask.h5_backend","text":"Returns logical mask indicating voxels contain valid data.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_mask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Mask from Backend — backend_get_mask.h5_backend","text":"","code":"# S3 method for class 'h5_backend' backend_get_mask(backend)  # S3 method for class 'latent_backend' backend_get_mask(backend)  # S3 method for class 'matrix_backend' backend_get_mask(backend)  # S3 method for class 'nifti_backend' backend_get_mask(backend)  backend_get_mask(backend)  # S3 method for class 'study_backend' backend_get_mask(backend)  # S3 method for class 'zarr_backend' backend_get_mask(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_mask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Mask from Backend — backend_get_mask.h5_backend","text":"backend storage backend object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_mask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Mask from Backend — backend_get_mask.h5_backend","text":"logical vector satisfying: length(mask) == prod(backend_get_dims(backend)$spatial) sum(mask) > 0 (empty masks allowed) NA values allowed","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_metadata.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Metadata from Backend — backend_get_metadata.h5_backend","title":"Get Metadata from Backend — backend_get_metadata.h5_backend","text":"Returns metadata associated data (e.g., affine matrix, voxel dimensions).","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_metadata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Metadata from Backend — backend_get_metadata.h5_backend","text":"","code":"# S3 method for class 'h5_backend' backend_get_metadata(backend)  # S3 method for class 'latent_backend' backend_get_metadata(backend)  # S3 method for class 'matrix_backend' backend_get_metadata(backend)  # S3 method for class 'nifti_backend' backend_get_metadata(backend)  backend_get_metadata(backend)  # S3 method for class 'zarr_backend' backend_get_metadata(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_metadata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Metadata from Backend — backend_get_metadata.h5_backend","text":"backend storage backend object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_get_metadata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Metadata from Backend — backend_get_metadata.h5_backend","text":"list containing neuroimaging metadata, may include: affine: 4x4 affine transformation matrix voxel_dims: numeric vector voxel dimensions intent_code: NIfTI intent code Additional format-specific metadata","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_open.html","id":null,"dir":"Reference","previous_headings":"","what":"Open a Storage Backend — backend_open.h5_backend","title":"Open a Storage Backend — backend_open.h5_backend","text":"Opens storage backend acquires necessary resources (e.g., file handles). Stateless backends can implement -op.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_open.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Open a Storage Backend — backend_open.h5_backend","text":"","code":"# S3 method for class 'h5_backend' backend_open(backend)  # S3 method for class 'latent_backend' backend_open(backend)  # S3 method for class 'matrix_backend' backend_open(backend)  # S3 method for class 'nifti_backend' backend_open(backend)  backend_open(backend)  # S3 method for class 'study_backend' backend_open(backend)  # S3 method for class 'zarr_backend' backend_open(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_open.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Open a Storage Backend — backend_open.h5_backend","text":"backend storage backend object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_open.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Open a Storage Backend — backend_open.h5_backend","text":"backend object (possibly modified state)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_reconstruct_voxels.html","id":null,"dir":"Reference","previous_headings":"","what":"Reconstruct Voxel Data from Latent Backend — backend_reconstruct_voxels","title":"Reconstruct Voxel Data from Latent Backend — backend_reconstruct_voxels","text":"Reconstruct Voxel Data Latent Backend","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_reconstruct_voxels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reconstruct Voxel Data from Latent Backend — backend_reconstruct_voxels","text":"","code":"backend_reconstruct_voxels(backend, rows = NULL, voxels = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_reconstruct_voxels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reconstruct Voxel Data from Latent Backend — backend_reconstruct_voxels","text":"backend latent_backend object rows Optional row indices (timepoints) voxels Optional voxel indices","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/backend_reconstruct_voxels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reconstruct Voxel Data from Latent Backend — backend_reconstruct_voxels","text":"Matrix reconstructed voxel data","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blockids.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Block IDs from Sampling Frame — blockids","title":"Get Block IDs from Sampling Frame — blockids","text":"Generates vector block/run identifiers timepoint.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blockids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Block IDs from Sampling Frame — blockids","text":"","code":"blockids(x, ...)  # S3 method for class 'matrix_dataset' blockids(x, ...)  # S3 method for class 'fmri_dataset' blockids(x, ...)  # S3 method for class 'fmri_mem_dataset' blockids(x, ...)  # S3 method for class 'fmri_file_dataset' blockids(x, ...)  # S3 method for class 'fmri_study_dataset' blockids(x, ...)  # S3 method for class 'sampling_frame' blockids(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blockids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Block IDs from Sampling Frame — blockids","text":"x object containing temporal structure (e.g., sampling_frame, fmri_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blockids.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Block IDs from Sampling Frame — blockids","text":"Integer vector length equal total timepoints, values indicating run membership (1 first run, 2 second, etc.)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blockids.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Block IDs from Sampling Frame — blockids","text":"function creates vector element indicates run/block corresponding timepoint belongs . useful run-wise analyses modeling run effects.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blockids.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Block IDs from Sampling Frame — blockids","text":"","code":"# \\donttest{ # Create a sampling frame with 2 runs of different lengths sf <- fmrihrf::sampling_frame(blocklens = c(3, 4), TR = 2) blockids(sf) # Returns: c(1, 1, 1, 2, 2, 2, 2) #> [1] 1 1 1 2 2 2 2 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blocklens.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Block Lengths from Objects — blocklens","title":"Get Block Lengths from Objects — blocklens","text":"Generic function extract block/run lengths various objects. Extends sampling_frame generic work dataset objects.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blocklens.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Block Lengths from Objects — blocklens","text":"","code":"blocklens(x, ...)  # S3 method for class 'matrix_dataset' blocklens(x, ...)  # S3 method for class 'fmri_dataset' blocklens(x, ...)  # S3 method for class 'fmri_mem_dataset' blocklens(x, ...)  # S3 method for class 'fmri_file_dataset' blocklens(x, ...)  # S3 method for class 'fmri_study_dataset' blocklens(x, ...)  # S3 method for class 'sampling_frame' blocklens(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blocklens.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Block Lengths from Objects — blocklens","text":"x object block structure (e.g., sampling_frame, fmri_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blocklens.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Block Lengths from Objects — blocklens","text":"Integer vector element represents number timepoints corresponding run/block","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blocklens.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Block Lengths from Objects — blocklens","text":"fMRI experiments, data often collected multiple runs blocks. function extracts length (number timepoints) run. sum block lengths equals total number timepoints.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/blocklens.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Block Lengths from Objects — blocklens","text":"","code":"# \\donttest{ # Create a dataset with 3 runs sf <- fmrihrf::sampling_frame(blocklens = c(100, 120, 110), TR = 2) blocklens(sf) # c(100, 120, 110) #> [1] 100 120 110  # Create dataset with multiple runs mat <- matrix(rnorm(330 * 50), nrow = 330, ncol = 50) ds <- matrix_dataset(mat, TR = 2, run_length = c(100, 120, 110)) blocklens(ds) # c(100, 120, 110) #> [1] 100 120 110 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/build_temporal_info_lazy.html","id":null,"dir":"Reference","previous_headings":"","what":"Temporal metadata builders for fmri_series — build_temporal_info_lazy","title":"Temporal metadata builders for fmri_series — build_temporal_info_lazy","text":"Internal helpers used construct temporal_info component fmri_series objects. functions return data.frame objects describing selected timepoint. exported users.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/build_temporal_info_lazy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Temporal metadata builders for fmri_series — build_temporal_info_lazy","text":"","code":"build_temporal_info_lazy(dataset, time_indices)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/collect_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"Collect all chunks from a chunk iterator — collect_chunks","title":"Collect all chunks from a chunk iterator — collect_chunks","text":"function collects chunks chunk iterator list.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/collect_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Collect all chunks from a chunk iterator — collect_chunks","text":"","code":"collect_chunks(chunk_iter)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/collect_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Collect all chunks from a chunk iterator — collect_chunks","text":"chunk_iter chunk iterator object created chunk_iter()","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/collect_chunks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Collect all chunks from a chunk iterator — collect_chunks","text":"list containing chunks iterator","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/create_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Backend Instance — create_backend","title":"Create Backend Instance — create_backend","text":"Creates backend instance using registered factory function. main interface creating backends name.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/create_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Backend Instance — create_backend","text":"","code":"create_backend(name, ..., validate = TRUE)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/create_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Backend Instance — create_backend","text":"name Character string, name registered backend type ... Arguments passed backend factory function validate Logical, whether validate created backend (default: TRUE)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/create_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Backend Instance — create_backend","text":"storage backend object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/create_backend.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Backend Instance — create_backend","text":"","code":"if (FALSE) { # \\dontrun{ # Create a NIfTI backend (assuming it's registered) backend <- create_backend(\"nifti\",   source = \"data.nii\",   mask_source = \"mask.nii\" )  # Create with validation disabled (faster, but riskier) backend <- create_backend(\"nifti\",   source = \"data.nii\",   mask_source = \"mask.nii\",   validate = FALSE ) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/create_study_cache.html","id":null,"dir":"Reference","previous_headings":"","what":"Create LRU Cache for Study Backend — create_study_cache","title":"Create LRU Cache for Study Backend — create_study_cache","text":"Create LRU Cache Study Backend","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/create_study_cache.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create LRU Cache for Study Backend — create_study_cache","text":"","code":"create_study_cache()"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunk.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Data Chunk Object — data_chunk","title":"Create a Data Chunk Object — data_chunk","text":"Creates data chunk object represents subset data fMRI dataset. data chunk contains data matrix along indices indicating voxels time points (rows) included chunk.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunk.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Data Chunk Object — data_chunk","text":"","code":"data_chunk(mat, voxel_ind, row_ind, chunk_num)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunk.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Data Chunk Object — data_chunk","text":"mat matrix containing chunk data (rows = time points, columns = voxels) voxel_ind Integer vector voxel indices included chunk row_ind Integer vector row (time point) indices included chunk chunk_num Integer indicating chunk number","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunk.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Data Chunk Object — data_chunk","text":"data_chunk object containing: data data matrix chunk voxel_ind Indices voxels chunk row_ind Indices rows (time points) chunk chunk_num chunk number","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunk.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Data Chunk Object — data_chunk","text":"","code":"# Create a simple data chunk mat <- matrix(rnorm(100), nrow = 10, ncol = 10) chunk <- data_chunk(mat, voxel_ind = 1:10, row_ind = 1:10, chunk_num = 1) print(chunk) #> Data Chunk Object #>   chunk 1 of 1 #>   Number of voxels: 10  #>   Number of rows: 10  #>   Data dimensions: 10 x 10"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.fmri_file_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Data Chunks for fmri_file_dataset Objects — data_chunks.fmri_file_dataset","title":"Create Data Chunks for fmri_file_dataset Objects — data_chunks.fmri_file_dataset","text":"function creates data chunks fmri_file_dataset objects. allows retrieval run-wise sequence-wise data chunks, well arbitrary chunks.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.fmri_file_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Data Chunks for fmri_file_dataset Objects — data_chunks.fmri_file_dataset","text":"","code":"# S3 method for class 'fmri_file_dataset' data_chunks(x, nchunks = 1, runwise = FALSE, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.fmri_file_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Data Chunks for fmri_file_dataset Objects — data_chunks.fmri_file_dataset","text":"x object class 'fmri_file_dataset'. nchunks number data chunks create. Default 1. runwise TRUE, data chunks created run-wise. Default FALSE. ... Additional arguments.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.fmri_file_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Data Chunks for fmri_file_dataset Objects — data_chunks.fmri_file_dataset","text":"list data chunks, chunk containing data, voxel indices, row indices, chunk number.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.fmri_mem_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Data Chunks for fmri_mem_dataset Objects — data_chunks.fmri_mem_dataset","title":"Create Data Chunks for fmri_mem_dataset Objects — data_chunks.fmri_mem_dataset","text":"function creates data chunks fmri_mem_dataset objects. allows retrieval run-wise sequence-wise data chunks, well arbitrary chunks.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.fmri_mem_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Data Chunks for fmri_mem_dataset Objects — data_chunks.fmri_mem_dataset","text":"","code":"# S3 method for class 'fmri_mem_dataset' data_chunks(x, nchunks = 1, runwise = FALSE, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.fmri_mem_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Data Chunks for fmri_mem_dataset Objects — data_chunks.fmri_mem_dataset","text":"x object class 'fmri_mem_dataset'. nchunks number data chunks create. Default 1. runwise TRUE, data chunks created run-wise. Default FALSE. ... Additional arguments.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.fmri_mem_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Data Chunks for fmri_mem_dataset Objects — data_chunks.fmri_mem_dataset","text":"list data chunks, chunk containing data, voxel indices, row indices, chunk number.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.fmri_mem_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Data Chunks for fmri_mem_dataset Objects — data_chunks.fmri_mem_dataset","text":"","code":"if (FALSE) { # \\dontrun{ # Create a simple fmri_mem_dataset for demonstration d <- c(10, 10, 10, 10) nvec <- neuroim2::NeuroVec(array(rnorm(prod(d)), d), space = neuroim2::NeuroSpace(d)) mask <- neuroim2::LogicalNeuroVol(array(TRUE, d[1:3]), neuroim2::NeuroSpace(d[1:3])) dset <- fmri_mem_dataset(list(nvec), mask, TR = 2)  # Create an iterator with 5 chunks iter <- data_chunks(dset, nchunks = 5) `%do%` <- foreach::`%do%` y <- foreach::foreach(chunk = iter) %do% {   colMeans(chunk$data) } length(y) == 5  # Create an iterator with 100 chunks iter <- data_chunks(dset, nchunks = 100) y <- foreach::foreach(chunk = iter) %do% {   colMeans(chunk$data) } length(y) == 100  # Create a \"runwise\" iterator iter <- data_chunks(dset, runwise = TRUE) y <- foreach::foreach(chunk = iter) %do% {   colMeans(chunk$data) } length(y) == 1 } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Data Chunks for Processing — data_chunks","title":"Create Data Chunks for Processing — data_chunks","text":"Generic function create data chunks parallel processing various fMRI dataset types. Supports different chunking strategies.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Data Chunks for Processing — data_chunks","text":"","code":"data_chunks(x, nchunks = 1, runwise = FALSE, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Data Chunks for Processing — data_chunks","text":"x fMRI dataset object nchunks Number chunks create (default: 1) runwise TRUE, create run-wise chunks (default: FALSE) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Data Chunks for Processing — data_chunks","text":"chunk iterator object yields data chunks iterated","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create Data Chunks for Processing — data_chunks","text":"Large fMRI datasets can processed efficiently dividing chunks. function creates iterator yields data chunks parallel sequential processing. Two chunking strategies supported: Equal-sized chunks: Divides voxels approximately equal groups Run-wise chunks: chunk contains voxels one complete runs","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create Data Chunks for Processing — data_chunks","text":"","code":"# \\donttest{ # Create a dataset mat <- matrix(rnorm(100 * 1000), nrow = 100, ncol = 1000) ds <- matrix_dataset(mat, TR = 2, run_length = 100)  # Create 4 chunks for parallel processing chunks <- data_chunks(ds, nchunks = 4)  # Process chunks (example) # results <- foreach(chunk = chunks) %dopar% { #   process_chunk(chunk) # } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.matrix_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Data Chunks for matrix_dataset Objects — data_chunks.matrix_dataset","title":"Create Data Chunks for matrix_dataset Objects — data_chunks.matrix_dataset","text":"function creates data chunks matrix_dataset objects. allows retrieval run-wise sequence-wise data chunks, well arbitrary chunks.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.matrix_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Data Chunks for matrix_dataset Objects — data_chunks.matrix_dataset","text":"","code":"# S3 method for class 'matrix_dataset' data_chunks(x, nchunks = 1, runwise = FALSE, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.matrix_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Data Chunks for matrix_dataset Objects — data_chunks.matrix_dataset","text":"x object class 'matrix_dataset' nchunks number chunks split data . Default 1. runwise TRUE, creates run-wise chunks instead arbitrary chunks ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/data_chunks.matrix_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Data Chunks for matrix_dataset Objects — data_chunks.matrix_dataset","text":"list data chunks, containing data, indices chunk number","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/dataset_methods.html","id":null,"dir":"Reference","previous_headings":"","what":"Dataset Methods for fmridataset — dataset_methods","title":"Dataset Methods for fmridataset — dataset_methods","text":"file implements methods dataset objects delegate internal sampling_frame objects temporal information.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/dim.fmri_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Dimensions of fmri_series — dim.fmri_series","title":"Dimensions of fmri_series — dim.fmri_series","text":"Dimensions fmri_series","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/dim.fmri_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Dimensions of fmri_series — dim.fmri_series","text":"","code":"# S3 method for class 'fmri_series' dim(x)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/dim.fmri_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Dimensions of fmri_series — dim.fmri_series","text":"x fmri_series object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/dim.fmri_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Dimensions of fmri_series — dim.fmri_series","text":"Integer vector length 2 (timepoints, voxels)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/dot-require_namespace.html","id":null,"dir":"Reference","previous_headings":"","what":"Internal wrapper for requireNamespace — .require_namespace","title":"Internal wrapper for requireNamespace — .require_namespace","text":"Centralises optional dependency checks tests can mock behaviour.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/dot-require_namespace.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Internal wrapper for requireNamespace — .require_namespace","text":"","code":".require_namespace(package, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/exec_strategy.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an Execution Strategy for Data Processing — exec_strategy","title":"Create an Execution Strategy for Data Processing — exec_strategy","text":"function creates execution strategy can used process fMRI datasets different ways: voxelwise, runwise, chunkwise.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/exec_strategy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an Execution Strategy for Data Processing — exec_strategy","text":"","code":"exec_strategy(   strategy = c(\"voxelwise\", \"runwise\", \"chunkwise\"),   nchunks = NULL )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/exec_strategy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an Execution Strategy for Data Processing — exec_strategy","text":"strategy Character string specifying processing strategy. Options \"voxelwise\", \"runwise\", \"chunkwise\". nchunks Number chunks use \"chunkwise\" strategy. Ignored strategies.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/exec_strategy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an Execution Strategy for Data Processing — exec_strategy","text":"function takes dataset returns chunk iterator configured according specified strategy.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/filter_subjects.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter subjects in an fmri_group — filter_subjects","title":"Filter subjects in an fmri_group — filter_subjects","text":"Expressions evaluated context subjects(gd) may refer columns directly. Multiple expressions combined logical .","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/filter_subjects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter subjects in an fmri_group — filter_subjects","text":"","code":"filter_subjects(gd, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/filter_subjects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter subjects in an fmri_group — filter_subjects","text":"gd fmri_group. ... Logical expressions used filter rows.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/filter_subjects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter subjects in an fmri_group — filter_subjects","text":"updated fmri_group containing rows satisfy predicates.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/find_subjects_for_rows.html","id":null,"dir":"Reference","previous_headings":"","what":"Find Which Subjects Contain Given Rows — find_subjects_for_rows","title":"Find Which Subjects Contain Given Rows — find_subjects_for_rows","text":"Find Subjects Contain Given Rows","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/find_subjects_for_rows.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Find Which Subjects Contain Given Rows — find_subjects_for_rows","text":"","code":"find_subjects_for_rows(rows, boundaries)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_cache_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get cache information and statistics — fmri_cache_info","title":"Get cache information and statistics — fmri_cache_info","text":"Returns information current state fmridataset cache, including size, number objects, hit/miss rates, memory usage.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_cache_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get cache information and statistics — fmri_cache_info","text":"","code":"fmri_cache_info()"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_cache_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get cache information and statistics — fmri_cache_info","text":"Named list cache statistics","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_cache_info.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get cache information and statistics — fmri_cache_info","text":"","code":"if (FALSE) { # \\dontrun{ # Get cache information cache_info <- fmri_cache_info() print(cache_info) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_cache_resize.html","id":null,"dir":"Reference","previous_headings":"","what":"Resize the fmridataset cache — fmri_cache_resize","title":"Resize the fmridataset cache — fmri_cache_resize","text":"Changes maximum size cache. immediately evict objects new size smaller current cache contents.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_cache_resize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resize the fmridataset cache — fmri_cache_resize","text":"","code":"fmri_cache_resize(size_mb)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_cache_resize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resize the fmridataset cache — fmri_cache_resize","text":"size_mb Numeric cache size megabytes","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_cache_resize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resize the fmridataset cache — fmri_cache_resize","text":"NULL (invisibly)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_cache_resize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Resize the fmridataset cache — fmri_cache_resize","text":"","code":"if (FALSE) { # \\dontrun{ # Resize cache to 1GB fmri_cache_resize(1024)  # Check new size fmri_cache_info() } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_clear_cache.html","id":null,"dir":"Reference","previous_headings":"","what":"Clear fmridataset cache — fmri_clear_cache","title":"Clear fmridataset cache — fmri_clear_cache","text":"Clears internal cache used fmridataset memoized file operations. can useful free memory force re-reading files.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_clear_cache.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Clear fmridataset cache — fmri_clear_cache","text":"","code":"fmri_clear_cache()"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_clear_cache.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Clear fmridataset cache — fmri_clear_cache","text":"NULL (invisibly)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_clear_cache.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Clear fmridataset cache — fmri_clear_cache","text":"","code":"if (FALSE) { # \\dontrun{ # Clear the cache to free memory fmri_clear_cache() } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an fMRI Dataset Object from a Set of Scans — fmri_dataset","title":"Create an fMRI Dataset Object from a Set of Scans — fmri_dataset","text":"function creates fMRI dataset object set scans, design information, data. new implementation uses pluggable backend architecture.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an fMRI Dataset Object from a Set of Scans — fmri_dataset","text":"","code":"fmri_dataset(   scans,   mask = NULL,   TR,   run_length,   event_table = data.frame(),   base_path = \".\",   censor = NULL,   preload = FALSE,   mode = c(\"normal\", \"bigvec\", \"mmap\", \"filebacked\"),   backend = NULL,   dummy_mode = FALSE )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an fMRI Dataset Object from a Set of Scans — fmri_dataset","text":"scans vector one file names images comprising dataset, pre-created storage backend object. mask Name binary mask file indicating voxels include analysis. Ignored scans backend object. TR repetition time seconds scan--scan interval. run_length vector one integers indicating number scans run. event_table data.frame containing event onsets experimental variables. Default empty data.frame. base_path Base directory relative file names. Absolute paths used -. censor binary vector indicating scans remove. Default NULL. preload Read image scans eagerly rather first access. Default FALSE. mode type storage mode ('normal', 'bigvec', 'mmap', filebacked'). Default 'normal'. Ignored scans backend object. backend Deprecated. Use scans parameter pass backend object. dummy_mode Logical, TRUE allows non-existent files (testing purposes ). Default FALSE.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an fMRI Dataset Object from a Set of Scans — fmri_dataset","text":"fMRI dataset object class c(\"fmri_file_dataset\", \"volumetric_dataset\", \"fmri_dataset\", \"list\").","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an fMRI Dataset Object from a Set of Scans — fmri_dataset","text":"","code":"if (FALSE) { # \\dontrun{ # Create an fMRI dataset with 3 scans and a mask dset <- fmri_dataset(c(\"scan1.nii\", \"scan2.nii\", \"scan3.nii\"),   mask = \"mask.nii\", TR = 2, run_length = rep(300, 3),   event_table = data.frame(     onsets = c(3, 20, 99, 3, 20, 99, 3, 20, 99),     run = c(1, 1, 1, 2, 2, 2, 3, 3, 3)   ) )  # Create an fMRI dataset with 1 scan and a mask dset <- fmri_dataset(\"scan1.nii\",   mask = \"mask.nii\", TR = 2,   run_length = 300,   event_table = data.frame(onsets = c(3, 20, 99), run = rep(1, 3)) )  # Create an fMRI dataset with a backend backend <- nifti_backend(c(\"scan1.nii\", \"scan2.nii\"), mask_source = \"mask.nii\") dset <- fmri_dataset(backend, TR = 2, run_length = c(150, 150))  # Create a dummy dataset for testing (files don't need to exist) dset_dummy <- fmri_dataset(   scans = c(\"dummy1.nii\", \"dummy2.nii\"),   mask = \"dummy_mask.nii\",   TR = 2,   run_length = c(100, 100),   dummy_mode = TRUE # Enable dummy mode for testing ) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset_legacy.html","id":null,"dir":"Reference","previous_headings":"","what":"Legacy fMRI Dataset Constructor — fmri_dataset_legacy","title":"Legacy fMRI Dataset Constructor — fmri_dataset_legacy","text":"Backward compatibility wrapper fmri_dataset. function provides interface original fmri_dataset function.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset_legacy.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Legacy fMRI Dataset Constructor — fmri_dataset_legacy","text":"","code":"fmri_dataset_legacy(scans, mask, TR, run_length, preload = FALSE, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset_legacy.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Legacy fMRI Dataset Constructor — fmri_dataset_legacy","text":"scans Either character vector file paths scans list NeuroVec objects mask Either character file path mask NeuroVol mask object TR repetition time run_length Numeric vector run lengths preload Whether preload data memory ... Additional arguments passed fmri_dataset","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset_legacy.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Legacy fMRI Dataset Constructor — fmri_dataset_legacy","text":"fmri_dataset object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_dataset_legacy.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Legacy fMRI Dataset Constructor — fmri_dataset_legacy","text":"","code":"if (FALSE) { # \\dontrun{ # Create dataset from files dset <- fmri_dataset_legacy(   scans = c(\"scan1.nii\", \"scan2.nii\"),   mask = \"mask.nii\",   TR = 2,   run_length = c(100, 100) ) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an fmri_group (one row per subject) — fmri_group","title":"Create an fmri_group (one row per subject) — fmri_group","text":"Create fmri_group (one row per subject)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an fmri_group (one row per subject) — fmri_group","text":"","code":"fmri_group(   subjects,   id,   dataset_col = \"dataset\",   space = NULL,   mask_strategy = c(\"subject_specific\", \"intersect\", \"union\") )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an fmri_group (one row per subject) — fmri_group","text":"subjects data.frame (tibble) one row per subject one column contains per-subject fmri_dataset objects stored list column. id Character scalar giving name subject identifier column. dataset_col Character scalar naming list column stores per-subject dataset handles. space Optional character string describing nominal common space subjects (e.g., \"MNI152NLin2009cAsym\"). mask_strategy One \"subject_specific\", \"intersect\", \"union\" describing masks handled combining subjects. declarative flag ; resampling performed constructor.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an fmri_group (one row per subject) — fmri_group","text":"object class fmri_group wraps input table.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_h5_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an fMRI Dataset Object from H5 Files — fmri_h5_dataset","title":"Create an fMRI Dataset Object from H5 Files — fmri_h5_dataset","text":"function creates fMRI dataset object specifically H5 files using fmristore package. scan stored H5 file loads H5NeuroVec object.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_h5_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an fMRI Dataset Object from H5 Files — fmri_h5_dataset","text":"","code":"fmri_h5_dataset(   h5_files,   mask_source,   TR,   run_length,   event_table = data.frame(),   base_path = \".\",   censor = NULL,   preload = FALSE,   mask_dataset = \"data/elements\",   data_dataset = \"data\" )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_h5_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an fMRI Dataset Object from H5 Files — fmri_h5_dataset","text":"h5_files vector one file paths H5 files containing fMRI data. mask_source File path H5 mask file, regular mask file, -memory NeuroVol object. TR repetition time seconds scan--scan interval. run_length vector one integers indicating number scans run. event_table data.frame containing event onsets experimental variables. Default empty data.frame. base_path Base directory relative file names. Absolute paths used -. censor binary vector indicating scans remove. Default NULL. preload Read H5NeuroVec objects eagerly rather first access. Default FALSE. mask_dataset Character string specifying dataset path within H5 file mask (default: \"data/elements\"). data_dataset Character string specifying dataset path within H5 files data (default: \"data\").","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_h5_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an fMRI Dataset Object from H5 Files — fmri_h5_dataset","text":"fMRI dataset object class c(\"fmri_file_dataset\", \"volumetric_dataset\", \"fmri_dataset\", \"list\").","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_h5_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an fMRI Dataset Object from H5 Files — fmri_h5_dataset","text":"","code":"if (FALSE) { # \\dontrun{ # Create an fMRI dataset with H5NeuroVec files (standard fmristore format) dset <- fmri_h5_dataset(   h5_files = c(\"scan1.h5\", \"scan2.h5\", \"scan3.h5\"),   mask_source = \"mask.h5\",   TR = 2,   run_length = c(150, 150, 150) )  # Create an fMRI dataset with H5 files and NIfTI mask dset <- fmri_h5_dataset(   h5_files = \"single_scan.h5\",   mask_source = \"mask.nii\",   TR = 2,   run_length = 300 )  # Custom dataset paths (if using non-standard H5 structure) dset <- fmri_h5_dataset(   h5_files = \"custom_scan.h5\",   mask_source = \"custom_mask.h5\",   TR = 2,   run_length = 200,   data_dataset = \"my_data_path\",   mask_dataset = \"my_mask_path\" ) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_latent_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an fMRI Dataset Object from LatentNeuroVec Files or Objects — fmri_latent_dataset","title":"Create an fMRI Dataset Object from LatentNeuroVec Files or Objects — fmri_latent_dataset","text":"function deprecated. Please use latent_dataset() instead, provides proper interface latent space data.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_latent_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an fMRI Dataset Object from LatentNeuroVec Files or Objects — fmri_latent_dataset","text":"","code":"fmri_latent_dataset(   latent_files,   mask_source = NULL,   TR,   run_length,   event_table = data.frame(),   base_path = \".\",   censor = NULL,   preload = FALSE )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_latent_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an fMRI Dataset Object from LatentNeuroVec Files or Objects — fmri_latent_dataset","text":"latent_files Source files objects mask_source Ignored TR repetition time seconds run_length Vector run lengths event_table Event table base_path Base path files censor Censor vector preload Whether preload data","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_latent_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an fMRI Dataset Object from LatentNeuroVec Files or Objects — fmri_latent_dataset","text":"latent_dataset object","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_latent_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an fMRI Dataset Object from LatentNeuroVec Files or Objects — fmri_latent_dataset","text":"","code":"if (FALSE) { # \\dontrun{ # Use latent_dataset() instead: dset <- latent_dataset(   source = c(\"run1.lv.h5\", \"run2.lv.h5\", \"run3.lv.h5\"),   TR = 2,   run_length = c(150, 150, 150) ) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_mem_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an fMRI Memory Dataset Object — fmri_mem_dataset","title":"Create an fMRI Memory Dataset Object — fmri_mem_dataset","text":"function creates fMRI memory dataset object, list containing information scans, mask, TR, number runs, event table, base path, sampling frame, censor.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_mem_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an fMRI Memory Dataset Object — fmri_mem_dataset","text":"","code":"fmri_mem_dataset(   scans,   mask,   TR,   run_length = sapply(scans, function(x) dim(x)[4]),   event_table = data.frame(),   base_path = \".\",   censor = NULL )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_mem_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an fMRI Memory Dataset Object — fmri_mem_dataset","text":"scans list objects class NeuroVec neuroim2 package. mask binary mask class NeuroVol neuroim2 package indicating set voxels include analyses. TR Repetition time (TR) fMRI acquisition. run_length numeric vector specifying length run dataset. Default length scans. event_table optional data frame containing event information. Default empty data frame. base_path Base directory relative file names. Absolute paths used -. censor optional numeric vector specifying time points censor. Default NULL.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_mem_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an fMRI Memory Dataset Object — fmri_mem_dataset","text":"fMRI memory dataset object class c(\"fmri_mem_dataset\", \"volumetric_dataset\", \"fmri_dataset\", \"list\").","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_mem_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an fMRI Memory Dataset Object — fmri_mem_dataset","text":"","code":"# Create a NeuroVec object d <- c(10, 10, 10, 10) nvec <- neuroim2::NeuroVec(array(rnorm(prod(d)), d), space = neuroim2::NeuroSpace(d))  # Create a NeuroVol mask mask <- neuroim2::NeuroVol(array(rnorm(10 * 10 * 10), d[1:3]), space = neuroim2::NeuroSpace(d[1:3])) mask[mask < .5] <- 0  # Create an fmri_mem_dataset dset <- fmri_mem_dataset(list(nvec), mask, TR = 2)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_series.html","id":null,"dir":"Reference","previous_headings":"","what":"fmri_series: fMRI Time Series Container — fmri_series","title":"fmri_series: fMRI Time Series Container — fmri_series","text":"S3 class representing lazily accessed fMRI time series data. underlying data stored lazy matrix (typically delarr object) rows corresponding timepoints columns corresponding voxels. Core interface retrieving voxel time series fMRI datasets.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"fmri_series: fMRI Time Series Container — fmri_series","text":"","code":"fmri_series(   dataset,   selector = NULL,   timepoints = NULL,   output = c(\"fmri_series\", \"DelayedMatrix\"),   event_window = NULL,   ... )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"fmri_series: fMRI Time Series Container — fmri_series","text":"dataset fmri_dataset object. selector Spatial selector NULL voxels. timepoints Optional temporal subset NULL . output Return type - \"FmriSeries\" (default) \"DelayedMatrix\". event_window Reserved future use. ... Additional arguments passed methods.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"fmri_series: fMRI Time Series Container — fmri_series","text":"object class fmri_series fmri_series (delarr lazy matrix payload) DelayedMatrix output = \"DelayedMatrix\".","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_series.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"fmri_series: fMRI Time Series Container — fmri_series","text":"fmri_series object contains: data: lazy matrix timepoints rows voxels columns voxel_info: data.frame containing spatial metadata voxel temporal_info: data.frame containing metadata timepoint selection_info: list describing data selected dataset_info: list describing source dataset backend","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_series.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"fmri_series: fMRI Time Series Container — fmri_series","text":"","code":"# \\donttest{ # Create example fmri_series object mat <- matrix(rnorm(100 * 50), nrow = 100, ncol = 50) backend <- matrix_backend(mat, mask = rep(TRUE, ncol(mat))) dataset <- fmri_dataset(backend, TR = 1, run_length = rep(25, 4)) fs <- fmri_series(dataset) fs #> <fmri_series> 50 voxels x 100 timepoints (lazy) #> Selector: NULL | Backend: matrix_backend | Orientation: time x voxels # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_series_resolvers.html","id":null,"dir":"Reference","previous_headings":"","what":"Helpers for fmri_series spatial and temporal selection — fmri_series_resolvers","title":"Helpers for fmri_series spatial and temporal selection — fmri_series_resolvers","text":"functions convert user-facing selectors numeric indices used fmri_series implementation. exported users directly.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_study_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an fmri_study_dataset — fmri_study_dataset","title":"Create an fmri_study_dataset — fmri_study_dataset","text":"High level constructor combines multiple fmri_dataset objects single study-level dataset using study_backend.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_study_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an fmri_study_dataset — fmri_study_dataset","text":"","code":"fmri_study_dataset(datasets, subject_ids = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_study_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an fmri_study_dataset — fmri_study_dataset","text":"datasets list fmri_dataset objects subject_ids Optional vector subject identifiers","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_study_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an fmri_study_dataset — fmri_study_dataset","text":"object class fmri_study_dataset","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_zarr_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an fMRI Dataset from Zarr Arrays — fmri_zarr_dataset","title":"Create an fMRI Dataset from Zarr Arrays — fmri_zarr_dataset","text":"Creates fMRI dataset object Zarr array files. Zarr cloud-native array format supports chunked, compressed storage ideal large neuroimaging datasets.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_zarr_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an fMRI Dataset from Zarr Arrays — fmri_zarr_dataset","text":"","code":"fmri_zarr_dataset(   zarr_source,   data_key = \"data\",   mask_key = \"mask\",   TR,   run_length,   event_table = data.frame(),   censor = NULL,   preload = FALSE,   cache_size = 100 )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_zarr_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an fMRI Dataset from Zarr Arrays — fmri_zarr_dataset","text":"zarr_source Path Zarr store (directory, zip file, URL) data_key Character key main data array within store (default: \"data\") mask_key Character key mask array (default: \"mask\"). Set NULL mask. TR repetition time seconds run_length Vector integers indicating number scans run event_table Optional data.frame containing event onsets experimental variables censor Optional binary vector indicating scans remove preload Whether load data memory (default: FALSE) cache_size Number chunks cache memory (default: 100)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_zarr_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an fMRI Dataset from Zarr Arrays — fmri_zarr_dataset","text":"fMRI dataset object class c(\"fmri_file_dataset\", \"volumetric_dataset\", \"fmri_dataset\", \"list\")","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_zarr_dataset.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Create an fMRI Dataset from Zarr Arrays — fmri_zarr_dataset","text":"Zarr backend expects data organized 4D array dimensions (x, y, z, time). data accessed lazily default, loading requested chunks memory. Zarr stores can : Local directories containing .zarr data Zip files containing zarr arrays Remote URLs (S3, GCS, HTTP) cloud-hosted data","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmri_zarr_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an fMRI Dataset from Zarr Arrays — fmri_zarr_dataset","text":"","code":"if (FALSE) { # \\dontrun{ # Local Zarr store dataset <- fmri_zarr_dataset(   \"path/to/data.zarr\",   TR = 2,   run_length = c(150, 150, 150) )  # Remote S3 store with custom keys dataset <- fmri_zarr_dataset(   \"s3://bucket/neuroimaging/subject01.zarr\",   data_key = \"bold/data\",   mask_key = \"bold/mask\",   TR = 1.5,   run_length = 300 )  # Preload small dataset into memory dataset <- fmri_zarr_dataset(   \"small_data.zarr\",   TR = 2,   run_length = 100,   preload = TRUE ) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset-errors.html","id":null,"dir":"Reference","previous_headings":"","what":"Custom Error Classes for fmridataset — fmridataset-errors","title":"Custom Error Classes for fmridataset — fmridataset-errors","text":"hierarchy custom S3 error classes fmridataset package. provide structured error handling storage backend operations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Custom fmridataset Error — fmridataset_error","title":"Create a Custom fmridataset Error — fmridataset_error","text":"Create Custom fmridataset Error","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Custom fmridataset Error — fmridataset_error","text":"","code":"fmridataset_error(message, class = character(), ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Custom fmridataset Error — fmridataset_error","text":"message Character string describing error class Character vector error classes ... Additional data include error condition","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Custom fmridataset Error — fmridataset_error","text":"condition object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error_backend_io.html","id":null,"dir":"Reference","previous_headings":"","what":"Backend I/O Error — fmridataset_error_backend_io","title":"Backend I/O Error — fmridataset_error_backend_io","text":"Raised storage backend encounters read/write failures.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error_backend_io.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Backend I/O Error — fmridataset_error_backend_io","text":"","code":"fmridataset_error_backend_io(message, file = NULL, operation = NULL, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error_backend_io.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Backend I/O Error — fmridataset_error_backend_io","text":"message Character string describing /O error file Path file caused error (optional) operation operation failed (e.g., \"read\", \"write\") ... Additional context","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error_backend_io.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Backend I/O Error — fmridataset_error_backend_io","text":"backend /O error condition","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Configuration Error — fmridataset_error_config","title":"Configuration Error — fmridataset_error_config","text":"Raised invalid configuration provided backend dataset.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Configuration Error — fmridataset_error_config","text":"","code":"fmridataset_error_config(message, parameter = NULL, value = NULL, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Configuration Error — fmridataset_error_config","text":"message Character string describing configuration error parameter parameter invalid value invalid value provided ... Additional context","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/fmridataset_error_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Configuration Error — fmridataset_error_config","text":"configuration error condition","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_benchmark_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Performance Benchmark Data — generate_benchmark_data","title":"Generate Performance Benchmark Data — generate_benchmark_data","text":"Generate Performance Benchmark Data","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_benchmark_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Performance Benchmark Data — generate_benchmark_data","text":"","code":"generate_benchmark_data(   dataset_sizes = c(100, 500, 1000, 5000),   operations = c(\"load\", \"chunk\", \"process\") )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_benchmark_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Performance Benchmark Data — generate_benchmark_data","text":"dataset_sizes Vector dataset sizes test operations Operations benchmark","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_benchmark_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Performance Benchmark Data — generate_benchmark_data","text":"Data frame benchmark results","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_events.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Example Event Table — generate_example_events","title":"Generate Example Event Table — generate_example_events","text":"Generate Example Event Table","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_events.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Example Event Table — generate_example_events","text":"","code":"generate_example_events(   n_runs = 2,   events_per_run = 4,   TR = 2,   run_length = 100 )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_events.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Example Event Table — generate_example_events","text":"n_runs Number runs events_per_run Events per run TR Repetition time run_length Length run TRs","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_events.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Example Event Table — generate_example_events","text":"Data frame event information","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_fmri_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper Functions for Vignettes — generate_example_fmri_data","title":"Helper Functions for Vignettes — generate_example_fmri_data","text":"Internal functions support vignette examples synthetic data consistent demonstrations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_fmri_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper Functions for Vignettes — generate_example_fmri_data","text":"","code":"generate_example_fmri_data(   n_timepoints = 200,   n_voxels = 1000,   n_active = 100,   activation_periods = NULL,   signal_strength = 0.5,   seed = 123 )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_fmri_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper Functions for Vignettes — generate_example_fmri_data","text":"n_timepoints Number time points n_voxels Number voxels n_active Number active voxels signal activation_periods Time indices activation occurs signal_strength Strength activation signal seed Random seed reproducibility","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_fmri_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper Functions for Vignettes — generate_example_fmri_data","text":"Matrix synthetic fMRI data","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_fmri_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Helper Functions for Vignettes — generate_example_fmri_data","text":"","code":"if (FALSE) { # \\dontrun{ data <- generate_example_fmri_data(200, 1000) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_mask.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Example Mask — generate_example_mask","title":"Create Example Mask — generate_example_mask","text":"Create Example Mask","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_mask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Example Mask — generate_example_mask","text":"","code":"generate_example_mask(n_voxels = 1000, fraction_valid = 0.8)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_mask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Example Mask — generate_example_mask","text":"n_voxels Total number voxels fraction_valid Fraction voxels include mask","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_mask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Example Mask — generate_example_mask","text":"Logical vector mask","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_paths.html","id":null,"dir":"Reference","previous_headings":"","what":"Create Example File Paths for Vignettes — generate_example_paths","title":"Create Example File Paths for Vignettes — generate_example_paths","text":"Create Example File Paths Vignettes","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_paths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create Example File Paths for Vignettes — generate_example_paths","text":"","code":"generate_example_paths(   n_runs = 2,   subject_id = \"sub-001\",   base_path = tempdir() )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_paths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create Example File Paths for Vignettes — generate_example_paths","text":"n_runs Number runs subject_id Subject identifier base_path Base directory path","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_example_paths.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create Example File Paths for Vignettes — generate_example_paths","text":"Character vector file paths","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_golden_test_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate Golden Test Data — generate_golden_test_data","title":"Generate Golden Test Data — generate_golden_test_data","text":"Generate reference data golden tests. function creates reproducible test data can used validate consistency across package versions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_golden_test_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate Golden Test Data — generate_golden_test_data","text":"","code":"generate_golden_test_data(output_dir = \"tests/testthat/golden\", seed = 42)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_golden_test_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate Golden Test Data — generate_golden_test_data","text":"output_dir Directory golden data files saved. Defaults \"tests/testthat/golden\". seed Random seed reproducibility. Defaults 42.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_golden_test_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate Golden Test Data — generate_golden_test_data","text":"Invisibly returns TRUE success.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_golden_test_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Generate Golden Test Data — generate_golden_test_data","text":"function generates following golden test data: reference_data.rds - Basic test matrices metadata matrix_dataset.rds - Example fmri_dataset object fmri_series.rds - Example FmriSeries data sampling_frame.rds - Example sampling_frame object mock_neurvec.rds - Mock NeuroVec object testing","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generate_golden_test_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate Golden Test Data — generate_golden_test_data","text":"","code":"if (FALSE) { # \\dontrun{ # Generate golden test data generate_golden_test_data()  # Generate with custom seed generate_golden_test_data(seed = 123) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/generics.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic Functions for fMRI Dataset Operations — generics","title":"Generic Functions for fMRI Dataset Operations — generics","text":"file contains generic function declarations refactored fmridataset package. establish interface contracts implemented dataset-specific methods files.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_TR.html","id":null,"dir":"Reference","previous_headings":"","what":"Get TR (Repetition Time) from Sampling Frame — get_TR","title":"Get TR (Repetition Time) from Sampling Frame — get_TR","text":"Extracts repetition time (TR) seconds objects containing temporal information fMRI acquisitions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_TR.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get TR (Repetition Time) from Sampling Frame — get_TR","text":"","code":"get_TR(x, ...)  # S3 method for class 'matrix_dataset' get_TR(x, ...)  # S3 method for class 'fmri_dataset' get_TR(x, ...)  # S3 method for class 'fmri_mem_dataset' get_TR(x, ...)  # S3 method for class 'fmri_file_dataset' get_TR(x, ...)  # S3 method for class 'fmri_study_dataset' get_TR(x, ...)  # S3 method for class 'sampling_frame' get_TR(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_TR.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get TR (Repetition Time) from Sampling Frame — get_TR","text":"x object containing temporal information (e.g., sampling_frame, fmri_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_TR.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get TR (Repetition Time) from Sampling Frame — get_TR","text":"Numeric value representing TR seconds","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_TR.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get TR (Repetition Time) from Sampling Frame — get_TR","text":"TR (repetition time) time successive acquisitions slice fMRI scan, typically measured seconds. parameter crucial temporal analyses hemodynamic modeling.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_TR.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get TR (Repetition Time) from Sampling Frame — get_TR","text":"","code":"# \\donttest{ # Create a sampling frame with TR = 2 seconds sf <- fmrihrf::sampling_frame(blocklens = c(100, 120), TR = 2) get_TR(sf) # Returns: 2 #> [1] 2 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_backend_registry.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Registered Backend Information — get_backend_registry","title":"Get Registered Backend Information — get_backend_registry","text":"Retrieves information registered backend lists registered backends.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_backend_registry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Registered Backend Information — get_backend_registry","text":"","code":"get_backend_registry(name = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_backend_registry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Registered Backend Information — get_backend_registry","text":"name Character string, name backend query. NULL, returns registrations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_backend_registry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Registered Backend Information — get_backend_registry","text":"specific backend: list registration details. backends: named list element contains registration details.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_backend_registry.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Registered Backend Information — get_backend_registry","text":"","code":"# List all registered backends get_backend_registry() #> $h5 #> $h5$name #> [1] \"h5\" #>  #> $h5$factory #> function (source, mask_source, mask_dataset = \"data/elements\",  #>     data_dataset = \"data\", preload = FALSE)  #> { #>     if (!requireNamespace(\"fmristore\", quietly = TRUE)) { #>         stop_fmridataset(fmridataset_error_config, message = \"Package 'fmristore' is required for H5 backend but is not available\",  #>             parameter = \"backend_type\") #>     } #>     if (is.character(source)) { #>         if (!all(file.exists(source))) { #>             missing_files <- source[!file.exists(source)] #>             stop_fmridataset(fmridataset_error_backend_io, message = sprintf(\"H5 source files not found: %s\",  #>                 paste(missing_files, collapse = \", \")), file = missing_files,  #>                 operation = \"open\") #>         } #>     } #>     else if (is.list(source)) { #>         valid_types <- vapply(source, function(x) { #>             inherits(x, \"H5NeuroVec\") #>         }, logical(1)) #>         if (!all(valid_types)) { #>             stop_fmridataset(fmridataset_error_config, message = \"All source objects must be H5NeuroVec objects\",  #>                 parameter = \"source\") #>         } #>     } #>     else { #>         stop_fmridataset(fmridataset_error_config, message = \"source must be character vector (H5 file paths) or list (H5NeuroVec objects)\",  #>             parameter = \"source\", value = class(source)) #>     } #>     if (is.character(mask_source)) { #>         if (!file.exists(mask_source)) { #>             stop_fmridataset(fmridataset_error_backend_io, message = sprintf(\"H5 mask file not found: %s\",  #>                 mask_source), file = mask_source, operation = \"open\") #>         } #>     } #>     else if (!inherits(mask_source, \"NeuroVol\") && !inherits(mask_source,  #>         \"H5NeuroVol\")) { #>         stop_fmridataset(fmridataset_error_config, message = \"mask_source must be file path, NeuroVol, or H5NeuroVol object\",  #>             parameter = \"mask_source\", value = class(mask_source)) #>     } #>     backend <- new.env(parent = emptyenv()) #>     backend$source <- source #>     backend$mask_source <- mask_source #>     backend$mask_dataset <- mask_dataset #>     backend$data_dataset <- data_dataset #>     backend$preload <- preload #>     backend$h5_objects <- NULL #>     backend$mask <- NULL #>     backend$mask_vec <- NULL #>     backend$dims <- NULL #>     backend$metadata <- NULL #>     class(backend) <- c(\"h5_backend\", \"storage_backend\") #>     backend #> } #> <bytecode: 0x55ce37de1b50> #> <environment: namespace:fmridataset> #>  #> $h5$description #> [1] \"HDF5 format backend using fmristore\" #>  #> $h5$validate_function #> NULL #>  #> $h5$registered_at #> [1] \"2026-01-22 12:18:50 UTC\" #>  #>  #> $latent #> $latent$name #> [1] \"latent\" #>  #> $latent$factory #> function (source, preload = FALSE)  #> { #>     if (is.character(source)) { #>         if (!all(file.exists(source))) { #>             missing <- source[!file.exists(source)] #>             stop_fmridataset(fmridataset_error_backend_io, sprintf(\"Source files not found: %s\",  #>                 paste(missing, collapse = \", \")), file = missing[1],  #>                 operation = \"create\") #>         } #>         if (!all(grepl(\"\\\\.(lv\\\\.h5|h5)$\", source, ignore.case = TRUE))) { #>             stop_fmridataset(fmridataset_error_config, \"All source files must be HDF5 files (.h5 or .lv.h5)\",  #>                 parameter = \"source\") #>         } #>     } #>     else if (is.list(source)) { #>         for (i in seq_along(source)) { #>             item <- source[[i]] #>             if (is.character(item)) { #>                 if (length(item) != 1 || !file.exists(item)) { #>                   stop_fmridataset(fmridataset_error_config,  #>                     sprintf(\"Source item %d must be an existing file path\",  #>                       i), parameter = \"source\") #>                 } #>             } #>             else if (!inherits(item, \"LatentNeuroVec\")) { #>                 has_basis <- isS4(item) && \"basis\" %in% methods::slotNames(item) #>                 if (!has_basis) { #>                   stop_fmridataset(fmridataset_error_config,  #>                     sprintf(\"Source item %d must be a LatentNeuroVec object or file path\",  #>                       i), parameter = \"source\") #>                 } #>             } #>         } #>     } #>     else { #>         stop_fmridataset(fmridataset_error_config, \"source must be character vector or list\",  #>             parameter = \"source\", value = class(source)) #>     } #>     backend <- list(source = source, preload = preload, data = NULL,  #>         dims = NULL, is_open = FALSE) #>     class(backend) <- c(\"latent_backend\", \"storage_backend\") #>     backend #> } #> <bytecode: 0x55ce379e13e0> #> <environment: namespace:fmridataset> #>  #> $latent$description #> [1] \"Latent space backend for dimension-reduced data\" #>  #> $latent$validate_function #> NULL #>  #> $latent$registered_at #> [1] \"2026-01-22 12:18:50 UTC\" #>  #>  #> $matrix #> $matrix$name #> [1] \"matrix\" #>  #> $matrix$factory #> function (data_matrix, mask = NULL, spatial_dims = NULL, metadata = NULL)  #> { #>     if (!is.matrix(data_matrix)) { #>         stop_fmridataset(fmridataset_error_config, message = \"data_matrix must be a matrix\",  #>             parameter = \"data_matrix\", value = class(data_matrix)) #>     } #>     n_timepoints <- nrow(data_matrix) #>     n_voxels <- ncol(data_matrix) #>     if (is.null(mask)) { #>         mask <- rep(TRUE, n_voxels) #>     } #>     if (!is.logical(mask)) { #>         stop_fmridataset(fmridataset_error_config, message = \"mask must be a logical vector\",  #>             parameter = \"mask\", value = class(mask)) #>     } #>     if (length(mask) != n_voxels) { #>         stop_fmridataset(fmridataset_error_config, message = sprintf(\"mask length (%d) must equal number of columns (%d)\",  #>             length(mask), n_voxels), parameter = \"mask\") #>     } #>     if (is.null(spatial_dims)) { #>         spatial_dims <- c(n_voxels, 1, 1) #>     } #>     if (length(spatial_dims) != 3 || !is.numeric(spatial_dims)) { #>         stop_fmridataset(fmridataset_error_config, message = \"spatial_dims must be a numeric vector of length 3\",  #>             parameter = \"spatial_dims\", value = spatial_dims) #>     } #>     if (prod(spatial_dims) != n_voxels) { #>         stop_fmridataset(fmridataset_error_config, message = sprintf(\"Product of spatial_dims (%d) must equal number of voxels (%d)\",  #>             prod(spatial_dims), n_voxels), parameter = \"spatial_dims\") #>     } #>     backend <- list(data_matrix = data_matrix, mask = mask, spatial_dims = spatial_dims,  #>         metadata = metadata %||% list()) #>     class(backend) <- c(\"matrix_backend\", \"storage_backend\") #>     backend #> } #> <bytecode: 0x55ce37d08000> #> <environment: namespace:fmridataset> #>  #> $matrix$description #> [1] \"In-memory matrix backend\" #>  #> $matrix$validate_function #> NULL #>  #> $matrix$registered_at #> [1] \"2026-01-22 12:18:50 UTC\" #>  #>  #> $nifti #> $nifti$name #> [1] \"nifti\" #>  #> $nifti$factory #> function (source, mask_source, preload = FALSE, mode = c(\"normal\",  #>     \"bigvec\", \"mmap\", \"filebacked\"), dummy_mode = FALSE)  #> { #>     mode <- match.arg(mode) #>     if (is.character(source)) { #>         if (!dummy_mode && !all(file.exists(source))) { #>             missing_files <- source[!file.exists(source)] #>             stop_fmridataset(fmridataset_error_backend_io, message = sprintf(\"Source files not found: %s\",  #>                 paste(missing_files, collapse = \", \")), file = missing_files,  #>                 operation = \"open\") #>         } #>     } #>     else if (is.list(source)) { #>         valid_types <- vapply(source, function(x) { #>             inherits(x, \"NeuroVec\") #>         }, logical(1)) #>         if (!all(valid_types)) { #>             stop_fmridataset(fmridataset_error_config, message = \"All source objects must be NeuroVec objects\",  #>                 parameter = \"source\") #>         } #>     } #>     else { #>         stop_fmridataset(fmridataset_error_config, message = \"source must be character vector (file paths) or list (in-memory objects)\",  #>             parameter = \"source\", value = class(source)) #>     } #>     if (is.character(mask_source)) { #>         if (!dummy_mode && !file.exists(mask_source)) { #>             stop_fmridataset(fmridataset_error_backend_io, message = sprintf(\"Mask file not found: %s\",  #>                 mask_source), file = mask_source, operation = \"open\") #>         } #>     } #>     else if (!inherits(mask_source, \"NeuroVol\")) { #>         stop_fmridataset(fmridataset_error_config, message = \"mask_source must be file path or NeuroVol object\",  #>             parameter = \"mask_source\", value = class(mask_source)) #>     } #>     backend <- new.env(parent = emptyenv()) #>     backend$source <- source #>     backend$mask_source <- mask_source #>     backend$preload <- preload #>     backend$mode <- mode #>     backend$dummy_mode <- dummy_mode #>     backend$data <- NULL #>     backend$mask <- NULL #>     backend$mask_vec <- NULL #>     backend$dims <- NULL #>     backend$metadata <- NULL #>     backend$run_length <- NULL #>     backend$cache <- cachem::cache_mem(max_size = 64 * 1024^2,  #>         evict = \"lru\") #>     class(backend) <- c(\"nifti_backend\", \"storage_backend\") #>     backend #> } #> <bytecode: 0x55ce37dd9218> #> <environment: namespace:fmridataset> #>  #> $nifti$description #> [1] \"NIfTI format backend using neuroim2\" #>  #> $nifti$validate_function #> NULL #>  #> $nifti$registered_at #> [1] \"2026-01-22 12:18:50 UTC\" #>  #>  #> $study #> $study$name #> [1] \"study\" #>  #> $study$factory #> function (backends, subject_ids = NULL, strict = getOption(\"fmridataset.mask_check\",  #>     \"identical\"))  #> { #>     if (!is.list(backends) || length(backends) == 0) { #>         stop_fmridataset(fmridataset_error_config, message = \"backends must be a non-empty list\") #>     } #>     backends <- lapply(backends, function(b) { #>         if (!inherits(b, \"storage_backend\")) { #>             if (inherits(b, \"matrix_dataset\") && !is.null(b$datamat)) { #>                 mask_logical <- as.logical(b$mask) #>                 matrix_backend(b$datamat, mask = mask_logical) #>             } #>             else if (!is.null(b$backend)) { #>                 b$backend #>             } #>             else { #>                 b #>             } #>         } #>         else { #>             b #>         } #>     }) #>     lapply(backends, function(b) { #>         if (!inherits(b, \"storage_backend\")) { #>             stop_fmridataset(fmridataset_error_config, message = \"all elements of backends must inherit from 'storage_backend'\") #>         } #>     }) #>     if (is.null(subject_ids)) { #>         subject_ids <- seq_along(backends) #>     } #>     if (length(subject_ids) != length(backends)) { #>         stop_fmridataset(fmridataset_error_config, message = \"subject_ids must match length of backends\") #>     } #>     dims_list <- lapply(backends, backend_get_dims) #>     spatial_dims <- lapply(dims_list, function(x) as.numeric(x$spatial)) #>     time_dims <- vapply(dims_list, function(x) x$time, numeric(1)) #>     ref_spatial <- spatial_dims[[1]] #>     for (i in seq_along(spatial_dims[-1])) { #>         sd <- spatial_dims[[i + 1]] #>         if (!identical(sd, ref_spatial)) { #>             stop_fmridataset(fmridataset_error_config, message = \"spatial dimensions must match across backends\") #>         } #>     } #>     masks <- lapply(backends, backend_get_mask) #>     ref_mask <- masks[[1]] #>     if (strict == \"identical\") { #>         for (m in masks[-1]) { #>             if (!identical(m, ref_mask)) { #>                 stop_fmridataset(fmridataset_error_config, message = \"masks differ across backends\") #>             } #>         } #>         combined_mask <- ref_mask #>     } #>     else if (strict == \"intersect\") { #>         for (m in masks[-1]) { #>             overlap <- sum(m & ref_mask)/length(ref_mask) #>             if (overlap < 0.95) { #>                 stop_fmridataset(fmridataset_error_config, message = \"mask overlap <95%\") #>             } #>         } #>         combined_mask <- Reduce(\"&\", masks) #>     } #>     else { #>         stop_fmridataset(fmridataset_error_config, message = \"unknown strict setting\") #>     } #>     subject_boundaries <- c(0L, cumsum(as.integer(time_dims))) #>     backend <- list(backends = backends, subject_ids = subject_ids,  #>         strict = strict, `_dims` = list(spatial = ref_spatial,  #>             time = sum(time_dims)), `_mask` = combined_mask,  #>         time_dims = as.integer(time_dims), subject_boundaries = as.integer(subject_boundaries)) #>     class(backend) <- c(\"study_backend\", \"storage_backend\") #>     backend #> } #> <bytecode: 0x55ce379ee118> #> <environment: namespace:fmridataset> #>  #> $study$description #> [1] \"Multi-subject study backend\" #>  #> $study$validate_function #> NULL #>  #> $study$registered_at #> [1] \"2026-01-22 12:18:50 UTC\" #>  #>  #> $zarr #> $zarr$name #> [1] \"zarr\" #>  #> $zarr$factory #> function (source, data_key = \"data\", mask_key = \"mask\", preload = FALSE,  #>     cache_size = 100)  #> { #>     if (!is.character(source) || length(source) != 1) { #>         stop_fmridataset(fmridataset_error_config, \"source must be a single character string\",  #>             parameter = \"source\", value = class(source)) #>     } #>     if (!requireNamespace(\"Rarr\", quietly = TRUE)) { #>         stop_fmridataset(fmridataset_error_config, \"The Rarr package is required for zarr_backend but is not installed.\",  #>             details = \"Install with: BiocManager::install('Rarr')\") #>     } #>     backend <- list(source = source, data_key = data_key, mask_key = mask_key,  #>         preload = preload, cache_size = cache_size, store = NULL,  #>         data_array = NULL, mask_array = NULL, dims = NULL, is_open = FALSE) #>     class(backend) <- c(\"zarr_backend\", \"storage_backend\") #>     backend #> } #> <bytecode: 0x55ce361c0508> #> <environment: namespace:fmridataset> #>  #> $zarr$description #> [1] \"Zarr format backend\" #>  #> $zarr$validate_function #> NULL #>  #> $zarr$registered_at #> [1] \"2026-01-22 12:18:50 UTC\" #>  #>   # Get specific backend info get_backend_registry(\"nifti\") #> $name #> [1] \"nifti\" #>  #> $factory #> function (source, mask_source, preload = FALSE, mode = c(\"normal\",  #>     \"bigvec\", \"mmap\", \"filebacked\"), dummy_mode = FALSE)  #> { #>     mode <- match.arg(mode) #>     if (is.character(source)) { #>         if (!dummy_mode && !all(file.exists(source))) { #>             missing_files <- source[!file.exists(source)] #>             stop_fmridataset(fmridataset_error_backend_io, message = sprintf(\"Source files not found: %s\",  #>                 paste(missing_files, collapse = \", \")), file = missing_files,  #>                 operation = \"open\") #>         } #>     } #>     else if (is.list(source)) { #>         valid_types <- vapply(source, function(x) { #>             inherits(x, \"NeuroVec\") #>         }, logical(1)) #>         if (!all(valid_types)) { #>             stop_fmridataset(fmridataset_error_config, message = \"All source objects must be NeuroVec objects\",  #>                 parameter = \"source\") #>         } #>     } #>     else { #>         stop_fmridataset(fmridataset_error_config, message = \"source must be character vector (file paths) or list (in-memory objects)\",  #>             parameter = \"source\", value = class(source)) #>     } #>     if (is.character(mask_source)) { #>         if (!dummy_mode && !file.exists(mask_source)) { #>             stop_fmridataset(fmridataset_error_backend_io, message = sprintf(\"Mask file not found: %s\",  #>                 mask_source), file = mask_source, operation = \"open\") #>         } #>     } #>     else if (!inherits(mask_source, \"NeuroVol\")) { #>         stop_fmridataset(fmridataset_error_config, message = \"mask_source must be file path or NeuroVol object\",  #>             parameter = \"mask_source\", value = class(mask_source)) #>     } #>     backend <- new.env(parent = emptyenv()) #>     backend$source <- source #>     backend$mask_source <- mask_source #>     backend$preload <- preload #>     backend$mode <- mode #>     backend$dummy_mode <- dummy_mode #>     backend$data <- NULL #>     backend$mask <- NULL #>     backend$mask_vec <- NULL #>     backend$dims <- NULL #>     backend$metadata <- NULL #>     backend$run_length <- NULL #>     backend$cache <- cachem::cache_mem(max_size = 64 * 1024^2,  #>         evict = \"lru\") #>     class(backend) <- c(\"nifti_backend\", \"storage_backend\") #>     backend #> } #> <bytecode: 0x55ce37dd9218> #> <environment: namespace:fmridataset> #>  #> $description #> [1] \"NIfTI format backend using neuroim2\" #>  #> $validate_function #> NULL #>  #> $registered_at #> [1] \"2026-01-22 12:18:50 UTC\" #>"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_component_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Component Information — get_component_info","title":"Get Component Information — get_component_info","text":"Get metadata latent components dataset.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_component_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Component Information — get_component_info","text":"","code":"get_component_info(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_component_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Component Information — get_component_info","text":"x latent_dataset object ... Additional arguments","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_component_info.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Component Information — get_component_info","text":"list containing component metadata","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Data from fMRI Dataset Objects — get_data","title":"Get Data from fMRI Dataset Objects — get_data","text":"Generic function extract data various fMRI dataset types. Returns underlying data native format (NeuroVec, matrix, etc.).","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Data from fMRI Dataset Objects — get_data","text":"","code":"get_data(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Data from fMRI Dataset Objects — get_data","text":"x fMRI dataset object (e.g., fmri_dataset, matrix_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Data from fMRI Dataset Objects — get_data","text":"Dataset-specific data object: fmri_dataset: Returns underlying NeuroVec matrix matrix_dataset: Returns data matrix","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Data from fMRI Dataset Objects — get_data","text":"function extracts raw data dataset objects, preserving original data type. NeuroVec-based datasets, returns NeuroVec object. matrix-based datasets, returns matrix.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Data from fMRI Dataset Objects — get_data","text":"","code":"# \\donttest{ # Create a matrix dataset mat <- matrix(rnorm(100 * 50), nrow = 100, ncol = 50) ds <- matrix_dataset(mat, TR = 2, run_length = 100)  # Extract the data data <- get_data(ds) identical(data, mat) # TRUE #> [1] TRUE # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Data Matrix from fMRI Dataset Objects — get_data_matrix","title":"Get Data Matrix from fMRI Dataset Objects — get_data_matrix","text":"Generic function extract data matrix various fMRI dataset types. Always returns matrix timepoints rows voxels columns.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Data Matrix from fMRI Dataset Objects — get_data_matrix","text":"","code":"get_data_matrix(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Data Matrix from fMRI Dataset Objects — get_data_matrix","text":"x fMRI dataset object (e.g., fmri_dataset, matrix_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Data Matrix from fMRI Dataset Objects — get_data_matrix","text":"numeric matrix dimensions: Rows: Number timepoints Columns: Number voxels (within mask)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data_matrix.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Data Matrix from fMRI Dataset Objects — get_data_matrix","text":"function provides unified interface accessing fMRI data matrix, regardless underlying storage format. returned matrix always timepoints rows voxels columns, matching conventional fMRI data organization.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_data_matrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Data Matrix from fMRI Dataset Objects — get_data_matrix","text":"","code":"# \\donttest{ # Create a matrix dataset mat <- matrix(rnorm(100 * 50), nrow = 100, ncol = 50) ds <- matrix_dataset(mat, TR = 2, run_length = 100)  # Extract as matrix data_mat <- get_data_matrix(ds) dim(data_mat) # 100 x 50 #> [1] 100  50 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_latent_scores.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Latent Scores from Dataset — get_latent_scores","title":"Get Latent Scores from Dataset — get_latent_scores","text":"Extract latent scores (temporal components) latent dataset. primary data access method latent datasets.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_latent_scores.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Latent Scores from Dataset — get_latent_scores","text":"","code":"get_latent_scores(x, rows = NULL, cols = NULL, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_latent_scores.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Latent Scores from Dataset — get_latent_scores","text":"x latent_dataset object rows Optional row indices (timepoints) extract cols Optional column indices (components) extract ... Additional arguments","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_latent_scores.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Latent Scores from Dataset — get_latent_scores","text":"Matrix latent scores (time × components)","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_mask.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Mask from fMRI Dataset Objects — get_mask","title":"Get Mask from fMRI Dataset Objects — get_mask","text":"Generic function extract masks various fMRI dataset types. Returns mask appropriate format dataset type.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_mask.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Mask from fMRI Dataset Objects — get_mask","text":"","code":"get_mask(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_mask.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Mask from fMRI Dataset Objects — get_mask","text":"x fMRI dataset object (e.g., fmri_dataset, matrix_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_mask.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Mask from fMRI Dataset Objects — get_mask","text":"Mask object appropriate dataset type: matrix_dataset: Logical vector fmri_dataset: NeuroVol logical vector","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_mask.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Mask from fMRI Dataset Objects — get_mask","text":"mask defines voxels included analysis. Different dataset types may store masks different formats (logical vectors, NeuroVol objects, etc.). function provides unified interface mask extraction.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_mask.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Mask from fMRI Dataset Objects — get_mask","text":"","code":"# \\donttest{ # Create a matrix dataset (matrix_dataset creates default mask internally) mat <- matrix(rnorm(100 * 50), nrow = 100, ncol = 50) ds <- matrix_dataset(mat, TR = 2, run_length = 100)  # Extract the mask (matrix_dataset creates all TRUE mask by default) extracted_mask <- get_mask(ds) sum(extracted_mask) # 50 (all TRUE values) #> [1] 50 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_duration.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Run Duration from Sampling Frame — get_run_duration","title":"Get Run Duration from Sampling Frame — get_run_duration","text":"Calculates duration run seconds.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_duration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Run Duration from Sampling Frame — get_run_duration","text":"","code":"get_run_duration(x, ...)  # S3 method for class 'matrix_dataset' get_run_duration(x, ...)  # S3 method for class 'fmri_dataset' get_run_duration(x, ...)  # S3 method for class 'fmri_mem_dataset' get_run_duration(x, ...)  # S3 method for class 'fmri_file_dataset' get_run_duration(x, ...)  # S3 method for class 'fmri_study_dataset' get_run_duration(x, ...)  # S3 method for class 'sampling_frame' get_run_duration(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_duration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Run Duration from Sampling Frame — get_run_duration","text":"x object containing temporal structure (e.g., sampling_frame, fmri_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_duration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Run Duration from Sampling Frame — get_run_duration","text":"Numeric vector element represents duration corresponding run seconds","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_duration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Run Duration from Sampling Frame — get_run_duration","text":"","code":"# \\donttest{ # Create a sampling frame with different run lengths sf <- fmrihrf::sampling_frame(blocklens = c(100, 120), TR = 2) get_run_duration(sf) # Returns: c(200, 240) seconds #> [1] 200 240 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_lengths.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Run Lengths from Sampling Frame — get_run_lengths","title":"Get Run Lengths from Sampling Frame — get_run_lengths","text":"Extracts lengths individual runs/blocks objects containing temporal structure information.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_lengths.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Run Lengths from Sampling Frame — get_run_lengths","text":"","code":"get_run_lengths(x, ...)  # S3 method for class 'matrix_dataset' get_run_lengths(x, ...)  # S3 method for class 'fmri_dataset' get_run_lengths(x, ...)  # S3 method for class 'fmri_mem_dataset' get_run_lengths(x, ...)  # S3 method for class 'fmri_file_dataset' get_run_lengths(x, ...)  # S3 method for class 'fmri_study_dataset' get_run_lengths(x, ...)  # S3 method for class 'sampling_frame' get_run_lengths(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_lengths.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Run Lengths from Sampling Frame — get_run_lengths","text":"x object containing temporal structure (e.g., sampling_frame, fmri_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_lengths.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Run Lengths from Sampling Frame — get_run_lengths","text":"Integer vector element represents number timepoints corresponding run","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_lengths.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Run Lengths from Sampling Frame — get_run_lengths","text":"function synonymous blocklens uses terminology common fMRI analysis. run represents continuous acquisition period, run length number timepoints (volumes) run.","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_run_lengths.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Run Lengths from Sampling Frame — get_run_lengths","text":"","code":"# \\donttest{ # Create a sampling frame with 3 runs sf <- fmrihrf::sampling_frame(blocklens = c(100, 120, 110), TR = 2) get_run_lengths(sf) # Returns: c(100, 120, 110) #> [1] 100 120 110 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_spatial_loadings.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Spatial Loadings from Dataset — get_spatial_loadings","title":"Get Spatial Loadings from Dataset — get_spatial_loadings","text":"Extract spatial loadings (spatial components) latent dataset.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_spatial_loadings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Spatial Loadings from Dataset — get_spatial_loadings","text":"","code":"get_spatial_loadings(x, components = NULL, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_spatial_loadings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Spatial Loadings from Dataset — get_spatial_loadings","text":"x latent_dataset object components Optional component indices extract ... Additional arguments","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_spatial_loadings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Spatial Loadings from Dataset — get_spatial_loadings","text":"Matrix sparse matrix spatial loadings (voxels × components)","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_total_duration.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Total Duration from Sampling Frame — get_total_duration","title":"Get Total Duration from Sampling Frame — get_total_duration","text":"Calculates total duration fMRI acquisition seconds across runs.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_total_duration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Total Duration from Sampling Frame — get_total_duration","text":"","code":"get_total_duration(x, ...)  # S3 method for class 'matrix_dataset' get_total_duration(x, ...)  # S3 method for class 'fmri_dataset' get_total_duration(x, ...)  # S3 method for class 'fmri_mem_dataset' get_total_duration(x, ...)  # S3 method for class 'fmri_file_dataset' get_total_duration(x, ...)  # S3 method for class 'fmri_study_dataset' get_total_duration(x, ...)  # S3 method for class 'sampling_frame' get_total_duration(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_total_duration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Total Duration from Sampling Frame — get_total_duration","text":"x object containing temporal structure (e.g., sampling_frame, fmri_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_total_duration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Total Duration from Sampling Frame — get_total_duration","text":"Numeric value representing total duration seconds","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/get_total_duration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Total Duration from Sampling Frame — get_total_duration","text":"","code":"# \\donttest{ # Create a sampling frame: 220 timepoints with TR = 2 seconds sf <- fmrihrf::sampling_frame(blocklens = c(100, 120), TR = 2) get_total_duration(sf) # Returns: 440 seconds #> [1] 440 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/grapes-or-or-grapes.html","id":null,"dir":"Reference","previous_headings":"","what":"Null-coalescing operator — grapes-or-or-grapes","title":"Null-coalescing operator — grapes-or-or-grapes","text":"x NULL, return y; otherwise return x","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/grapes-or-or-grapes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Null-coalescing operator — grapes-or-or-grapes","text":"","code":"x %||% y"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/group_map.html","id":null,"dir":"Reference","previous_headings":"","what":"Map a function over subjects in an fmri_group — group_map","title":"Map a function over subjects in an fmri_group — group_map","text":"Map function subjects fmri_group","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/group_map.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Map a function over subjects in an fmri_group — group_map","text":"","code":"group_map(   gd,   .f,   ...,   out = c(\"list\", \"bind_rows\"),   order_by = NULL,   on_error = c(\"stop\", \"warn\", \"skip\") )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/group_map.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Map a function over subjects in an fmri_group — group_map","text":"gd fmri_group. .f function signature function(row, ...) row one-row data.frame corresponding single subject. ... Additional arguments passed .f. Either \"list\" (default) \"bind_rows\" describing collect outputs. order_by Optional column name used define iteration order. on_error One \"stop\", \"warn\", \"skip\" describing handle errors raised .f.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/group_map.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Map a function over subjects in an fmri_group — group_map","text":"Either list (= \"list\") bound table (= \"bind_rows\").","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/group_reduce.html","id":null,"dir":"Reference","previous_headings":"","what":"Reduce over subjects in a single pass — group_reduce","title":"Reduce over subjects in a single pass — group_reduce","text":"Reduce subjects single pass","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/group_reduce.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reduce over subjects in a single pass — group_reduce","text":"","code":"group_reduce(   gd,   .map,   .reduce,   .init,   order_by = NULL,   on_error = c(\"stop\", \"warn\", \"skip\"),   ... )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/group_reduce.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reduce over subjects in a single pass — group_reduce","text":"gd fmri_group. .map Function applied subject row. return object can combined .reduce. .reduce Binary function combining accumulator mapped value. .init Initial accumulator value. order_by Optional ordering column. on_error Error handling policy: \"stop\", \"warn\", \"skip\". ... Additional arguments passed .map.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/group_reduce.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reduce over subjects in a single pass — group_reduce","text":"reduced value visiting subjects.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/h5-backend.html","id":null,"dir":"Reference","previous_headings":"","what":"H5 Storage Backend — h5-backend","title":"H5 Storage Backend — h5-backend","text":"storage backend implementation H5 format neuroimaging data using fmristore. scan stored H5 file loads H5NeuroVec object.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/h5-backend.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"H5 Storage Backend — h5-backend","text":"H5Backend integrates fmristore package work : File paths H5 neuroimaging files Pre-loaded H5NeuroVec objects fmristore Multiple H5 files representing different scans","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/h5_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an H5 Backend — h5_backend","title":"Create an H5 Backend — h5_backend","text":"Create H5 Backend","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/h5_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an H5 Backend — h5_backend","text":"","code":"h5_backend(   source,   mask_source,   mask_dataset = \"data/elements\",   data_dataset = \"data\",   preload = FALSE )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/h5_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an H5 Backend — h5_backend","text":"source Character vector file paths H5 files list H5NeuroVec objects mask_source File path H5 mask file, H5 file containing mask, -memory NeuroVol object mask_dataset Character string specifying dataset path within H5 file mask (default: \"data/elements\") data_dataset Character string specifying dataset path within H5 files data (default: \"data\") preload Logical, whether eagerly load H5NeuroVec objects memory","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/h5_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an H5 Backend — h5_backend","text":"h5_backend S3 object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/hasMethod.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if a method exists for a given class — hasMethod","title":"Check if a method exists for a given class — hasMethod","text":"Internal helper check S3 method existence","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/hasMethod.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if a method exists for a given class — hasMethod","text":"","code":"hasMethod(generic, class)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/hasMethod.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if a method exists for a given class — hasMethod","text":"generic Generic function name class Class name","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/index_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"Index-based Series Selector — index_selector","title":"Index-based Series Selector — index_selector","text":"Select voxels direct indices masked data.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/index_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Index-based Series Selector — index_selector","text":"","code":"index_selector(indices)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/index_selector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Index-based Series Selector — index_selector","text":"indices Integer vector voxel indices","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/index_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Index-based Series Selector — index_selector","text":"object class index_selector","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/index_selector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Index-based Series Selector — index_selector","text":"","code":"# Select first 10 voxels sel <- index_selector(1:10)  # Select specific voxels sel <- index_selector(c(1, 5, 10, 20))"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is.fmri_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if object is an fmri_series — is.fmri_series","title":"Check if object is an fmri_series — is.fmri_series","text":"Check object fmri_series","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is.fmri_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if object is an fmri_series — is.fmri_series","text":"","code":"is.fmri_series(x)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is.fmri_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if object is an fmri_series — is.fmri_series","text":"x object test","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is.fmri_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if object is an fmri_series — is.fmri_series","text":"Logical TRUE x fmri_series object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is.sampling_frame.html","id":null,"dir":"Reference","previous_headings":"","what":"Test if Object is a Sampling Frame — is.sampling_frame","title":"Test if Object is a Sampling Frame — is.sampling_frame","text":"function tests whether object class 'sampling_frame'.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is.sampling_frame.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Test if Object is a Sampling Frame — is.sampling_frame","text":"","code":"is.sampling_frame(x)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is.sampling_frame.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Test if Object is a Sampling Frame — is.sampling_frame","text":"x object test","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is.sampling_frame.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Test if Object is a Sampling Frame — is.sampling_frame","text":"TRUE x sampling_frame object, FALSE otherwise","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is_backend_registered.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Backend is Registered — is_backend_registered","title":"Check if Backend is Registered — is_backend_registered","text":"Tests whether backend type registered system.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is_backend_registered.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Backend is Registered — is_backend_registered","text":"","code":"is_backend_registered(name)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is_backend_registered.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Backend is Registered — is_backend_registered","text":"name Character string, name backend check","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is_backend_registered.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Backend is Registered — is_backend_registered","text":"Logical, TRUE backend registered","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/is_backend_registered.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check if Backend is Registered — is_backend_registered","text":"","code":"is_backend_registered(\"nifti\") # TRUE (built-in) #> [1] TRUE is_backend_registered(\"custom\") # FALSE (unless registered) #> [1] FALSE"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/iter_subjects.html","id":null,"dir":"Reference","previous_headings":"","what":"Iterate subjects one-by-one (streaming) — iter_subjects","title":"Iterate subjects one-by-one (streaming) — iter_subjects","text":"Iterate subjects one--one (streaming)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/iter_subjects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Iterate subjects one-by-one (streaming) — iter_subjects","text":"","code":"iter_subjects(gd, order_by = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/iter_subjects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Iterate subjects one-by-one (streaming) — iter_subjects","text":"gd fmri_group. order_by Optional character scalar giving column used order iteration. supplied, subjects iterated ascending order column (NA values placed last).","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/iter_subjects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Iterate subjects one-by-one (streaming) — iter_subjects","text":"list single element next yields one-row data.frame subject called repeatedly. dataset column automatically flattened underlying fmri_dataset object.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent-backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Latent Storage Backend — latent-backend","title":"Latent Storage Backend — latent-backend","text":"storage backend implementation latent space representations fMRI data. backend works data decomposed temporal components (basis functions) spatial loadings.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent-backend.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Latent Storage Backend — latent-backend","text":"Unlike traditional voxel-based backends, latent backends store: Temporal basis functions (time × components) Spatial loadings (voxels × components) Optional per-voxel offsets backend maintains compatibility storage_backend contract providing specialized methods latent data access.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Latent Backend — latent_backend","title":"Create a Latent Backend — latent_backend","text":"Creates storage backend latent space fMRI data.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Latent Backend — latent_backend","text":"","code":"latent_backend(source, preload = FALSE)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Latent Backend — latent_backend","text":"source Character vector paths LatentNeuroVec HDF5 files (.lv.h5) list LatentNeuroVec objects fmristore package. preload Logical, whether load data memory (default: FALSE)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Latent Backend — latent_backend","text":"latent_backend S3 object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_backend.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Latent Backend — latent_backend","text":"","code":"if (FALSE) { # \\dontrun{ # From HDF5 files backend <- latent_backend(c(\"run1.lv.h5\", \"run2.lv.h5\"))  # From pre-loaded objects lvec1 <- fmristore::read_vec(\"run1.lv.h5\") lvec2 <- fmristore::read_vec(\"run2.lv.h5\") backend <- latent_backend(list(lvec1, lvec2)) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Latent Dataset Interface — latent_dataset","title":"Latent Dataset Interface — latent_dataset","text":"specialized dataset interface working latent space representations fMRI data. Unlike traditional fMRI datasets work voxel-space data, latent datasets operate compressed representations using basis functions. interface designed data decomposed temporal components (basis functions) spatial loadings, PCA, ICA, dictionary learning methods. Creates dataset object working latent space representations fMRI data. primary constructor latent datasets.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Latent Dataset Interface — latent_dataset","text":"","code":"latent_dataset(   source,   TR,   run_length,   event_table = data.frame(),   base_path = \".\",   censor = NULL,   preload = FALSE )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Latent Dataset Interface — latent_dataset","text":"source Character vector file paths LatentNeuroVec HDF5 files (.lv.h5) list LatentNeuroVec objects fmristore package. TR repetition time seconds. run_length Vector integers indicating number scans run. event_table Optional data.frame containing event onsets experimental variables. base_path Base directory relative file paths. censor Optional binary vector indicating scans remove. preload Logical indicating whether preload data memory.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Latent Dataset Interface — latent_dataset","text":"latent_dataset object class c(\"latent_dataset\", \"fmri_dataset\").","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_dataset.html","id":"key-differences-from-standard-datasets-","dir":"Reference","previous_headings":"","what":"Key Differences from Standard Datasets:","title":"Latent Dataset Interface — latent_dataset","text":"Data Access: Returns latent scores (time × components) instead voxel data Mask: Represents active components, spatial voxels Dimensions: Component space rather voxel space Reconstruction: Can optionally reconstruct voxel space demand","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_dataset.html","id":"data-structure-","dir":"Reference","previous_headings":"","what":"Data Structure:","title":"Latent Dataset Interface — latent_dataset","text":"Latent representations store data : basis: Temporal components (n_timepoints × k_components) loadings: Spatial components (n_voxels × k_components) offset: Optional per-voxel offset terms Reconstruction: data = basis %*% t(loadings) + offset","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/latent_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Latent Dataset Interface — latent_dataset","text":"","code":"if (FALSE) { # \\dontrun{ # From LatentNeuroVec files dataset <- latent_dataset(   source = c(\"run1.lv.h5\", \"run2.lv.h5\"),   TR = 2,   run_length = c(100, 100) )  # From pre-loaded objects lvec1 <- fmristore::read_vec(\"run1.lv.h5\") lvec2 <- fmristore::read_vec(\"run2.lv.h5\") dataset <- latent_dataset(   source = list(lvec1, lvec2),   TR = 2,   run_length = c(100, 100) )  # Access latent scores scores <- get_latent_scores(dataset)  # Get component metadata comp_info <- get_component_info(dataset) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/left_join_subjects.html","id":null,"dir":"Reference","previous_headings":"","what":"Left join additional subject metadata — left_join_subjects","title":"Left join additional subject metadata — left_join_subjects","text":"Left join additional subject metadata","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/left_join_subjects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Left join additional subject metadata — left_join_subjects","text":"","code":"left_join_subjects(gd, y, by = NULL, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/left_join_subjects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Left join additional subject metadata — left_join_subjects","text":"gd fmri_group. y data frame containing additional subject-level columns. Character vector join keys (defaults group id column). ... Additional arguments passed dplyr::left_join() available.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/left_join_subjects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Left join additional subject metadata — left_join_subjects","text":"fmri_group metadata y attached.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/list_backend_names.html","id":null,"dir":"Reference","previous_headings":"","what":"List Registered Backend Names — list_backend_names","title":"List Registered Backend Names — list_backend_names","text":"Returns character vector registered backend names.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/list_backend_names.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"List Registered Backend Names — list_backend_names","text":"","code":"list_backend_names()"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/list_backend_names.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"List Registered Backend Names — list_backend_names","text":"Character vector backend names","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/list_backend_names.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"List Registered Backend Names — list_backend_names","text":"","code":"list_backend_names() #> [1] \"h5\"     \"latent\" \"matrix\" \"nifti\"  \"study\"  \"zarr\""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask-standards.html","id":null,"dir":"Reference","previous_headings":"","what":"Mask Representation Standards — mask-standards","title":"Mask Representation Standards — mask-standards","text":"package enforces consistent mask representation across components:","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask-standards.html","id":"backend-level-internal-","dir":"Reference","previous_headings":"","what":"Backend Level (Internal)","title":"Mask Representation Standards — mask-standards","text":"backend_get_mask() always returns logical vector Length equals product spatial dimensions TRUE indicates valid voxels, FALSE indicates excluded voxels NA values allowed Must contain least one TRUE value","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask-standards.html","id":"user-level-public-api-","dir":"Reference","previous_headings":"","what":"User Level (Public API)","title":"Mask Representation Standards — mask-standards","text":"get_mask() returns format appropriate dataset type: volumetric datasets: 3D array NeuroVol object matrix datasets: logical vector latent datasets: logical vector (components, voxels)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask-standards.html","id":"conversion-helpers","dir":"Reference","previous_headings":"","what":"Conversion Helpers","title":"Mask Representation Standards — mask-standards","text":"mask_to_logical(): Convert mask representation logical vector mask_to_volume(): Convert logical vector 3D array given dimensions","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"Mask-based Series Selector — mask_selector","title":"Mask-based Series Selector — mask_selector","text":"Select voxels TRUE binary mask.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mask-based Series Selector — mask_selector","text":"","code":"mask_selector(mask)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_selector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mask-based Series Selector — mask_selector","text":"mask logical vector matching dataset's mask length, 3D logical array","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mask-based Series Selector — mask_selector","text":"object class mask_selector","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_selector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mask-based Series Selector — mask_selector","text":"","code":"if (FALSE) { # \\dontrun{ # Using a logical vector mask_vec <- backend_get_mask(dataset$backend) sel <- mask_selector(mask_vec > 0.5) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_to_logical.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Mask to Logical Vector — mask_to_logical","title":"Convert Mask to Logical Vector — mask_to_logical","text":"Converts mask representation logical vector.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_to_logical.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Mask to Logical Vector — mask_to_logical","text":"","code":"mask_to_logical(mask)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_to_logical.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Mask to Logical Vector — mask_to_logical","text":"mask mask supported format (numeric, logical, array, NeuroVol)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_to_logical.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Mask to Logical Vector — mask_to_logical","text":"logical vector","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_to_volume.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Logical Vector to Volume — mask_to_volume","title":"Convert Logical Vector to Volume — mask_to_volume","text":"Converts logical vector mask 3D array.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_to_volume.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Logical Vector to Volume — mask_to_volume","text":"","code":"mask_to_volume(mask_vec, dims)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_to_volume.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Logical Vector to Volume — mask_to_volume","text":"mask_vec logical vector dims Spatial dimensions (length 3)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mask_to_volume.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Logical Vector to Volume — mask_to_volume","text":"3D logical array","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix-backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix Storage Backend — matrix-backend","title":"Matrix Storage Backend — matrix-backend","text":"storage backend implementation -memory matrix data. backend wraps existing matrix data storage backend interface.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Matrix Backend — matrix_backend","title":"Create a Matrix Backend — matrix_backend","text":"Create Matrix Backend","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Matrix Backend — matrix_backend","text":"","code":"matrix_backend(data_matrix, mask = NULL, spatial_dims = NULL, metadata = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Matrix Backend — matrix_backend","text":"data_matrix matrix timepoints × voxels orientation mask Logical vector indicating voxels valid spatial_dims Numeric vector length 3 specifying spatial dimensions metadata Optional list metadata","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Matrix Backend — matrix_backend","text":"matrix_backend S3 object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Matrix Dataset Constructor — matrix_dataset","title":"Matrix Dataset Constructor — matrix_dataset","text":"function creates matrix dataset object, list containing information data matrix, TR, number runs, event table, sampling frame, mask.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Matrix Dataset Constructor — matrix_dataset","text":"","code":"matrix_dataset(datamat, TR, run_length, event_table = data.frame())"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Matrix Dataset Constructor — matrix_dataset","text":"datamat matrix column voxel time-series. TR Repetition time (TR) fMRI acquisition. run_length numeric vector specifying length run dataset. event_table optional data frame containing event information. Default empty data frame.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Matrix Dataset Constructor — matrix_dataset","text":"matrix dataset object class c(\"matrix_dataset\", \"fmri_dataset\", \"list\").","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/matrix_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Matrix Dataset Constructor — matrix_dataset","text":"","code":"# A matrix with 100 rows and 100 columns (voxels) X <- matrix(rnorm(100 * 100), 100, 100) dset <- matrix_dataset(X, TR = 2, run_length = 100)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mutate_subjects.html","id":null,"dir":"Reference","previous_headings":"","what":"Mutate subject-level attributes — mutate_subjects","title":"Mutate subject-level attributes — mutate_subjects","text":"Adds modifies columns underlying subjects table. Expressions evaluated sequentially newly created columns available later expressions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mutate_subjects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mutate subject-level attributes — mutate_subjects","text":"","code":"mutate_subjects(gd, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mutate_subjects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mutate subject-level attributes — mutate_subjects","text":"gd fmri_group. ... Logical expressions used filter rows.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/mutate_subjects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mutate subject-level attributes — mutate_subjects","text":"updated fmri_group modified subject attributes.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_runs.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Number of Runs from Sampling Frame — n_runs","title":"Get Number of Runs from Sampling Frame — n_runs","text":"Extracts total number runs/blocks objects containing temporal structure information.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_runs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Number of Runs from Sampling Frame — n_runs","text":"","code":"n_runs(x, ...)  # S3 method for class 'matrix_dataset' n_runs(x, ...)  # S3 method for class 'fmri_dataset' n_runs(x, ...)  # S3 method for class 'fmri_mem_dataset' n_runs(x, ...)  # S3 method for class 'fmri_file_dataset' n_runs(x, ...)  # S3 method for class 'fmri_study_dataset' n_runs(x, ...)  # S3 method for class 'sampling_frame' n_runs(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_runs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Number of Runs from Sampling Frame — n_runs","text":"x object containing temporal structure (e.g., sampling_frame, fmri_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_runs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Number of Runs from Sampling Frame — n_runs","text":"Integer representing total number runs","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_runs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Number of Runs from Sampling Frame — n_runs","text":"","code":"# \\donttest{ # Create a sampling frame with 3 runs sf <- fmrihrf::sampling_frame(blocklens = c(100, 120, 110), TR = 2) n_runs(sf) # Returns: 3 #> [1] 3 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_subjects.html","id":null,"dir":"Reference","previous_headings":"","what":"Number of subjects in a group — n_subjects","title":"Number of subjects in a group — n_subjects","text":"Number subjects group","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_subjects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Number of subjects in a group — n_subjects","text":"","code":"n_subjects(gd)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_subjects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Number of subjects in a group — n_subjects","text":"gd fmri_group.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_subjects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Number of subjects in a group — n_subjects","text":"Integer number subjects.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_timepoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Number of Timepoints from Sampling Frame — n_timepoints","title":"Get Number of Timepoints from Sampling Frame — n_timepoints","text":"Extracts total number timepoints (volumes) across runs objects containing temporal structure information.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_timepoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Number of Timepoints from Sampling Frame — n_timepoints","text":"","code":"n_timepoints(x, ...)  # S3 method for class 'matrix_dataset' n_timepoints(x, ...)  # S3 method for class 'fmri_dataset' n_timepoints(x, ...)  # S3 method for class 'fmri_mem_dataset' n_timepoints(x, ...)  # S3 method for class 'fmri_file_dataset' n_timepoints(x, ...)  # S3 method for class 'fmri_study_dataset' n_timepoints(x, ...)  # S3 method for class 'sampling_frame' n_timepoints(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_timepoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Number of Timepoints from Sampling Frame — n_timepoints","text":"x object containing temporal structure (e.g., sampling_frame, fmri_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_timepoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Number of Timepoints from Sampling Frame — n_timepoints","text":"Integer representing total number timepoints","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/n_timepoints.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Number of Timepoints from Sampling Frame — n_timepoints","text":"","code":"# \\donttest{ # Create a sampling frame with 3 runs sf <- fmrihrf::sampling_frame(blocklens = c(100, 120, 110), TR = 2) n_timepoints(sf) # Returns: 330 (sum of run lengths) #> [1] 330 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/ncol.fmri_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Number of columns in fmri_series — ncol.fmri_series","title":"Number of columns in fmri_series — ncol.fmri_series","text":"Number columns fmri_series","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/ncol.fmri_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Number of columns in fmri_series — ncol.fmri_series","text":"","code":"# S3 method for class 'fmri_series' ncol(x)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/ncol.fmri_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Number of columns in fmri_series — ncol.fmri_series","text":"x fmri_series object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/ncol.fmri_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Number of columns in fmri_series — ncol.fmri_series","text":"Number voxels","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/new_fmri_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Constructor for fmri_series objects — new_fmri_series","title":"Constructor for fmri_series objects — new_fmri_series","text":"Constructor fmri_series objects","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/new_fmri_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Constructor for fmri_series objects — new_fmri_series","text":"","code":"new_fmri_series(data, voxel_info, temporal_info, selection_info, dataset_info)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/new_fmri_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Constructor for fmri_series objects — new_fmri_series","text":"data lazy matrix (e.g., delarr), DelayedMatrix, base matrix voxel_info data.frame containing spatial metadata voxel temporal_info data.frame containing metadata timepoint selection_info list describing data selected dataset_info list describing source dataset backend","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/new_fmri_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Constructor for fmri_series objects — new_fmri_series","text":"object class fmri_series","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nifti-backend.html","id":null,"dir":"Reference","previous_headings":"","what":"NIfTI Storage Backend — nifti-backend","title":"NIfTI Storage Backend — nifti-backend","text":"storage backend implementation NIfTI format neuroimaging data. Supports file-based -memory NIfTI data.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nifti-backend.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"NIfTI Storage Backend — nifti-backend","text":"NiftiBackend can work : File paths NIfTI images Pre-loaded neuroim2 NeuroVec objects","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nifti_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a NIfTI Backend — nifti_backend","title":"Create a NIfTI Backend — nifti_backend","text":"Create NIfTI Backend","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nifti_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a NIfTI Backend — nifti_backend","text":"","code":"nifti_backend(   source,   mask_source,   preload = FALSE,   mode = c(\"normal\", \"bigvec\", \"mmap\", \"filebacked\"),   dummy_mode = FALSE )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nifti_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a NIfTI Backend — nifti_backend","text":"source Character vector file paths list -memory NeuroVec objects mask_source File path mask -memory NeuroVol object preload Logical, whether eagerly load data memory mode Storage mode file-backed data: 'normal', 'bigvec', 'mmap', 'filebacked' dummy_mode Logical, TRUE allows non-existent files (testing). Default FALSE.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nifti_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a NIfTI Backend — nifti_backend","text":"nifti_backend S3 object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nrow.fmri_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Number of rows in fmri_series — nrow.fmri_series","title":"Number of rows in fmri_series — nrow.fmri_series","text":"Number rows fmri_series","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nrow.fmri_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Number of rows in fmri_series — nrow.fmri_series","text":"","code":"# S3 method for class 'fmri_series' nrow(x)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nrow.fmri_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Number of rows in fmri_series — nrow.fmri_series","text":"x fmri_series object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/nrow.fmri_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Number of rows in fmri_series — nrow.fmri_series","text":"Number timepoints","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.backend_registry.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Backend Registry — print.backend_registry","title":"Print Backend Registry — print.backend_registry","text":"Prints formatted summary registered backends.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.backend_registry.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Backend Registry — print.backend_registry","text":"","code":"# S3 method for class 'backend_registry' print(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.backend_registry.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Backend Registry — print.backend_registry","text":"x Result get_backend_registry() ... Additional arguments (ignored)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.backend_registry.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Backend Registry — print.backend_registry","text":"Invisibly returns input","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.fmri_series.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Method for fmri_series Objects — print.fmri_series","title":"Print Method for fmri_series Objects — print.fmri_series","text":"Display concise summary fmri_series object, including dimensions, selector type, backend, data orientation.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.fmri_series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Method for fmri_series Objects — print.fmri_series","text":"","code":"# S3 method for class 'fmri_series' print(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.fmri_series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Method for fmri_series Objects — print.fmri_series","text":"x fmri_series object ... Additional arguments (unused)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.fmri_series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Method for fmri_series Objects — print.fmri_series","text":"Returns x invisibly. Called side effect printing console.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.fmri_series.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print Method for fmri_series Objects — print.fmri_series","text":"","code":"# \\donttest{ # This method is called automatically when printing # Create example object (see fmri_series documentation) # fs <- new_fmri_series(...) # fs  # Automatically calls print method # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Methods for fmridataset Objects — print","title":"Print Methods for fmridataset Objects — print","text":"Display formatted summaries fmridataset objects including datasets, chunk iterators, data chunks. function prints summary chunk iterator. function prints summary data chunk.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Methods for fmridataset Objects — print","text":"","code":"# S3 method for class 'fmri_dataset' print(x, full = FALSE, ...)  # S3 method for class 'fmri_dataset' summary(object, ...)  # S3 method for class 'chunkiter' print(x, ...)  # S3 method for class 'data_chunk' print(x, ...)  # S3 method for class 'matrix_dataset' print(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Methods for fmridataset Objects — print","text":"x data_chunk object. full Logical; TRUE, print additional details datasets (default: FALSE) ... Additional arguments (ignored). object object summarize (summary methods)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Methods for fmridataset Objects — print","text":"object invisibly","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print Methods for fmridataset Objects — print","text":"","code":"# \\donttest{ # Print dataset summary # dataset <- fmri_dataset(...) # print(dataset) # print(dataset, full = TRUE)  # More details # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.series_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Methods for Series Selectors — print.series_selector","title":"Print Methods for Series Selectors — print.series_selector","text":"Display formatted summaries series selector objects.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.series_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Methods for Series Selectors — print.series_selector","text":"","code":"# S3 method for class 'series_selector' print(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.series_selector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Methods for Series Selectors — print.series_selector","text":"x series selector object ... Additional arguments (currently unused)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.series_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print Methods for Series Selectors — print.series_selector","text":"object invisibly","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print.series_selector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Print Methods for Series Selectors — print.series_selector","text":"","code":"# Print different selector types sel1 <- index_selector(1:10) print(sel1) #> <index_selector> #>   indices: 1, 2, 3, ... (10 total)  sel2 <- voxel_selector(c(10, 20, 15)) print(sel2) #> <voxel_selector> #>   coordinates: 1 voxel(s) #>     [10, 20, 15]"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print_dataset_info.html","id":null,"dir":"Reference","previous_headings":"","what":"Print Formatted Dataset Information — print_dataset_info","title":"Print Formatted Dataset Information — print_dataset_info","text":"Print Formatted Dataset Information","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print_dataset_info.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print Formatted Dataset Information — print_dataset_info","text":"","code":"print_dataset_info(dataset, title = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/print_dataset_info.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print Formatted Dataset Information — print_dataset_info","text":"dataset Dataset object title Optional title","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/read_dcf_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Read DCF-style configuration (backwards compatibility) — read_dcf_config","title":"Read DCF-style configuration (backwards compatibility) — read_dcf_config","text":"Read DCF-style configuration (backwards compatibility)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/read_dcf_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read DCF-style configuration (backwards compatibility) — read_dcf_config","text":"","code":"read_dcf_config(file_name)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/read_fmri_config.html","id":null,"dir":"Reference","previous_headings":"","what":"read a basic fMRI configuration file — read_fmri_config","title":"read a basic fMRI configuration file — read_fmri_config","text":"Reads fMRI configuration file YAML JSON format. replaces previous implementation used source() security reasons.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/read_fmri_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"read a basic fMRI configuration file — read_fmri_config","text":"","code":"read_fmri_config(file_name, base_path = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/read_fmri_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"read a basic fMRI configuration file — read_fmri_config","text":"file_name name configuration file (YAML JSON format) base_path file path prepended relative file names","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/read_fmri_config.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"read a basic fMRI configuration file — read_fmri_config","text":"fmri_config instance","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/reconstruct_voxels.html","id":null,"dir":"Reference","previous_headings":"","what":"Reconstruct Voxel Data from Latent Representation — reconstruct_voxels","title":"Reconstruct Voxel Data from Latent Representation — reconstruct_voxels","text":"Reconstruct full voxel-space data latent representation. computationally expensive used sparingly.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/reconstruct_voxels.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Reconstruct Voxel Data from Latent Representation — reconstruct_voxels","text":"","code":"reconstruct_voxels(x, rows = NULL, voxels = NULL, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/reconstruct_voxels.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Reconstruct Voxel Data from Latent Representation — reconstruct_voxels","text":"x latent_dataset object rows Optional row indices (timepoints) reconstruct voxels Optional voxel indices reconstruct ... Additional arguments","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/reconstruct_voxels.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Reconstruct Voxel Data from Latent Representation — reconstruct_voxels","text":"Matrix reconstructed voxel data (time × voxels)","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Register a Storage Backend — register_backend","title":"Register a Storage Backend — register_backend","text":"Registers new storage backend type global registry. External packages can use function add custom backend support.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register a Storage Backend — register_backend","text":"","code":"register_backend(   name,   factory,   description = NULL,   validate_function = NULL,   overwrite = FALSE )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Register a Storage Backend — register_backend","text":"name Character string, unique name backend type factory Function creates backend instances, must accept ... arguments description Optional character string describing backend validate_function Optional function validate backend instances beyond standard contract overwrite Logical, whether overwrite existing registration (default: FALSE)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Register a Storage Backend — register_backend","text":"Invisibly returns TRUE successful registration","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_backend.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Register a Storage Backend — register_backend","text":"factory function : Accept necessary parameters create backend instance Return object inherits \"storage_backend\" Implement required storage backend methods validate_function : Accept backend object first argument Return TRUE valid, throw informative error invalid Perform backend-specific validation beyond standard contract","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_backend.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Register a Storage Backend — register_backend","text":"","code":"if (FALSE) { # \\dontrun{ # Register a custom backend my_backend_factory <- function(source, ...) {   # Create and return backend instance   backend <- list(source = source, ...)   class(backend) <- c(\"my_backend\", \"storage_backend\")   backend }  register_backend(   name = \"my_backend\",   factory = my_backend_factory,   description = \"Custom backend for my data format\" ) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_builtin_backends.html","id":null,"dir":"Reference","previous_headings":"","what":"Register Built-in Backends — register_builtin_backends","title":"Register Built-in Backends — register_builtin_backends","text":"Registers built-backend types. called automatically package loaded, can called manually needed.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_builtin_backends.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register Built-in Backends — register_builtin_backends","text":"","code":"register_builtin_backends()"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_builtin_backends.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Register Built-in Backends — register_builtin_backends","text":"Invisibly returns TRUE","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_study_backend_seed_methods.html","id":null,"dir":"Reference","previous_headings":"","what":"Register Study Backend Seed Methods — register_study_backend_seed_methods","title":"Register Study Backend Seed Methods — register_study_backend_seed_methods","text":"Ensures DelayedArray-compatible seed associated S4 methods available. Called automatically study-level backends coerced DelayedArray objects.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/register_study_backend_seed_methods.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Register Study Backend Seed Methods — register_study_backend_seed_methods","text":"","code":"register_study_backend_seed_methods()"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_indices.html","id":null,"dir":"Reference","previous_headings":"","what":"Resolve Indices from Series Selector — resolve_indices","title":"Resolve Indices from Series Selector — resolve_indices","text":"Converts series selector specification actual voxel indices within dataset mask.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_indices.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resolve Indices from Series Selector — resolve_indices","text":"","code":"resolve_indices(selector, dataset, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_indices.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resolve Indices from Series Selector — resolve_indices","text":"selector series selector object (e.g., index_selector, voxel_selector) dataset fMRI dataset object providing spatial context ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_indices.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resolve Indices from Series Selector — resolve_indices","text":"Integer vector indices masked data","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_indices.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Resolve Indices from Series Selector — resolve_indices","text":"Series selectors provide various ways specify spatial subsets fMRI data. generic function resolves specifications actual indices can used extract data. Different selector types support different selection methods: index_selector: Direct indices masked data voxel_selector: 3D coordinates roi_selector: Region interest masks sphere_selector: Spherical regions","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_indices.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Resolve Indices from Series Selector — resolve_indices","text":"","code":"# \\donttest{ # Example with index selector sel <- index_selector(1:10) # indices <- resolve_indices(sel, dataset)  # Example with voxel coordinates sel <- voxel_selector(cbind(x = 10, y = 20, z = 15)) # indices <- resolve_indices(sel, dataset) # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"Resolve Spatial Selector — resolve_selector","title":"Resolve Spatial Selector — resolve_selector","text":"Resolve Spatial Selector","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resolve Spatial Selector — resolve_selector","text":"","code":"resolve_selector(dataset, selector)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_selector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resolve Spatial Selector — resolve_selector","text":"dataset fmri_dataset object. selector Spatial selector NULL voxels. Supported types integer indices, coordinate matrices three columns, logical ROI volumes.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resolve Spatial Selector — resolve_selector","text":"Integer vector voxel indices within dataset mask.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_timepoints.html","id":null,"dir":"Reference","previous_headings":"","what":"Resolve Timepoint Selection — resolve_timepoints","title":"Resolve Timepoint Selection — resolve_timepoints","text":"Resolve Timepoint Selection","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_timepoints.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Resolve Timepoint Selection — resolve_timepoints","text":"","code":"resolve_timepoints(dataset, timepoints)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_timepoints.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Resolve Timepoint Selection — resolve_timepoints","text":"dataset fmri_dataset object. timepoints Integer logical vector timepoints, NULL .","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/resolve_timepoints.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Resolve Timepoint Selection — resolve_timepoints","text":"Integer vector timepoint indices.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/roi_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"ROI-based Series Selector — roi_selector","title":"ROI-based Series Selector — roi_selector","text":"Select voxels within region interest (ROI) volume mask.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/roi_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ROI-based Series Selector — roi_selector","text":"","code":"roi_selector(roi)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/roi_selector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"ROI-based Series Selector — roi_selector","text":"roi 3D array, ROIVol, LogicalNeuroVol, similar mask object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/roi_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"ROI-based Series Selector — roi_selector","text":"object class roi_selector","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/roi_selector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"ROI-based Series Selector — roi_selector","text":"","code":"if (FALSE) { # \\dontrun{ # Using a binary mask mask <- array(FALSE, dim = c(64, 64, 30)) mask[30:40, 30:40, 15:20] <- TRUE sel <- roi_selector(mask) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sample_subjects.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample subjects from an fmri_group — sample_subjects","title":"Sample subjects from an fmri_group — sample_subjects","text":"Sample subjects fmri_group","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sample_subjects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample subjects from an fmri_group — sample_subjects","text":"","code":"sample_subjects(gd, n, replace = FALSE, strata = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sample_subjects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample subjects from an fmri_group — sample_subjects","text":"gd fmri_group. n Number subjects sample. strata supplied n length 1, number drawn stratum. Provide named vector request different counts per stratum. replace Logical indicating whether sample replacement. strata Optional column name used stratify sampling.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sample_subjects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample subjects from an fmri_group — sample_subjects","text":"sampled fmri_group.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/samples.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Sample Indices from Sampling Frame — samples","title":"Get Sample Indices from Sampling Frame — samples","text":"Generates vector timepoint indices, typically used time series analysis indexing operations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/samples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Sample Indices from Sampling Frame — samples","text":"","code":"samples(x, ...)  # S3 method for class 'matrix_dataset' samples(x, ...)  # S3 method for class 'fmri_dataset' samples(x, ...)  # S3 method for class 'fmri_mem_dataset' samples(x, ...)  # S3 method for class 'fmri_file_dataset' samples(x, ...)  # S3 method for class 'fmri_study_dataset' samples(x, ...)  # S3 method for class 'sampling_frame' samples(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/samples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Sample Indices from Sampling Frame — samples","text":"x object containing temporal structure (e.g., sampling_frame, fmri_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/samples.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Sample Indices from Sampling Frame — samples","text":"Integer vector 1 total number timepoints","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/samples.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get Sample Indices from Sampling Frame — samples","text":"","code":"# \\donttest{ # Create a sampling frame sf <- fmrihrf::sampling_frame(blocklens = c(100, 120), TR = 2) s <- samples(sf) length(s) # 220 #> [1] 220 range(s) # c(1, 220) #> [1]   1 220 # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sampling_frame_adapters.html","id":null,"dir":"Reference","previous_headings":"","what":"Adapter Methods for fmrihrf sampling_frame — sampling_frame_adapters","title":"Adapter Methods for fmrihrf sampling_frame — sampling_frame_adapters","text":"methods provide compatibility local sampling_frame implementation fmrihrf sampling_frame implementation.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/series.html","id":null,"dir":"Reference","previous_headings":"","what":"Deprecated alias for fmri_series — series","title":"Deprecated alias for fmri_series — series","text":"series() forwards fmri_series() backward compatibility. deprecation warning emitted per session.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/series.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Deprecated alias for fmri_series — series","text":"","code":"series(   dataset,   selector = NULL,   timepoints = NULL,   output = c(\"fmri_series\", \"DelayedMatrix\"),   event_window = NULL,   ... )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/series.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Deprecated alias for fmri_series — series","text":"dataset fmri_dataset object. selector Spatial selector NULL voxels. timepoints Optional temporal subset NULL . output Return type - \"FmriSeries\" (default) \"DelayedMatrix\". event_window Reserved future use. ... Additional arguments passed methods.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/series.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Deprecated alias for fmri_series — series","text":"See fmri_series()","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/series_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"Series Selector Classes for fMRI Data — series_selector","title":"Series Selector Classes for fMRI Data — series_selector","text":"family S3 classes specifying spatial selections fMRI datasets. selectors provide type-safe, explicit interface selecting voxels fmri_series() related functions.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sphere_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"Spherical ROI Series Selector — sphere_selector","title":"Spherical ROI Series Selector — sphere_selector","text":"Select voxels within spherical region.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sphere_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spherical ROI Series Selector — sphere_selector","text":"","code":"sphere_selector(center, radius)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sphere_selector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spherical ROI Series Selector — sphere_selector","text":"center Numeric vector length 3 (x, y, z) specifying sphere center radius Numeric radius voxel units","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sphere_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Spherical ROI Series Selector — sphere_selector","text":"object class sphere_selector","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/sphere_selector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Spherical ROI Series Selector — sphere_selector","text":"","code":"# Select 10-voxel radius sphere around voxel (30, 30, 20) sel <- sphere_selector(center = c(30, 30, 20), radius = 10)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/stop_fmridataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Stop with a Custom Error — stop_fmridataset","title":"Stop with a Custom Error — stop_fmridataset","text":"Stop Custom Error","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/stop_fmridataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stop with a Custom Error — stop_fmridataset","text":"","code":"stop_fmridataset(error_fn, message = NULL, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/stop_fmridataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stop with a Custom Error — stop_fmridataset","text":"error_fn Error constructor function message Error message (optional provided first ... argument) ... Arguments passed error constructor","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/storage-backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Storage Backend S3 Contract — storage-backend","title":"Storage Backend S3 Contract — storage-backend","text":"Defines S3 generic functions storage backends must implement. provides pluggable architecture different data storage formats.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/storage-backend.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Storage Backend S3 Contract — storage-backend","text":"storage backend responsible : Managing stateful resources (file handles, connections) Providing dimension information Reading data canonical timepoints × voxels orientation Providing mask information Extracting metadata","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/stream_subjects.html","id":null,"dir":"Reference","previous_headings":"","what":"Stream subjects with optional ordering — stream_subjects","title":"Stream subjects with optional ordering — stream_subjects","text":"Stream subjects optional ordering","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/stream_subjects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stream subjects with optional ordering — stream_subjects","text":"","code":"stream_subjects(gd, prefetch = 1L, order_by = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/stream_subjects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stream subjects with optional ordering — stream_subjects","text":"gd fmri_group. prefetch Number subjects prefetch. Currently 1L supported; higher values accepted future compatibility change behaviour. order_by Optional column name used order subjects.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/stream_subjects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stream subjects with optional ordering — stream_subjects","text":"iterator identical iter_subjects().","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study-backend-seed.html","id":null,"dir":"Reference","previous_headings":"","what":"Study Backend Seed for DelayedArray — study-backend-seed","title":"Study Backend Seed for DelayedArray — study-backend-seed","text":"DelayedArray-compatible seed provides lazy access multi-subject fMRI data without loading subjects memory . Implemented S4 class inheriting Array integrates natively DelayedArray.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Study Backend — study_backend","title":"Study Backend — study_backend","text":"Composite backend lazily combines multiple subject-level backends.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Study Backend — study_backend","text":"","code":"study_backend(   backends,   subject_ids = NULL,   strict = getOption(\"fmridataset.mask_check\", \"identical\") )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Study Backend — study_backend","text":"backends list storage_backend objects subject_ids vector subject identifiers matching backends strict mask validation mode. \"identical\" \"intersect\"","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Study Backend — study_backend","text":"study_backend object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_backend_seed.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Study Backend Seed — study_backend_seed","title":"Create a Study Backend Seed — study_backend_seed","text":"Create Study Backend Seed","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_backend_seed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Study Backend Seed — study_backend_seed","text":"","code":"study_backend_seed(backends, subject_ids)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_backend_seed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Study Backend Seed — study_backend_seed","text":"backends List subject backend objects subject_ids Character vector subject identifiers","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_backend_seed.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Study Backend Seed — study_backend_seed","text":"StudyBackendSeed S4 object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_seed_chunk_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Chunk Grid for Study Backend Seed — study_seed_chunk_grid","title":"Get Chunk Grid for Study Backend Seed — study_seed_chunk_grid","text":"Get Chunk Grid Study Backend Seed","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_seed_chunk_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Chunk Grid for Study Backend Seed — study_seed_chunk_grid","text":"","code":"study_seed_chunk_grid(x, chunk_dim = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_seed_chunk_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Chunk Grid for Study Backend Seed — study_seed_chunk_grid","text":"x study_backend_seed object chunk_dim Optional chunk dimensions","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_seed_chunk_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Chunk Grid for Study Backend Seed — study_seed_chunk_grid","text":"RegularArrayGrid object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_seed_is_sparse.html","id":null,"dir":"Reference","previous_headings":"","what":"Check if Study Backend Seed is Sparse — study_seed_is_sparse","title":"Check if Study Backend Seed is Sparse — study_seed_is_sparse","text":"Check Study Backend Seed Sparse","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_seed_is_sparse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check if Study Backend Seed is Sparse — study_seed_is_sparse","text":"","code":"study_seed_is_sparse(x)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_seed_is_sparse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check if Study Backend Seed is Sparse — study_seed_is_sparse","text":"x study_backend_seed object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/study_seed_is_sparse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check if Study Backend Seed is Sparse — study_seed_is_sparse","text":"Logical indicating data sparse","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/subject_ids.html","id":null,"dir":"Reference","previous_headings":"","what":"Get Subject IDs from Multi-Subject Dataset — subject_ids","title":"Get Subject IDs from Multi-Subject Dataset — subject_ids","text":"Generic function extract subject identifiers multi-subject fMRI dataset objects.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/subject_ids.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get Subject IDs from Multi-Subject Dataset — subject_ids","text":"","code":"subject_ids(x, ...)  # S3 method for class 'fmri_study_dataset' subject_ids(x, ...)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/subject_ids.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get Subject IDs from Multi-Subject Dataset — subject_ids","text":"x multi-subject dataset object (e.g., fmri_study_dataset) ... Additional arguments passed methods","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/subject_ids.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get Subject IDs from Multi-Subject Dataset — subject_ids","text":"Character vector subject identifiers","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/subject_ids.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Get Subject IDs from Multi-Subject Dataset — subject_ids","text":"Multi-subject datasets contain data multiple participants. function extracts subject identifiers associated dataset. order subject IDs corresponds order datasets.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/subjects.html","id":null,"dir":"Reference","previous_headings":"","what":"Access the subjects tibble stored inside an fmri_group — subjects","title":"Access the subjects tibble stored inside an fmri_group — subjects","text":"Access subjects tibble stored inside fmri_group","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/subjects.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Access the subjects tibble stored inside an fmri_group — subjects","text":"","code":"subjects(x)  subjects(x) <- value"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/subjects.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Access the subjects tibble stored inside an fmri_group — subjects","text":"x fmri_group. value replacement table containing dataset column used group.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/subjects.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Access the subjects tibble stored inside an fmri_group — subjects","text":"underlying data.frame one row per subject. updated fmri_group.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/unregister_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Unregister a Backend — unregister_backend","title":"Unregister a Backend — unregister_backend","text":"Removes backend registry. Use caution may break code depends backend.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/unregister_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Unregister a Backend — unregister_backend","text":"","code":"unregister_backend(name)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/unregister_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Unregister a Backend — unregister_backend","text":"name Character string, name backend remove","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/unregister_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Unregister a Backend — unregister_backend","text":"Invisibly returns TRUE backend removed, FALSE registered","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/unregister_backend.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Unregister a Backend — unregister_backend","text":"","code":"if (FALSE) { # \\dontrun{ # Remove a custom backend unregister_backend(\"my_custom_backend\") } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/update_golden_test_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Update Golden Test Data — update_golden_test_data","title":"Update Golden Test Data — update_golden_test_data","text":"Update existing golden test data new expected outputs. Use intentional changes package require updating reference data.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/update_golden_test_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Update Golden Test Data — update_golden_test_data","text":"","code":"update_golden_test_data(   output_dir = \"tests/testthat/golden\",   seed = 42,   confirm = TRUE )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/update_golden_test_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Update Golden Test Data — update_golden_test_data","text":"output_dir Directory golden data files saved. Defaults \"tests/testthat/golden\". seed Random seed reproducibility. Defaults 42. confirm Logical. TRUE, prompt confirmation updating. Defaults TRUE.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/update_golden_test_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Update Golden Test Data — update_golden_test_data","text":"Invisibly returns TRUE success.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/update_golden_test_data.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Update Golden Test Data — update_golden_test_data","text":"function used caution overwrite existing golden test data. use certain current outputs correct become new reference.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/update_golden_test_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Update Golden Test Data — update_golden_test_data","text":"","code":"if (FALSE) { # \\dontrun{ # Update golden data (will prompt for confirmation) update_golden_test_data()  # Update without confirmation (use with caution!) update_golden_test_data(confirm = FALSE) } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Backend Implementation — validate_backend","title":"Validate Backend Implementation — validate_backend","text":"Validates backend implements required contract correctly. Internal function validate storage backend objects","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Backend Implementation — validate_backend","text":"","code":"validate_backend(backend)  validate_backend(backend)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Backend Implementation — validate_backend","text":"backend storage backend object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Backend Implementation — validate_backend","text":"TRUE valid, otherwise throws error","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_fmri_group.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate an fmri_group object — validate_fmri_group","title":"Validate an fmri_group object — validate_fmri_group","text":"Validate fmri_group object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_fmri_group.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate an fmri_group object — validate_fmri_group","text":"","code":"validate_fmri_group(x)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_fmri_group.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate an fmri_group object — validate_fmri_group","text":"x Object validate.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_fmri_group.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate an fmri_group object — validate_fmri_group","text":"validated fmri_group (invisibly).","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_golden_test_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Golden Test Data — validate_golden_test_data","title":"Validate Golden Test Data — validate_golden_test_data","text":"Check expected golden test data files exist readable.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_golden_test_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Golden Test Data — validate_golden_test_data","text":"","code":"validate_golden_test_data(output_dir = \"tests/testthat/golden\")"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_golden_test_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Golden Test Data — validate_golden_test_data","text":"output_dir Directory golden data files saved. Defaults \"tests/testthat/golden\".","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_golden_test_data.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Golden Test Data — validate_golden_test_data","text":"logical vector indicating files exist, names corresponding expected files.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_golden_test_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate Golden Test Data — validate_golden_test_data","text":"","code":"if (FALSE) { # \\dontrun{ # Check golden data files validate_golden_test_data() } # }"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_registered_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate Registered Backend — validate_registered_backend","title":"Validate Registered Backend — validate_registered_backend","text":"Validates backend instance using standard contract backend-specific validation function.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_registered_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate Registered Backend — validate_registered_backend","text":"","code":"validate_registered_backend(backend, registration = NULL)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_registered_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate Registered Backend — validate_registered_backend","text":"backend storage backend object registration Backend registration information (optional, looked provided)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/validate_registered_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate Registered Backend — validate_registered_backend","text":"TRUE valid, otherwise throws error","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/voxel_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"Voxel Coordinate Series Selector — voxel_selector","title":"Voxel Coordinate Series Selector — voxel_selector","text":"Select voxels 3D coordinates image space.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/voxel_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Voxel Coordinate Series Selector — voxel_selector","text":"","code":"voxel_selector(coords)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/voxel_selector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Voxel Coordinate Series Selector — voxel_selector","text":"coords Matrix 3 columns (x, y, z) vector length 3","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/voxel_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Voxel Coordinate Series Selector — voxel_selector","text":"object class voxel_selector","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/voxel_selector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Voxel Coordinate Series Selector — voxel_selector","text":"","code":"# Select single voxel sel <- voxel_selector(c(10, 20, 15))  # Select multiple voxels coords <- cbind(x = c(10, 20), y = c(20, 30), z = c(15, 15)) sel <- voxel_selector(coords)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/with_rowData.html","id":null,"dir":"Reference","previous_headings":"","what":"Attach rowData metadata to a lazy matrix — with_rowData","title":"Attach rowData metadata to a lazy matrix — with_rowData","text":"Helper reattaching metadata DelayedMatrixStats operations.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/with_rowData.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Attach rowData metadata to a lazy matrix — with_rowData","text":"","code":"with_rowData(x, rowData)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/with_rowData.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Attach rowData metadata to a lazy matrix — with_rowData","text":"x lazy matrix matrix-like object rowData data.frame row-wise metadata","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/with_rowData.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Attach rowData metadata to a lazy matrix — with_rowData","text":"x rowData attribute set","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/write_fmri_config.html","id":null,"dir":"Reference","previous_headings":"","what":"Write fMRI configuration file — write_fmri_config","title":"Write fMRI configuration file — write_fmri_config","text":"Writes fMRI configuration YAML file easy editing sharing.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/write_fmri_config.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write fMRI configuration file — write_fmri_config","text":"","code":"write_fmri_config(config, file_name)"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/write_fmri_config.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write fMRI configuration file — write_fmri_config","text":"config fmri_config object list configuration parameters file_name Output file name (end .yaml .yml)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/zarr-backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Zarr Storage Backend — zarr-backend","title":"Zarr Storage Backend — zarr-backend","text":"storage backend implementation Zarr array format using Rarr package. Zarr cloud-native array storage format supports chunked, compressed n-dimensional arrays concurrent read/write access.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/zarr-backend.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Zarr Storage Backend — zarr-backend","text":"backend provides efficient access neuroimaging data stored Zarr format, particularly well-suited : Large datasets fit memory Cloud storage (S3, GCS, Azure) Parallel processing workflows Progressive data access patterns backend expects Zarr arrays organized : 4D array dimensions (x, y, z, time) Optional mask array \"mask\" key Metadata stored Zarr attributes","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/zarr_backend.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a Zarr Backend — zarr_backend","title":"Create a Zarr Backend — zarr_backend","text":"Creates storage backend Zarr array data.","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/zarr_backend.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a Zarr Backend — zarr_backend","text":"","code":"zarr_backend(   source,   data_key = \"data\",   mask_key = \"mask\",   preload = FALSE,   cache_size = 100 )"},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/zarr_backend.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a Zarr Backend — zarr_backend","text":"source Character path Zarr store (directory zip) URL remote stores data_key Character key main data array within store (default: \"data\") mask_key Character key mask array (default: \"mask\"). Set NULL mask. preload Logical, whether load data memory (default: FALSE) cache_size Integer, number chunks cache memory (default: 100)","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/zarr_backend.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a Zarr Backend — zarr_backend","text":"zarr_backend S3 object","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/reference/zarr_backend.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a Zarr Backend — zarr_backend","text":"","code":"if (FALSE) { # \\dontrun{ # Local Zarr store backend <- zarr_backend(\"path/to/data.zarr\")  # Remote S3 store backend <- zarr_backend(\"s3://bucket/path/to/data.zarr\")  # Custom array keys backend <- zarr_backend(   \"data.zarr\",   data_key = \"fmri/bold\",   mask_key = \"fmri/mask\" ) } # }"},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/news/index.html","id":"new-features-0-9-0","dir":"Changelog","previous_headings":"","what":"New features","title":"fmridataset 0.9.0 (Development)","text":"Allows creation datasets non-existent file paths testing Returns placeholder data (zeros) standard dimensions Useful testing dependent packages without requiring actual data files Enable dummy_mode = TRUE fmri_dataset() constructor fmri_series() study helpers now return delarr objects default Added as_delarr() generics storage backends study adapters Retained optional as_delayed_array() paths explicit DelayedMatrix output","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/news/index.html","id":"critical-fixes-0-8-9","dir":"Changelog","previous_headings":"","what":"Critical fixes","title":"fmridataset 0.8.9 (Hotfix)","text":"Memoization now uses cachem configurable size limit (default 512MB) Added fmri_clear_cache() function manually clear cache Cache size configurable via options(fmridataset.cache_max_mb = 1024) Warning operations load >1GB memory Automatic chunking operations load >2GB Recommends using data_chunks() large datasets","code":""},{"path":[]},{"path":"https://bbuchsbaum.github.io/fmridataset/news/index.html","id":"new-features-0-1-0","dir":"Changelog","previous_headings":"","what":"New features","title":"fmridataset 0.1.0","text":"Added comprehensive CI/CD pipeline GitHub Actions Added test coverage reporting codecov Added code style checking automatic formatting Added issue PR templates better project management Implemented as_tibble.fmri_study_dataset metadata optimization Added integration performance tests fmri_study_dataset workflow","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/news/index.html","id":"bug-fixes-0-1-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"fmridataset 0.1.0","text":"Fixed chunking edge case nchunks > number voxels Updated deprecated with_mock() calls with_mocked_bindings() Fixed dimensional consistency issues storage backends Resolved test failures package refactoring","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/news/index.html","id":"documentation-0-1-0","dir":"Changelog","previous_headings":"","what":"Documentation","title":"fmridataset 0.1.0","text":"Added comprehensive README badges examples Improved package architecture documentation Added codecov configuration coverage reporting New vignette “Single-Subject Study-Level Analysis” performance guidelines architectural diagram","code":""},{"path":"https://bbuchsbaum.github.io/fmridataset/news/index.html","id":"internal-changes-0-1-0","dir":"Changelog","previous_headings":"","what":"Internal changes","title":"fmridataset 0.1.0","text":"Refactored monolithic codebase modular architecture Improved test organization coverage Enhanced error handling validation Modernized CI/CD workflows tooling","code":""}]
